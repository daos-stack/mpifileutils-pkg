diff --git a/.github/CONTRIBUTING.md b/.github/CONTRIBUTING.md
deleted file mode 100644
index d7c9a01..0000000
--- a/.github/CONTRIBUTING.md
+++ /dev/null
@@ -1,96 +0,0 @@
-# Contributing to mpiFileUtils
-
-*Thank you for taking the time to contribute!*
-
-## Table of Contents
-[Resources](#resources)
-
-[How To Contribute](#how-to-contribute)
-  * [Reporting an Issue & Feature Suggestions](#reporting-an-issue--feature-suggestions)
-  * [Pull Requests](#pull-requests)
-  * [License](#license)
-  * [Contributor's Declaration](#contributors-declaration)
-
-## Resources
-[mpiFileUtils Google Group](https://groups.google.com/forum/#!forum/mpifileutils)
-
-## How To Contribute
-
-### Reporting an Issue & Feature Suggestions
-You can find a list of all outstanding issues and feature requests on our Github
-[issue tracker](https://github.com/hpc/mpifileutils/issues). Before creating a new
-issue or feature suggestion, ensure that:
-* The issue or feature request hasn't been addressed in a newer version of mpiFileUtils.
-* That the issue or feature request doesn't already exist in the issue tracker.
-
-If the issue or feature request already exists, please provide additional
-information if possible.
-
-When creating a new issue, please provide at least the following information:
-* mpiFileUtils Version
-* Linux Distribution & Version
-* Steps to reproduce the problem
-* Information from various system logs (i.e. call stacks, dmesg output)
-
-### Pull Requests
-If you would like us to consider a change to mpiFileUtils, feel free to submit
-a pull request. Your pull request must meet the following criteria before
-it can be merged.
-
-* Your pull request must be based on the current master branch and
-apply without conflicts.
-* Please try to limit pull requests to a single commit which resolves
-one issue.
-* Make sure your commit messages have a `Signed-off-by` line. See
-[Contributor's Declaration](#contributors-declaration) for more information.
-* For large pull requests, consider structuring your changes as a stack of
-logically independent patches which build on each other.  This makes large
-changes easier to review and approve which speeds up the merging process.
-* Try to keep pull requests simple. Simple code with comments is much easier
-to review and approve.
-* Test cases should be provided when appropriate.
-* Your pull request must pass automated testing.
-* All proposed changes must be approved by an mpiFileUtils project member.
-
-### License
-mpiFileUtils is distributed under the New BSD License with a few additional notices.
-Note that the phrase "above copyright notice" in the license text refers to the
-current list of copyrights that appears in the
-[license](https://github.com/hpc/mpifileutils/blob/master/LICENSE) file found in
-the mpiFileUtils master branch.
-
-### Contributor's Declaration
-mpiFileUtils has adopted the signed-off-by process as described in Section
-11 of the Linux kernel document on
-[Submitting Patches](https://www.kernel.org/doc/html/latest/process/submitting-patches.html).
-Each proposed contribution to the mpiFileUtils code base must include the text
-"Signed-off-by:" followed by the contributor's name and email address. This is
-the developer's certification that they have the right to submit the patch for
-inclusion into the code base and indicates agreement to the Developer's
-Certificate of Origin:
-
-"By making a contribution to this project, I certify that:
-1. The contribution was created in whole or in part by me and I have the right
-to submit it under the open source license indicated in the file; or
-2. The contribution is based upon previous work that, to the best of my knowledge,
-is covered under an appropriate open source license and I have the right under
-that license to submit that work with modifications, whether created in whole or
-in part by me, under the same open source license (unless I am permitted to submit
-under a different license), as indicated in the file; or
-3. The contribution was provided directly to me by some other person who certified
-(1), (2) or (3) and I have not modified it.
-4. I understand and agree that this project and the contribution are public and
-that a record of the contribution (including all personal information I submit
-with it, including my sign-off) is maintained indefinitely and may be
-redistributed consistent with this project or the open source license(s) involved."
-
-After reading and agreeing to the Contributor's Declaration, include your
-Signed-off-by text like the following as the last line in your commit message:
-
-```
-Signed-off-by: Random J Developer <random@developer.example.org>
-```
-
-Proposed contributions failing to include a "Signed-off-by:" certification will
-not be accepted into mpiFileUtils. The maintainers reserve the right to revert
-any commit made without the required certification.
diff --git a/.travis.yml b/.travis.yml
index 605bf4d..8c1dc77 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -1,5 +1,5 @@
 sudo: required
-dist: trusty
+dist: xenial
 
 language: c
 
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 8cef16b..4880124 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -13,11 +13,20 @@ LIST(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/cmake")
 # Configuration Options
 
 OPTION(ENABLE_XATTRS "Enable code for extended attributes" ON)
+MESSAGE(STATUS "ENABLE_XATTRS: ${ENABLE_XATTRS}")
 IF(ENABLE_XATTRS)
   ADD_DEFINITIONS(-DDCOPY_USE_XATTRS)
+
+  FIND_PACKAGE(LibAttr REQUIRED)
+  IF(LibAttr_FOUND)
+    ADD_DEFINITIONS(-DHAVE_LIBATTR)
+    INCLUDE_DIRECTORIES(${LibAttr_INCLUDE_DIRS})
+    LIST(APPEND MFU_EXTERNAL_LIBS ${LibAttr_LIBRARIES})
+  ENDIF(LibAttr_FOUND)
 ENDIF(ENABLE_XATTRS)
 
 OPTION(ENABLE_LUSTRE "Enable optimization and features for Lustre" OFF)
+MESSAGE(STATUS "ENABLE_LUSTRE: ${ENABLE_LUSTRE}")
 IF(ENABLE_LUSTRE)
   ADD_DEFINITIONS(-DLUSTRE_SUPPORT)
 
@@ -49,6 +58,7 @@ IF(ENABLE_LUSTRE)
 ENDIF(ENABLE_LUSTRE)
 
 OPTION(ENABLE_GPFS "Enable GFPS/Spectrum Scale support")
+MESSAGE(STATUS "ENABLE_GPFS: ${ENABLE_GPFS}")
 IF(ENABLE_GPFS)
   FIND_PACKAGE(GPFS REQUIRED)
   INCLUDE_DIRECTORIES(${GPFS_INCLUDE_DIRS})
@@ -57,6 +67,7 @@ IF(ENABLE_GPFS)
 ENDIF(ENABLE_GPFS)
 
 OPTION(ENABLE_EXPERIMENTAL "Build experimental tools" OFF)
+MESSAGE(STATUS "ENABLE_EXPERIMENTAL: ${ENABLE_EXPERIMENTAL}")
 
 ## HEADERS
 INCLUDE(CheckIncludeFile)
@@ -81,6 +92,7 @@ LIST(APPEND MFU_EXTERNAL_LIBS ${DTCMP_LIBRARIES})
 
 ## LIBARCHIVE
 OPTION(ENABLE_LIBARCHIVE "Enable usage of libarchive and corresponding tools" ON)
+MESSAGE(STATUS "ENABLE_LIBARCHIVE: ${ENABLE_LIBARCHIVE}")
 # TODO how would we pass a version from spack?
 # libarchive 3.1.2 is available on some systems,
 # but pick a newer libarchive to avoid bug with files starting with "._",
@@ -92,17 +104,26 @@ IF(ENABLE_LIBARCHIVE)
   ADD_DEFINITIONS(-DLIBARCHIVE_SUPPORT)
 ENDIF(ENABLE_LIBARCHIVE)
 
+## hdf5 
+OPTION(ENABLE_HDF5 "Enable HDF5 library")
+MESSAGE(STATUS "ENABLE_HDF5: ${ENABLE_HDF5}")
+IF(ENABLE_HDF5)
+  FIND_PACKAGE(HDF5 REQUIRED)
+  INCLUDE_DIRECTORIES(${HDF5_INCLUDE_DIRS})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${HDF5_LIBRARIES})
+  ADD_DEFINITIONS(-DHDF5_SUPPORT)
+  ADD_DEFINITIONS(-DHDF5_BIN_DIR=\"${HDF5_BIN_DIR}\")
+ENDIF(ENABLE_HDF5)
+
 OPTION(ENABLE_DAOS "Enable DAOS support")
+MESSAGE(STATUS "ENABLE_DAOS: ${ENABLE_DAOS}")
 IF(ENABLE_DAOS)
   SET(CMAKE_EXE_LINKER_FLAGS -luuid)
-  FIND_PACKAGE(CART REQUIRED)
-  INCLUDE_DIRECTORIES(${CART_INCLUDE_DIRS})
-  message(${CART_INCLUDE_DIRS})
-  LIST(APPEND MFU_EXTERNAL_LIBS ${CART_LIBRARIES})
-  LIST(APPEND MFU_EXTERNAL_LIBS ${GURT_LIBRARIES})
   FIND_PACKAGE(DAOS REQUIRED)
   INCLUDE_DIRECTORIES(${DAOS_INCLUDE_DIRS})
   LIST(APPEND MFU_EXTERNAL_LIBS ${DAOS_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${GURT_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${CART_LIBRARIES})
   LIST(APPEND MFU_EXTERNAL_LIBS ${DUNS_LIBRARIES})
   LIST(APPEND MFU_EXTERNAL_LIBS ${DFS_LIBRARIES})
   LIST(APPEND MFU_EXTERNAL_LIBS ${DAOS_COMMON_LIBRARIES})
@@ -169,7 +190,10 @@ ENDIF("${isSystemDir}" STREQUAL "-1")
 
 # Subdirectories
 INCLUDE(MFU_ADD_TOOL)
-INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/src/common)
+
+# use BEFORE to place build directory first when searching for header files
+# otherwise the build can use old header files from the install path instead
+INCLUDE_DIRECTORIES(BEFORE ${CMAKE_CURRENT_SOURCE_DIR}/src/common)
 
 ADD_SUBDIRECTORY(src)
 ADD_SUBDIRECTORY(test)
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
new file mode 100644
index 0000000..d7c9a01
--- /dev/null
+++ b/CONTRIBUTING.md
@@ -0,0 +1,96 @@
+# Contributing to mpiFileUtils
+
+*Thank you for taking the time to contribute!*
+
+## Table of Contents
+[Resources](#resources)
+
+[How To Contribute](#how-to-contribute)
+  * [Reporting an Issue & Feature Suggestions](#reporting-an-issue--feature-suggestions)
+  * [Pull Requests](#pull-requests)
+  * [License](#license)
+  * [Contributor's Declaration](#contributors-declaration)
+
+## Resources
+[mpiFileUtils Google Group](https://groups.google.com/forum/#!forum/mpifileutils)
+
+## How To Contribute
+
+### Reporting an Issue & Feature Suggestions
+You can find a list of all outstanding issues and feature requests on our Github
+[issue tracker](https://github.com/hpc/mpifileutils/issues). Before creating a new
+issue or feature suggestion, ensure that:
+* The issue or feature request hasn't been addressed in a newer version of mpiFileUtils.
+* That the issue or feature request doesn't already exist in the issue tracker.
+
+If the issue or feature request already exists, please provide additional
+information if possible.
+
+When creating a new issue, please provide at least the following information:
+* mpiFileUtils Version
+* Linux Distribution & Version
+* Steps to reproduce the problem
+* Information from various system logs (i.e. call stacks, dmesg output)
+
+### Pull Requests
+If you would like us to consider a change to mpiFileUtils, feel free to submit
+a pull request. Your pull request must meet the following criteria before
+it can be merged.
+
+* Your pull request must be based on the current master branch and
+apply without conflicts.
+* Please try to limit pull requests to a single commit which resolves
+one issue.
+* Make sure your commit messages have a `Signed-off-by` line. See
+[Contributor's Declaration](#contributors-declaration) for more information.
+* For large pull requests, consider structuring your changes as a stack of
+logically independent patches which build on each other.  This makes large
+changes easier to review and approve which speeds up the merging process.
+* Try to keep pull requests simple. Simple code with comments is much easier
+to review and approve.
+* Test cases should be provided when appropriate.
+* Your pull request must pass automated testing.
+* All proposed changes must be approved by an mpiFileUtils project member.
+
+### License
+mpiFileUtils is distributed under the New BSD License with a few additional notices.
+Note that the phrase "above copyright notice" in the license text refers to the
+current list of copyrights that appears in the
+[license](https://github.com/hpc/mpifileutils/blob/master/LICENSE) file found in
+the mpiFileUtils master branch.
+
+### Contributor's Declaration
+mpiFileUtils has adopted the signed-off-by process as described in Section
+11 of the Linux kernel document on
+[Submitting Patches](https://www.kernel.org/doc/html/latest/process/submitting-patches.html).
+Each proposed contribution to the mpiFileUtils code base must include the text
+"Signed-off-by:" followed by the contributor's name and email address. This is
+the developer's certification that they have the right to submit the patch for
+inclusion into the code base and indicates agreement to the Developer's
+Certificate of Origin:
+
+"By making a contribution to this project, I certify that:
+1. The contribution was created in whole or in part by me and I have the right
+to submit it under the open source license indicated in the file; or
+2. The contribution is based upon previous work that, to the best of my knowledge,
+is covered under an appropriate open source license and I have the right under
+that license to submit that work with modifications, whether created in whole or
+in part by me, under the same open source license (unless I am permitted to submit
+under a different license), as indicated in the file; or
+3. The contribution was provided directly to me by some other person who certified
+(1), (2) or (3) and I have not modified it.
+4. I understand and agree that this project and the contribution are public and
+that a record of the contribution (including all personal information I submit
+with it, including my sign-off) is maintained indefinitely and may be
+redistributed consistent with this project or the open source license(s) involved."
+
+After reading and agreeing to the Contributor's Declaration, include your
+Signed-off-by text like the following as the last line in your commit message:
+
+```
+Signed-off-by: Random J Developer <random@developer.example.org>
+```
+
+Proposed contributions failing to include a "Signed-off-by:" certification will
+not be accepted into mpiFileUtils. The maintainers reserve the right to revert
+any commit made without the required certification.
diff --git a/DAOS-Support.md b/DAOS-Support.md
index 3e01829..7d3b722 100644
--- a/DAOS-Support.md
+++ b/DAOS-Support.md
@@ -1,46 +1,75 @@
 # DAOS Support
 
-[DAOS](https://github.com/daos-stack/daos) is supported as a backend storage system in dcp, dsync, and dcmp. The build instructions for
-enabling DAOS support can be found here:
-[Enable DAOS](https://mpifileutils.readthedocs.io/en/latest/build.html#build-everything-directly).
-The following are ways that DAOS can be used to move data both across DAOS as well as POSIX
-filesystems in dcp and dsync:
-
-1. DAOS  -> POSIX
-2. POSIX -> DAOS
-3. DAOS  -> DAOS
-
-For dcp, the DAOS->DAOS case supports both POSIX and non-POSIX DAOS containers.
-
-For dsync, the DAOS->DAOS case currently only supports POSIX containers.
-
-## DAOS Data Movement Use Cases
-
-In each use case, it is assumed that the pools and containers used already exist.
-Also, only one DAOS source is supported.
-
-1. **DAOS Destination**
-    * Container is assumed to exist already
-
-2. **DAOS Source**
-    * Source container exists
 
-3. **DAOS Source and Destination**
-    * Copy across two different pools
-    * Copy across containers in the same pool
-    * Copying non-POSIX containers is only supported for the DAOS -> DAOS case in dcp
+[DAOS](https://github.com/daos-stack/daos) is supported as a backend
+storage system in the mpiFileUtils commands dcp, dsync, and dcmp.
+The build instructions for enabling DAOS support in mpiFileUtils
+can be found here:
+[Enable DAOS](https://mpifileutils.readthedocs.io/en/latest/build.html#build-everything-directly).
 
-## DAOS POSIX Data Movement Examples with dcp
+With dcp and dsync, the DAOS backend can be used to move data across
+two DAOS containers of the same type, as well as between a DAOS
+POSIX container and a (non-DAOS) POSIX filesystem like Lustre or GPFS:
+
+1. DAOS POSIX container -> POSIX FS
+2. POSIX FS -> DAOS POSIX container
+3. DAOS POSIX container  -> DAOS POSIX container
+4. DAOS non-POSIX container  -> DAOS non-POSIX container
+
+When the source or the destination is a (non-DAOS) POSIX filesystem,
+the DAOS File System (DFS) API will be used.
+When both the source and the destination are DAOS POSIX containers,
+both the DFS API and the DAOS Object API are supported to perform
+the copy/sync operations (by default, the DFS API will be used).
+When both the source and the destination are DAOS non-POSIX containers
+(of the same type),
+the DAOS Object API will be used to perform the copy/sync operations.
+
+When DAOS and HDF5 support is enabled, mpiFileUtils also provides the
+daos-serialize and daos-deserialize commands.
+These commands can be used to save the contents of a DAOS container
+and its associated metadata into a POSIX filesystem,
+and to restore that DAOS container with its associated metadata
+back into a DAOS storage system at a later time.
+This functionality is also known as "container parking",
+and it is supported for any type of DAOS container.
+The typical use case is the "archiving" of a DAOS container to a
+lower cost, higher capacity storage system
+to free up space in the DAOS storage system for other projects.
+This step is performed with the daos-serialze command.
+When the DAOS container needs to be accessed again, it can be re-established
+in the DAOS storage system by using the daos-deserialize command.
+The daos-serialize and daos-deserialize commands are MPI-parallel
+applications.
+They use HDF5 to write and read the DAOS container data and metadata
+as "opaque" HDF5 files, one HDF5 file per MPI rank.
+In order to use these commands, HDF5 needs to be installed and
+mpiFileUtils needs to be built with both DAOS support and HDF5 support.
+See the above build instructions for details.
+
+
+
+## Data Movement Examples with dcp, using the DFS API
 
 #### Example One
 
-Show a copy from a regular /tmp source path to a DAOS container. 
-A DAOS Unified Namespace path is used as the destination, which allows you to lookup
-the pool and container UUID from the path. This feature can only be used if the 
-container is created with a path.
+Perform a copy from a regular /tmp source path to a DAOS POSIX container.
+A DAOS Unified Namespace (DUNS) path is used as the destination,
+which enables the tool to look up the pool UUID and container UUID from
+the DUNS path. This feature can only be used if the
+container has been created with the ` --path=` option
+to store the container information as extended attributes in a DUNS path.
+
+Note: The DUNS path should be a directory in a globally accessible filesystem.
+All nodes in the MPI job that execute the dcp command will need to
+extract the DAOS pool and container information.
+If the path given to the `daos cont create` command is in a local
+ filesystem on the node that runs this daos command
+(like a node-local /tmp), that information will only be visible locally
+on that node and the DUNS lookup on other nodes will fail.
 
 ```shell
-$ mpirun -np 3 dcp -v /tmp/$USER/s /tmp/$USER/conts/p1cont1
+$ mpirun -np 3 dcp -v /tmp/$USER/s /lustrefs/$USER/conts/p1cont1
 [2020-04-23T17:04:15]   Items: 6
 [2020-04-23T17:04:15]   Directories: 3
 [2020-04-23T17:04:15]   Files: 3
@@ -49,10 +78,12 @@ $ mpirun -np 3 dcp -v /tmp/$USER/s /tmp/$USER/conts/p1cont1
 
 #### Example Two
 
-Show a copy where the pool and container UUID are passed in directly. This option
-can be used if the type of container is POSIX, but the container was not created
-with a path. The destination is the relative path within the DAOS container, which 
-in this example is the root of the container. 
+Perform a copy from a POSIX filesystem to a DAOS POSIX container,
+where the destination pool and container UUIDs are passed in directly.
+This option can be used if the type of the DAOS container is POSIX,
+but the container was not created with a DUNS path.
+The destination is a relative path within the DAOS container, which
+in this example is the root of the container.
 
 ```shell
 $ mpirun -np 3 dcp -v /tmp/$USER/s daos://$pool/$cont
@@ -61,13 +92,17 @@ $ mpirun -np 3 dcp -v /tmp/$USER/s daos://$pool/$cont
 [2020-04-23T17:17:51]   Files: 3
 [2020-04-23T17:17:51]   Links: 0
 ```
+
 #### Example Three
 
-Show a copy from one DAOS container to another container that exists in the same
-pool. A DAOS Unified Namespace path is used as the source and the destination.
+Perform a copy from one DAOS POSIX container to another DAOS POSIX container.
+The two containers can reside in the same pool, or in different pools.
+In this example, a DAOS Unified Namespace path is used as the source and
+as the destination.
+The complete source container will be copied to the destination.
 
 ```shell
-$ mpirun -np 3 dcp -v /tmp/$USER/conts/p1cont1 /tmp/$USER/conts/p1cont2
+$ mpirun -np 3 dcp -v /lustrefs/$USER/conts/p1cont1 /lustrefs/$USER/conts/p1cont2
 [2020-04-23T17:04:15] Items: 6
 [2020-04-23T17:04:15]   Directories: 3
 [2020-04-23T17:04:15]   Files: 3
@@ -76,9 +111,11 @@ $ mpirun -np 3 dcp -v /tmp/$USER/conts/p1cont1 /tmp/$USER/conts/p1cont2
 
 #### Example Four
 
-This example passes in the pool and container UUID directly. The destination path
-is the relative path within the DAOS container, which in this case is a subset of 
-the DAOS container. 
+Perform a copy from one DAOS POSIX container to another DAOS POSIX container.
+The two containers can reside in the same pool, or in different pools.
+This example passes in the pool and container UUIDs directly. The source path
+is a relative path within the DAOS container, which in this case is a subdirectory within
+the source DAOS container. Only this subdirectory tree will be copied to the destination.
 
 ```shell
 $ mpirun -np 3 dcp -v daos://$pool1/$cont1/s/biggerfile daos://$pool2/$cont2
@@ -90,23 +127,26 @@ $ mpirun -np 3 dcp -v daos://$pool1/$cont1/s/biggerfile daos://$pool2/$cont2
 
 #### Example Five
 
-This example copies data from a DAOS container to /tmp, where a DAOS
-Unified Namespace path is used as the source. 
+This example copies data from a DAOS POSIX container to /tmp,
+where a DAOS Unified Namespace path is used as the source.
 
 ```shell
-$ mpirun -np 3 dcp -v /tmp/$USER/conts/p1cont1 /tmp/$USER/d
+$ mpirun -np 3 dcp -v /lustrefs/$USER/conts/p1cont1 /tmp/$USER/d
 [2020-04-23T17:17:51] Items: 6
 [2020-04-23T17:17:51]   Directories: 3
 [2020-04-23T17:17:51]   Files: 3
 [2020-04-23T17:17:51]   Links: 0
 ```
-## DAOS non-POSIX Data Movement Examples With dsync
 
-#### Example One 
 
-Show the copy from one DAOS container to another, where both of the DAOS
-containers are of the POSIX type, but a user would like to copy DAOS
-containers at the object level.
+## Data Movement Examples with dcp, using the DAOS API
+
+#### Example One
+
+Perform a copy from one DAOS POSIX container to another DAOS POSIX container.
+The two containers can reside in the same pool, or in different pools.
+in this example, the user requests to copy the DAOS POSIX container
+using the DAOS object level API instead of the DFS API.
 
 ```shell
 $ mpirun -np 3 dcp -v --daos-api=DAOS daos://$pool1/$p1cont1 daos://$pool1/$p1cont2
@@ -114,26 +154,29 @@ $ mpirun -np 3 dcp -v --daos-api=DAOS daos://$pool1/$p1cont1 daos://$pool1/$p1co
 ```
 #### Example Two
 
-Show the copy from one DAOS container to another, where both of the DAOS
-containers are not of the POSIX type. This run does not require passing
-in the --daos-api=DAOS flag as it will detect the containers as non-POSIX.
+Perform a copy from one DAOS container to another DAOS container,
+where both DAOS containers are *not* of type POSIX.
+Both containers must be of the same type.
+This scenario does not need the `--daos-api=DAOS` option:
+It will be automatically detected that the DAOS containers are of a non-POSIX
+container type, and thus the DAOS object API has to be used.
 
 ```shell
 $ mpirun -np 3 dcp -v daos://$pool1/$p1cont1 daos://$pool1/$p1cont2
 [2021-01-20T16:16:25] Successfully copied to DAOS Destination Container.
 ```
 
-## DAOS POSIX Data Movement Examples with dsync
 
-The usage for dsync is similar to the usage for dcp. The source and destination
-can be DAOS, POSIX, or UNS paths. However, dsync currently only supports
-POSIX-type DAOS containers.
+## Data Movement Examples with dsync
+
+The usage for dsync is similar to the usage for dcp.
+The source and destination can be DAOS, POSIX, or DUNS paths.
 
 #### Example One
 
-Show the sync from one DAOS container to another, where both of the DAOS
-containers are of the POSIX type, and both the source and destination use
-a path relative to the root of each container.
+Perform the sync from one DAOS container to another, where both of the DAOS
+containers are of type POSIX. In this example, both the source and
+the destination use a path relative to the root of each container.
 
 ```shell
 $ mpirun -np 3 dsync -v daos://$pool1/$cont1/source daos://$pool2/$cont2/dest
@@ -143,16 +186,47 @@ $ mpirun -np 3 dsync -v daos://$pool1/$cont1/source daos://$pool2/$cont2/dest
 [2020-04-28T00:47:59]   Links: 0
 ```
 
-## DAOS POSIX Data Comparison Examples with dcmp
+#### Example Two
 
-Similar to dcp and dsync, dcmp supports DAOS, POSIX, and UNS paths,
-but dcmp currently only supports POSIX-type DAOS containers.
+Perform the sync from one DAOS container to another, where both of the DAOS
+containers are *not* of type POSIX. Both containers must be of the same type.
+
+```shell
+$ mpirun -np 3 dsync -v daos://$pool1/$cont1 daos://$pool1/$cont2
+[2021-03-04T19:14:17] Objects    : 6
+[2021-03-04T19:14:17]   D-Keys   : 10
+[2021-03-04T19:14:17]   A-Keys   : 14
+[2021-03-04T19:14:17] Bytes read    : 4.000 MiB (4194660 bytes)
+[2021-03-04T19:14:17] Bytes written : 4.000 MiB (4194660 bytes)
+```
+
+#### Example Three
+
+Perform the same sync as in Example Two, but here the destination container
+already contains the same data as the source container.
+In this case the data in the destination is compared to the source,
+and nothing needs to be written.
+
+```shell
+$ mpirun -np 3 dsync -v daos://$pool1/$cont1 daos://$pool1/$cont2
+[2021-03-04T19:15:03] Objects    : 6
+[2021-03-04T19:15:03]   D-Keys   : 10
+[2021-03-04T19:15:03]   A-Keys   : 14
+[2021-03-04T19:15:03] Bytes read    : 8.000 MiB (8389320 bytes)
+[2021-03-04T19:15:03] Bytes written : 0.000 B (0 bytes)
+```
+
+
+## Data Comparison Examples with dcmp
+
+Similar to dcp and dsync, dcmp supports DAOS, POSIX, and UNS paths.
+dcmp currently only supports DAOS containers of type POSIX.
 
 #### Example One
 
-Show the comparison between two DAOS containers, where both of the DAOS
-containers are of the POSIX type, and both containers use a path relative
-to the root of each container.
+Perform the comparison between two DAOS containers, where both of the DAOS
+containers are of type POSIX. In this example, both containers use a
+path relative to the root of each container.
 
 ```shell
 $ mpirun -np 3 dcmp -v daos://$pool1/$cont1/source daos://$pool2/$cont2/dest
@@ -163,3 +237,73 @@ Number of items that exist in both directories and have different types: 0 (Src:
 Number of items that exist in both directories and have the same content: 1 (Src: 1 Dest: 1)
 Number of items that exist in both directories and have different contents: 0 (Src: 0 Dest: 0)
 ```
+
+## DAOS Container Serialization and Deserialization Examples
+
+daos-serialize and daos-deserialize can be used on any type of DAOS container.
+They are DAOS only tools that require HDF5.
+
+daos-serialize will serialize a DAOS container to one or more HDF5 files.
+Depending on the amount of data, multiple files may be written
+for each rank specified in the job.
+
+daos-deserialize will deserialize or restore the HDF5 files that have been
+previously created by daos-serialize. A pool UUID to deserialize the data to
+must be specified, and a container with the original data and metadata
+will be created in that DAOS pool.
+
+#### Example One
+
+Perform the serialization of a DAOS container into a set of HDF5 files.
+In this example, the HDF5 files are stored in the current working directory
+within a global parallel filesystem.
+
+```shell
+$ mpirun -np 3 daos-serialize -v daos://$pool1/$cont1
+Serializing Container to 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank0.h5
+Serializing Container to 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank1.h5
+Serializing Container to 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank2.h5
+```
+
+#### Example Two
+
+Perform the serialization of a DAOS container into a set of HDF5 files.
+In this example, a directory in which to store the HDF5 files is specified.
+
+```shell
+$ mpirun -np 3 daos-serialize -v -o serialized-cont daos://$pool1/$cont1
+Serializing Container to serialized-cont/7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank0.h5
+Serializing Container to serialized-cont/7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank1.h5
+Serializing Container to serialized-cont/7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank2.h5
+```
+
+#### Example Three
+
+Perform the deserialization of a DAOS container that was previously serialized
+into a number of HDF5 files. In this example, the HDF5 files are individually
+specified on the command line.
+
+```shell
+$ mpirun -np 3 daos-deserialize -v --pool $pool1 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank0.h5
+7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank1.h5 2-7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank2.h5
+
+Successfully created container cbc52064-303e-497d-afaf-fa554c18e08f
+    Deserializing filename: 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank0.h5
+    Deserializing filename: 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank1.h5
+    Deserializing filename: 7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank2.h5
+```
+
+#### Example Four
+
+Perform the deserialization of a DAOS container that was previously serialized
+into a number of HDF5 files. In this example, the directory in which 
+he HDF5 files are residing is specified on the command line.
+
+```shell
+$ mpirun -np 3 daos-deserialize -v serialized-cont
+
+Successfully created container 8d6a6083-4009-4afe-8364-7caa5ebaa72b
+    Deserializing filename: serialized-cont/7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank0.h5
+    Deserializing filename: serialized-cont/7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank1.h5
+    Deserializing filename: serialized-cont/7bf8037d-823f-4fa5-ac2a-c2cae8f81f57_rank2.h5
+```
diff --git a/README.md b/README.md
index 6807ddc..64f0fb2 100644
--- a/README.md
+++ b/README.md
@@ -5,10 +5,10 @@ Documentation is available on [ReadTheDocs](http://mpifileutils.readthedocs.io).
 
 ## DAOS Support
 
-mpiFileUtils supports a DAOS backend for dcp, dsync, and dcmp. Details and usage examples are provided in [DAOS Support](DAOS-Support.md).
+mpiFileUtils supports a DAOS backend for dcp, dsync, and dcmp. Custom serialization and deserialization for DAOS containers to and from a POSIX filesystem is provided with daos-serialize and daos-deserialize. Details and usage examples are provided in [DAOS Support](DAOS-Support.md).
  
 ## Contributors
-We welcome contributions to the project.  For details on how to help, see our [Contributor Guide](.github/CONTRIBUTING.md)
+We welcome contributions to the project.  For details on how to help, see our [Contributor Guide](CONTRIBUTING.md)
 
 ### Copyrights
 
diff --git a/cmake/FindCART.cmake b/cmake/FindCART.cmake
deleted file mode 100755
index 216c146..0000000
--- a/cmake/FindCART.cmake
+++ /dev/null
@@ -1,38 +0,0 @@
-# - Try to find cart libs 
-# Once done this will define
-#  cart_FOUND - System has libcart
-#  cart_INCLUDE_DIRS - cart include directories
-#  cart_LIBRARIES - The libraries needed to use cart
-#  gurt_LIBRARIES - The libraries needed to use gurt (part of cart)
-
-FIND_PATH(WITH_CART_PREFIX
-    NAMES cart/include
-)
-
-FIND_LIBRARY(CART_LIBRARIES
-    NAMES cart
-    HINTS ${WITH_CART_PREFIX}/lib
-)
-
-FIND_LIBRARY(GURT_LIBRARIES
-    NAMES gurt
-    HINTS ${WITH_CART_PREFIX}/lib
-)
-
-FIND_PATH(CART_INCLUDE_DIRS
-    NAMES cart/types.h
-    HINTS ${WITH_CART_PREFIX}/include
-)
-
-INCLUDE(FindPackageHandleStandardArgs)
-FIND_PACKAGE_HANDLE_STANDARD_ARGS(CART DEFAULT_MSG
-    CART_LIBRARIES
-    CART_INCLUDE_DIRS
-)
-
-# Hide these vars from ccmake GUI
-MARK_AS_ADVANCED(
-	CART_LIBRARIES
-	GURT_LIBRARIES
-	CART_INCLUDE_DIRS
-)
diff --git a/cmake/FindDAOS.cmake b/cmake/FindDAOS.cmake
index c06ba2c..9646aa0 100755
--- a/cmake/FindDAOS.cmake
+++ b/cmake/FindDAOS.cmake
@@ -3,6 +3,8 @@
 #  daos_FOUND - System has libdaos
 #  daos_INCLUDE_DIRS - daos.h
 #  daos_LIBRARIES - libdaos
+#  GURT_LIBRARIES - libgurt
+#  CART_LIBRARIES - libcart
 #  DUNS_LIBRARIES - libduns 
 #  DFS_LIBRARIES - libdfs 
 #  COMMON_LIBRARIES - libdaos_common
@@ -16,6 +18,16 @@ FIND_LIBRARY(DAOS_LIBRARIES
     HINTS ${WITH_DAOS_PREFIX}/lib
 )
 
+FIND_LIBRARY(GURT_LIBRARIES
+    NAMES gurt 
+    HINTS ${WITH_DAOS_PREFIX}/lib
+)
+
+FIND_LIBRARY(CART_LIBRARIES
+    NAMES cart 
+    HINTS ${WITH_DAOS_PREFIX}/lib
+)
+
 FIND_LIBRARY(DUNS_LIBRARIES
     NAMES duns
     HINTS ${WITH_DAOS_PREFIX}/lib
@@ -39,6 +51,8 @@ FIND_PATH(DAOS_INCLUDE_DIRS
 INCLUDE(FindPackageHandleStandardArgs)
 FIND_PACKAGE_HANDLE_STANDARD_ARGS(DAOS DEFAULT_MSG
     DAOS_LIBRARIES
+    CART_LIBRARIES
+    GURT_LIBRARIES
     DUNS_LIBRARIES
     DFS_LIBRARIES
     COMMON_LIBRARIES
@@ -48,6 +62,8 @@ FIND_PACKAGE_HANDLE_STANDARD_ARGS(DAOS DEFAULT_MSG
 # Hide these vars from ccmake GUI
 MARK_AS_ADVANCED(
         DAOS_LIBRARIES
+        CART_LIBRARIES
+        GURT_LIBRARIES
         DUNS_LIBRARIES
         DFS_LIBRARIES
         COMMON_LIBRARIES
diff --git a/cmake/FindHDF5.cmake b/cmake/FindHDF5.cmake
new file mode 100644
index 0000000..ee9e0a1
--- /dev/null
+++ b/cmake/FindHDF5.cmake
@@ -0,0 +1,38 @@
+# - Try to find hdf5
+# Once done this will define
+#  HDF5_FOUND - System has hdf5
+#  HDF5_INCLUDE_DIRS - The hdf5 include directories
+#  HDF5_LIBRARIES - The libraries needed to use hdf5
+
+FIND_PATH(WITH_HDF5_PREFIX
+    NAMES include/hdf5.h
+)
+
+FIND_LIBRARY(HDF5_LIBRARIES
+    NAMES hdf5 
+    HINTS ${WITH_HDF5_PREFIX}/lib
+)
+
+FIND_PATH(HDF5_INCLUDE_DIRS
+    NAMES hdf5.h
+    HINTS ${WITH_HDF5_PREFIX}/include
+)
+
+FIND_PATH(HDF5_BIN_DIR
+    NAMES h5cc h5pcc
+    HINTS ${WITH_HDF5_PREFIX}/bin
+)
+
+INCLUDE(FindPackageHandleStandardArgs)
+FIND_PACKAGE_HANDLE_STANDARD_ARGS(HDF5 DEFAULT_MSG
+    HDF5_LIBRARIES
+    HDF5_INCLUDE_DIRS
+    HDF5_BIN_DIR
+)
+
+# Hide these vars from ccmake GUI
+MARK_AS_ADVANCED(
+    HDF5_LIBRARIES
+    HDF5_INCLUDE_DIRS
+    HDF5_BIN_DIR
+)
diff --git a/cmake/FindLibAttr.cmake b/cmake/FindLibAttr.cmake
new file mode 100644
index 0000000..b5ca36b
--- /dev/null
+++ b/cmake/FindLibAttr.cmake
@@ -0,0 +1,25 @@
+# - Try to find libattr
+# Once done this will define
+#  LibAttr_FOUND - System has libattr
+#  LibAttr_INCLUDE_DIRS - The libattr include directories
+#  LibAttr_LIBRARIES - The libraries needed to use libattr
+
+FIND_LIBRARY(LibAttr_LIBRARIES
+    NAMES attr
+)
+
+FIND_PATH(LibAttr_INCLUDE_DIRS
+    NAMES attr/libattr.h
+)
+
+INCLUDE(FindPackageHandleStandardArgs)
+FIND_PACKAGE_HANDLE_STANDARD_ARGS(LibAttr DEFAULT_MSG
+    LibAttr_LIBRARIES
+    LibAttr_INCLUDE_DIRS
+)
+
+# Hide these vars from ccmake GUI
+MARK_AS_ADVANCED(
+	LibAttr_LIBRARIES
+	LibAttr_INCLUDE_DIRS
+)
diff --git a/dist/CMakeLists.txt b/dist/CMakeLists.txt
new file mode 100644
index 0000000..3b6c581
--- /dev/null
+++ b/dist/CMakeLists.txt
@@ -0,0 +1,327 @@
+PROJECT(MFU)
+
+CMAKE_MINIMUM_REQUIRED(VERSION 3.1)
+
+IF(POLICY CMP0042)
+  CMAKE_POLICY(SET CMP0042 NEW)
+ENDIF(POLICY CMP0042)
+SET(CMAKE_MACOSX_RPATH ON)
+SET(CMAKE_POSITION_INDEPENDENT_CODE ON)
+
+LIST(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/cmake")
+
+# Configuration Options
+
+OPTION(ENABLE_XATTRS "Enable code for extended attributes" ON)
+MESSAGE(STATUS "ENABLE_XATTRS: ${ENABLE_XATTRS}")
+IF(ENABLE_XATTRS)
+  ADD_DEFINITIONS(-DDCOPY_USE_XATTRS)
+
+  FIND_PACKAGE(LibAttr REQUIRED)
+  IF(LibAttr_FOUND)
+    ADD_DEFINITIONS(-DHAVE_LIBATTR)
+    INCLUDE_DIRECTORIES(${LibAttr_INCLUDE_DIRS})
+    LIST(APPEND MFU_EXTERNAL_LIBS ${LibAttr_LIBRARIES})
+  ENDIF(LibAttr_FOUND)
+ENDIF(ENABLE_XATTRS)
+
+OPTION(ENABLE_LUSTRE "Enable optimization and features for Lustre" OFF)
+MESSAGE(STATUS "ENABLE_LUSTRE: ${ENABLE_LUSTRE}")
+IF(ENABLE_LUSTRE)
+  ADD_DEFINITIONS(-DLUSTRE_SUPPORT)
+
+  FIND_LIBRARY(LUSTREAPI lustreapi)
+  IF(LUSTREAPI)
+#  INCLUDE_DIRECTORIES(${LUSTREAPI_INCLUDE_DIRS})
+    LIST(APPEND MFU_EXTERNAL_LIBS ${LUSTREAPI})
+  ENDIF(LUSTREAPI)
+
+  INCLUDE(CheckLibraryExists)
+
+  CHECK_LIBRARY_EXISTS(lustreapi llapi_layout_alloc ${LUSTREAPI} HAVE_LLAPI_LAYOUT)
+  IF(HAVE_LLAPI_LAYOUT)
+    ADD_DEFINITIONS(-DHAVE_LLAPI_LAYOUT)
+  ENDIF(HAVE_LLAPI_LAYOUT)
+
+  CHECK_LIBRARY_EXISTS(lustreapi llapi_file_create ${LUSTREAPI} HAVE_LLAPI_FILE_CREATE)
+  IF(HAVE_LLAPI_FILE_CREATE)
+    ADD_DEFINITIONS(-DHAVE_LLAPI_FILE_CREATE)
+  ENDIF(HAVE_LLAPI_FILE_CREATE)
+
+  CHECK_LIBRARY_EXISTS(lustreapi llapi_file_get_stripe ${LUSTREAPI} HAVE_LLAPI_FILE_GET_STRIPE)
+  IF(HAVE_LLAPI_FILE_GET_STRIPE)
+    ADD_DEFINITIONS(-DHAVE_LLAPI_FILE_GET_STRIPE)
+  ENDIF(HAVE_LLAPI_FILE_GET_STRIPE)
+
+  # todo investigate usage of other lustre #defs
+  # - LUSTRE_STAT
+ENDIF(ENABLE_LUSTRE)
+
+OPTION(ENABLE_GPFS "Enable GFPS/Spectrum Scale support")
+MESSAGE(STATUS "ENABLE_GPFS: ${ENABLE_GPFS}")
+IF(ENABLE_GPFS)
+  FIND_PACKAGE(GPFS REQUIRED)
+  INCLUDE_DIRECTORIES(${GPFS_INCLUDE_DIRS})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${GPFS_LIBRARIES})
+  ADD_DEFINITIONS(-DGPFS_SUPPORT)
+ENDIF(ENABLE_GPFS)
+
+OPTION(ENABLE_EXPERIMENTAL "Build experimental tools" OFF)
+MESSAGE(STATUS "ENABLE_EXPERIMENTAL: ${ENABLE_EXPERIMENTAL}")
+
+## HEADERS
+INCLUDE(CheckIncludeFile)
+CHECK_INCLUDE_FILE(byteswap.h HAVE_BYTESWAP_H)
+if(HAVE_BYTESWAP_H)
+  ADD_DEFINITIONS(-DHAVE_BYTESWAP_H)
+ELSE(HAVE_BYTESWAP_H)
+  MESSAGE(SEND_ERROR "byteswap.h is required")
+ENDIF(HAVE_BYTESWAP_H)
+
+# Dependencies
+
+## MPI
+INCLUDE(SetupMPI)
+INCLUDE_DIRECTORIES(${MPI_C_INCLUDE_PATH})
+LIST(APPEND MFU_EXTERNAL_LIBS ${MPI_C_LIBRARIES})
+
+## LIBARCHIVE
+OPTION(ENABLE_LIBARCHIVE "Enable usage of libarchive and corresponding tools" ON)
+MESSAGE(STATUS "ENABLE_LIBARCHIVE: ${ENABLE_LIBARCHIVE}")
+# TODO how would we pass a version from spack?
+# libarchive 3.1.2 is available on some systems,
+# but pick a newer libarchive to avoid bug with files starting with "._",
+# which is misinterpretted as a MacOS extension on Linuxlibarchive 3.1.2
+IF(ENABLE_LIBARCHIVE)
+  FIND_PACKAGE(LibArchive 3.5.1 REQUIRED)
+  INCLUDE_DIRECTORIES(${LibArchive_INCLUDE_DIRS})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${LibArchive_LIBRARIES})
+  ADD_DEFINITIONS(-DLIBARCHIVE_SUPPORT)
+ENDIF(ENABLE_LIBARCHIVE)
+
+## hdf5
+OPTION(ENABLE_HDF5 "Enable HDF5 library")
+MESSAGE(STATUS "ENABLE_HDF5: ${ENABLE_HDF5}")
+IF(ENABLE_HDF5)
+  FIND_PACKAGE(HDF5 REQUIRED)
+  INCLUDE_DIRECTORIES(${HDF5_INCLUDE_DIRS})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${HDF5_LIBRARIES})
+  ADD_DEFINITIONS(-DHDF5_SUPPORT)
+  ADD_DEFINITIONS(-DHDF5_BIN_DIR=\"${HDF5_BIN_DIR}\")
+ENDIF(ENABLE_HDF5)
+
+OPTION(ENABLE_DAOS "Enable DAOS support")
+MESSAGE(STATUS "ENABLE_DAOS: ${ENABLE_DAOS}")
+IF(ENABLE_DAOS)
+  SET(CMAKE_EXE_LINKER_FLAGS -luuid)
+  FIND_PACKAGE(DAOS REQUIRED)
+  INCLUDE_DIRECTORIES(${DAOS_INCLUDE_DIRS})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${DAOS_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${GURT_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${CART_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${DUNS_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${DFS_LIBRARIES})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${DAOS_COMMON_LIBRARIES})
+  ADD_DEFINITIONS(-DDAOS_SUPPORT)
+ENDIF(ENABLE_DAOS)
+
+## BZip2
+FIND_PACKAGE(BZip2 REQUIRED)
+LIST(APPEND MFU_EXTERNAL_LIBS ${BZIP2_LIBRARIES})
+
+## libcap for checks on linux capabilities
+FIND_PACKAGE(LibCap)
+IF(LibCap_FOUND)
+  ADD_DEFINITIONS(-DHAVE_LIBCAP)
+  INCLUDE_DIRECTORIES(${LibCap_INCLUDE_DIRS})
+  LIST(APPEND MFU_EXTERNAL_LIBS ${LibCap_LIBRARIES})
+ENDIF(LibCap_FOUND)
+
+## OPENSSL for ddup
+FIND_PACKAGE(OpenSSL)
+
+# Setup Installation
+
+INCLUDE(GNUInstallDirs)
+SET(X_BINDIR ${CMAKE_INSTALL_FULL_BINDIR} CACHE INTERNAL "bin")
+SET(X_DATADIR ${CMAKE_INSTALL_FULL_DATADIR} CACHE INTERNAL "share")
+SET(X_INCLUDEDIR ${CMAKE_INSTALL_FULL_INCLUDEDIR} CACHE INTERNAL "include")
+SET(X_LIBDIR ${CMAKE_INSTALL_FULL_LIBDIR} CACHE INTERNAL "lib")
+
+############
+# This sets an rpath to buildtime libraries in build directory
+# and rewrites the rpath to the install location during install
+# these lines must come before add_library and add_executable macros
+############
+
+# https://gitlab.kitware.com/cmake/community/wikis/doc/cmake/RPATH-handling
+# use, i.e. don't skip the full RPATH for the build tree
+SET(CMAKE_SKIP_BUILD_RPATH  FALSE)
+
+# when building, don't use the install RPATH already
+# (but later on when installing)
+SET(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)
+
+SET(CMAKE_INSTALL_RPATH "${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}")
+
+# add the automatically determined parts of the RPATH
+# which point to directories outside the build tree to the install RPATH
+SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
+
+# the RPATH to be used when installing, but only if it's not a system directory
+LIST(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES "${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}" isSystemDir)
+IF("${isSystemDir}" STREQUAL "-1")
+   SET(CMAKE_INSTALL_RPATH "${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}")
+ENDIF("${isSystemDir}" STREQUAL "-1")
+
+############
+# End rpath stuff
+############
+
+# Subdirectories
+INCLUDE(MFU_ADD_TOOL)
+INCLUDE_DIRECTORIES(
+  ${CMAKE_CURRENT_SOURCE_DIR}/lwgrp/src
+  ${CMAKE_CURRENT_SOURCE_DIR}/dtcmp/src
+  ${CMAKE_CURRENT_SOURCE_DIR}/libcircle/libcircle
+  ${CMAKE_CURRENT_SOURCE_DIR}/mpifileutils/src/common
+)
+
+ADD_SUBDIRECTORY(mpifileutils/src)
+ADD_SUBDIRECTORY(mpifileutils/test)
+ADD_SUBDIRECTORY(mpifileutils/man)
+
+# Version for the shared mfu library
+set(MFU_VERSION_MAJOR 4) # Incompatible API changes
+set(MFU_VERSION_MINOR 0) # Backwards-compatible functionality
+set(MFU_VERSION_PATCH 0) # Backwards-compatible fixes
+set(MFU_VERSION ${MFU_VERSION_MAJOR}.${MFU_VERSION_MINOR}.${MFU_VERSION_PATCH})
+
+LIST(APPEND lwgrp_srcs
+    lwgrp/src/lwgrp.c
+    lwgrp/src/lwgrp_util.c
+    lwgrp/src/lwgrp_chain_ops.c
+    lwgrp/src/lwgrp_ring_ops.c
+    lwgrp/src/lwgrp_logchain_ops.c
+    lwgrp/src/lwgrp_logring_ops.c
+    lwgrp/src/lwgrp_comm.c
+    lwgrp/src/lwgrp_comm_split.c
+)
+
+LIST(APPEND dtcmp_srcs
+    dtcmp/src/dtcmp.c
+    dtcmp/src/dtcmp_util.c
+    dtcmp/src/dtcmp_ops.c
+    dtcmp/src/dtcmp_uniqify.c
+    dtcmp/src/dtcmp_search_binary.c
+    dtcmp/src/dtcmp_partitionz.c
+    dtcmp/src/dtcmp_partitionz_list.c
+    dtcmp/src/dtcmp_partition_local.c
+    dtcmp/src/dtcmp_merge_2way.c
+    dtcmp/src/dtcmp_merge_kway_heap.c
+    dtcmp/src/dtcmp_select_local_ends.c
+    dtcmp/src/dtcmp_select_local_randpartition.c
+    dtcmp/src/dtcmp_selectv_rand.c
+    dtcmp/src/dtcmp_selectv_medianofmedians.c
+    dtcmp/src/dtcmp_is_sorted.c
+    dtcmp/src/dtcmp_sort_local_insertionsort.c
+    dtcmp/src/dtcmp_sort_local_randquicksort.c
+    dtcmp/src/dtcmp_sort_local_mergesort.c
+    dtcmp/src/dtcmp_sort_local_qsort.c
+    dtcmp/src/dtcmp_sort_allgather.c
+    dtcmp/src/dtcmp_sort_bitonic.c
+    dtcmp/src/dtcmp_sort_samplesort.c
+    dtcmp/src/dtcmp_sortv_allgather.c
+    dtcmp/src/dtcmp_sortv_sortgather_scatter.c
+    dtcmp/src/dtcmp_sortv_cheng.c
+    dtcmp/src/dtcmp_sortz_samplesort.c
+    dtcmp/src/dtcmp_rankv_sort.c
+    dtcmp/src/dtcmp_seg_exscan.c
+)
+
+LIST(APPEND libcircle_srcs
+    libcircle/libcircle/lib.c
+    libcircle/libcircle/queue.c
+    libcircle/libcircle/token.c
+    libcircle/libcircle/worker.c
+)
+
+# todo re-asses if all of these must be *installed*
+LIST(APPEND libmfu_install_headers
+  mpifileutils/src/common/mfu.h
+  mpifileutils/src/common/mfu_errors.h
+  mpifileutils/src/common/mfu_bz2.h
+  mpifileutils/src/common/mfu_flist.h
+  mpifileutils/src/common/mfu_flist_internal.h
+  mpifileutils/src/common/mfu_io.h
+  mpifileutils/src/common/mfu_param_path.h
+  mpifileutils/src/common/mfu_path.h
+  mpifileutils/src/common/mfu_pred.h
+  mpifileutils/src/common/mfu_progress.h
+  mpifileutils/src/common/mfu_util.h
+  )
+if(ENABLE_DAOS)
+  LIST(APPEND libmfu_install_headers
+    mpifileutils/src/common/mfu_daos.h
+    )
+ENDIF(ENABLE_DAOS)
+
+INSTALL(FILES ${libmfu_install_headers} DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})
+
+# common library
+LIST(APPEND libmfu_srcs
+  mpifileutils/src/common/mfu_bz2.c
+  mpifileutils/src/common/mfu_bz2_static.c
+  mpifileutils/src/common/mfu_compress_bz2_libcircle.c
+  mpifileutils/src/common/mfu_decompress_bz2_libcircle.c
+  mpifileutils/src/common/mfu_flist.c
+  mpifileutils/src/common/mfu_flist_chunk.c
+  mpifileutils/src/common/mfu_flist_copy.c
+  mpifileutils/src/common/mfu_flist_io.c
+  mpifileutils/src/common/mfu_flist_chmod.c
+  mpifileutils/src/common/mfu_flist_create.c
+  mpifileutils/src/common/mfu_flist_remove.c
+  mpifileutils/src/common/mfu_flist_sort.c
+  mpifileutils/src/common/mfu_flist_usrgrp.c
+  mpifileutils/src/common/mfu_flist_walk.c
+  mpifileutils/src/common/mfu_io.c
+  mpifileutils/src/common/mfu_param_path.c
+  mpifileutils/src/common/mfu_path.c
+  mpifileutils/src/common/mfu_pred.c
+  mpifileutils/src/common/mfu_progress.c
+  mpifileutils/src/common/mfu_util.c
+  mpifileutils/src/common/strmap.c
+  ${lwgrp_srcs}
+  ${dtcmp_srcs}
+  ${libcircle_srcs}
+  )
+IF(ENABLE_LIBARCHIVE)
+  LIST(APPEND libmfu_srcs
+    mpifileutils/src/common/mfu_flist_archive.c
+    )
+ENDIF(ENABLE_LIBARCHIVE)
+IF(ENABLE_DAOS)
+  LIST(APPEND libmfu_srcs
+    mpifileutils/src/common/mfu_daos.c
+    )
+ENDIF(ENABLE_DAOS)
+
+ADD_LIBRARY(mfu_o OBJECT ${libmfu_srcs})
+SET_TARGET_PROPERTIES(mfu_o PROPERTIES C_STANDARD 99)
+
+ADD_LIBRARY(mfu SHARED $<TARGET_OBJECTS:mfu_o>)
+TARGET_LINK_LIBRARIES(mfu LINK_PUBLIC ${MFU_EXTERNAL_LIBS})
+SET_TARGET_PROPERTIES(mfu PROPERTIES VERSION ${MFU_VERSION} OUTPUT_NAME mfu CLEAN_DIRECT_OUTPUT 1)
+INSTALL(TARGETS mfu DESTINATION ${CMAKE_INSTALL_LIBDIR})
+
+ADD_LIBRARY(mfu-static STATIC $<TARGET_OBJECTS:mfu_o>)
+TARGET_LINK_LIBRARIES(mfu-static LINK_PUBLIC ${MFU_EXTERNAL_LIBS})
+SET_TARGET_PROPERTIES(mfu-static PROPERTIES OUTPUT_NAME mfu CLEAN_DIRECT_OUTPUT 1)
+INSTALL(TARGETS mfu-static DESTINATION ${CMAKE_INSTALL_LIBDIR})
+
+# some projects require a "make install" command to work,
+# so define at least a basic INSTALL function
+INSTALL(FILES lwgrp/README lwgrp/LICENSE.TXT DESTINATION share/lwgrp)
+INSTALL(FILES dtcmp/README.md dtcmp/LICENSE.TXT DESTINATION share/dtcmp)
+INSTALL(FILES libcircle/COPYING DESTINATION share/libcircle)
+INSTALL(FILES mpifileutils/LICENSE mpifileutils/NOTICE DESTINATION share/mpifileutils)
diff --git a/dist/README.dist b/dist/README.dist
new file mode 100644
index 0000000..76b42ec
--- /dev/null
+++ b/dist/README.dist
@@ -0,0 +1,24 @@
+###########################################
+mpiFileUtils
+###########################################
+
+This is a release package of mpiFileUtils.
+It contains source for mpiFileUtils and several of its dependencies,
+including LWGRP, DTCMP, and libcircle.
+All included source files are compiled into a single library.
+
+For documentation, refer to:
+
+    https://mpifileutils.readthedocs.io
+
+For a simple build, one may run the following commands:
+
+    mkdir build
+    cd build
+    cmake -DCMAKE_INSTALL_PREFIX=../install ..
+    make -j install
+
+For details on available build flags,
+refer to the "Build" section of the documentation:
+
+    https://mpifileutils.readthedocs.io/en/latest/build.html
diff --git a/dist/README.md b/dist/README.md
new file mode 100644
index 0000000..c44204a
--- /dev/null
+++ b/dist/README.md
@@ -0,0 +1,19 @@
+# mpiFileUtils release tarball
+The builddist script creates an mpiFileUtils release tarball.
+
+```bash
+    ./builddist main
+```
+
+This tarball is added as a binary attachment to the corresponding mpiFileUtils release page.
+This contains source for mpiFileUtils, LWGRP, DTCMP, and libcircle.
+It also contains a set of top-level CMake files that compiles all source files into a single libmfu library.
+
+# Steps to add a new release
+To add a new release:
+1. Define new CMake files if needed (TODO: support multiple versions of top-level CMake files)
+2. Edit builddist to define the appropriate tags for all packages
+3. Run builddist for appropriate tag
+4. Test build and run with resulting tarball
+5. Attach tarball to github release page (attach binary)
+6. Update mpiFileUtils readthedocs to describe builds from this tarball
diff --git a/dist/builddist b/dist/builddist
new file mode 100755
index 0000000..3b31b82
--- /dev/null
+++ b/dist/builddist
@@ -0,0 +1,124 @@
+#!/bin/bash
+
+print_usage() {
+    echo "Usage: builddist <tag>"
+    echo ""
+    echo "Tags:"
+    echo "  main    - build tarball of latest"
+    echo "  v0.11.1 - build tarball of v0.11.1"
+}
+
+# check that we got an argument or print usage
+if [ $# -ne 1 ] ; then
+    print_usage
+    exit 1
+fi
+
+# for a given release, define tags for each component
+if [ "$1" == "main" ] ; then
+    # to build from latest branch of all repos
+    ORGS=(
+        "lwgrp"        "llnl" "main"
+        "dtcmp"        "llnl" "main"
+        "libcircle"    "hpc"  "master"
+        "mpifileutils" "hpc"  "master"
+    )
+elif [ "$1" == "v0.11.1" ] ; then
+    # to build from latest branch of all repos
+    ORGS=(
+        "lwgrp"        "llnl" "v1.0.4"
+        "dtcmp"        "llnl" "v1.1.4"
+        "libcircle"    "hpc"  "v0.3"
+        "mpifileutils" "hpc"  "v0.11.1"
+    )
+else
+    echo "Error: unknown tag: $1"
+    echo ""
+    print_usage
+    exit 1
+fi
+
+set -x
+
+# we assume everything is hosted at github
+REPOHOST=https://github.com
+
+# create a temporary directory to package things up
+rm -rf dist
+mkdir dist
+cd dist
+
+ARCH_DIR="archive"
+rm -rf $ARCH_DIR
+mkdir -p $ARCH_DIR
+
+len=${#ORGS[@]}
+for (( i=0; i<${len}; i=$(($i + 3)) )); do
+    # component name
+    component=${ORGS[$i]}
+
+    # github path to component
+    j=$(($i + 1))
+    repo=$REPOHOST/${ORGS[$j]}/$component
+
+    # repo tag to checkout
+    j=$(($i + 2))
+    TAG=${ORGS[$j]}
+
+    # clone the repo
+    git clone --depth 1 --branch $TAG $repo
+
+    # git archive the source files into a tarfile
+    cd $component
+        #TAG=`git describe --tags $(git rev-list --tags --max-count=1)`
+        git archive --format=tar --prefix=$component/ $TAG | gzip > $component-$TAG.tar.gz 2> /dev/null
+    cd ..
+
+    # unpack source files for this component in a directory with other components
+    cd $ARCH_DIR
+        tar -zxf ../$component/$component-$TAG.tar.gz
+
+        # hack out include of autotools config.h (not used anyway)
+        if [ "$component" == "lwgrp" ] ; then
+            sed -i 's@#include "../config/config.h"@@g' lwgrp/src/lwgrp_internal.h
+        fi
+        if [ "$component" == "libcircle" ] ; then
+            sed -i 's@#include <config.h>@@g' libcircle/libcircle/lib.h
+        fi
+
+        # hack out common dir for library (maybe could leave this in)
+        if [ "$component" == "mpifileutils" ] ; then
+            sed -i 's@ADD_SUBDIRECTORY(common)@#ADD_SUBDIRECTORY(common)@g' mpifileutils/src/CMakeLists.txt
+        fi
+
+        # remove doc and test directories for a smaller tarball
+        rm -rf ${component}/doc
+        rm -rf ${component}/doc-dev
+        if [ "$component" != "mpifileutils" ] ; then
+          rm -rf ${component}/test
+        fi
+    cd ..
+done
+
+# NOTE: last TAG is from SCR
+# rename archive directory to mpifileutils-TAG
+mv $ARCH_DIR mpifileutils-$TAG
+
+# copy in top-level CMake files
+cp -r ../CMakeLists.txt ../../cmake mpifileutils-$TAG
+
+# copy in README
+cp ../README.dist mpifileutils-$TAG/README
+
+# drop FindDTCMP and FindLWGRP cmake files
+#rm -f mpifileutils-$TAG/cmake/{FindDTCMP.cmake,FindLibCircle.cmake}
+
+# delete original CMakeLists to avoid confusion
+rm -rf mpifileutils-$TAG/mpifileutils/CMakeLists.txt mpifileutils-$TAG/mpifileutils/cmake
+
+# zip up release tarball
+tar -czf ../mpifileutils-${TAG}.tgz mpifileutils-$TAG
+
+# delete prep directory
+cd ..
+rm -rf dist
diff --git a/doc/rst/build.rst b/doc/rst/build.rst
index 6c8d277..047ba87 100644
--- a/doc/rst/build.rst
+++ b/doc/rst/build.rst
@@ -2,15 +2,102 @@
 Build
 ==============================
 
-mpiFileUtils and its dependencies can be installed with and without Spack.
-There are several common variations described here:
+mpiFileUtils and its dependencies can be installed with CMake or Spack.
+Several build variations are described in this section:
 
-- install both mpiFileUtils and its dependencies with Spack
-- install both mpiFileUtils and its dependencies directly
-- install mpiFileUtis directly after installing its dependencies with Spack
+- CMake
+- Spack
+- development build with CMake
+- development build with Spack
+
+-------------------------
+CMake
+-------------------------
+
+mpiFileUtils requires CMake 3.1 or higher.
+Before running cmake, ensure that the MPI wrapper scripts like mpicc are loaded in your environment.
+
+The simplest mpiFileUtils install for many users is to build from a release package.
+This packages the source for a specific version of mpiFileUtils along with the corresponding source for several of its dependencies in a single tarball.
+mpiFileUtils release packages are available as attachments from their respective GitHub Releases page:
+
+https://github.com/hpc/mpifileutils/releases
+
+mpiFileUtils optionally depends on libarchive, version 3.5.1.
+If new enough, the system install of libarchive may be sufficient,
+though even newer versions may be incompatible with the required version.
+To be certain of compatibility, it is recommended that one install libarchive-3.5.1 with commands like the following
+
+.. code-block:: Bash
+
+   #!/bin/bash
+   mkdir install
+   installdir=`pwd`/install
+
+   wget https://github.com/libarchive/libarchive/releases/download/3.5.1/libarchive-3.5.1.tar.gz
+   tar -zxf libarchive-3.5.1.tar.gz
+   cd libarchive-3.5.1
+     ./configure --prefix=$installdir
+     make install
+   cd ..
+
+To build on PowerPC, one may need to add :code:`--build=powerpc64le-redhat-linux-gnu` to the configure command.
+
+Assuming libarchive has been installed to an `install` directory as shown above,
+one can then build mpiFileUtils from a release like v0.11.1 with commands like the following:
+
+.. code-block:: Bash
+
+   wget https://github.com/hpc/mpifileutils/releases/download/v0.11.1/mpifileutils-v0.11.1.tgz
+   tar -zxf mpifileutils-v0.11.1.tgz
+   cd mpifileutils-v0.11.1
+     mkdir build
+     cd build
+       cmake .. \
+         -DWITH_LibArchive_PREFIX=../../install \
+         -DCMAKE_INSTALL_PREFIX=../../install
+       make -j install
+     cd ..
+   cd ..
+
+Additional CMake options:
+
+* :code:`-DENABLE_LIBARCHIVE=[ON/OFF]` : use libarchive and build tools requiring libarchive like dtar, defaults to :code:`ON`
+* :code:`-DENABLE_XATTRS=[ON/OFF]` : use extended attributes and libattr, defaults to :code:`ON`
+* :code:`-DENABLE_LUSTRE=[ON/OFF]` : specialization for Lustre, defaults to :code:`OFF`
+* :code:`-DENABLE_GPFS=[ON/OFF]` : specialization for GPFS, defaults to :code:`OFF`
+* :code:`-DENABLE_EXPERIMENTAL=[ON/OFF]` : build experimental tools, defaults to :code:`OFF`
+
+-------------------------------------------
+DAOS support
+-------------------------------------------
+
+To build with DAOS support, first install mpiFileUtils dependencies as mentioned above,
+and also make sure DAOS is installed. If DAOS is installed under a standard
+system path then specifying the DAOS path with :code:`-DWITH_DAOS_PREFIX` is unnecessary.
+
+.. code-block:: Bash
+
+   cmake ../mpifileutils \
+     -DCMAKE_INSTALL_PREFIX=../install \
+     -DWITH_DAOS_PREFIX=</path/to/daos/> \
+     -DENABLE_DAOS=ON
+   make -j install
+
+Some DAOS-enabled tools require HDF5.
+To use the `daos-serialize` and `daos-deserialize` tools, HDF5 1.2+ is required.
+To copy HDF5 containers with `dcp`, HDF5 1.8+ is required, along with the daos-vol.
+
+To build with HDF5 support, add the following flags during CMake.
+If HDF5 is installed under a standard system path then specifying the HDF5 path with :code:`-DWITH_HDF5_PREFIX` is unnecessary.
+
+.. code-block:: Bash
+
+   -DENABLE_HDF5=ON \
+   -DWITH_HDF5_PREFIX=</path/to/hdf5>
 
 ---------------------------
-Build everything with Spack
+Spack
 ---------------------------
 
 To use `Spack <https://github.com/spack/spack>`_, it is recommended that one first create a `packages.yaml` file to list system-provided packages, like MPI.
@@ -29,13 +116,13 @@ or to enable all features:
 
     spack install mpifileutils +lustre +gpfs +experimental
 
--------------------------
-Build everything directly
--------------------------
+----------------------------
+Development build with CMake
+----------------------------
 
-To build directly, mpiFileUtils requires CMake 3.1 or higher.
-First ensure MPI wrapper scripts like mpicc are loaded in your environment.
-Then to install the dependencies, run the following commands:
+To make changes to mpiFileUtils, one may wish to build from a clone of the repository.
+This requires that one installs the mpiFileUtils dependencies separately,
+which can be done with the following commands:
 
 .. code-block:: Bash
 
@@ -46,24 +133,24 @@ Then to install the dependencies, run the following commands:
    mkdir deps
    cd deps
      wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz
-     wget https://github.com/llnl/lwgrp/releases/download/v1.0.3/lwgrp-1.0.3.tar.gz
-     wget https://github.com/llnl/dtcmp/releases/download/v1.1.1/dtcmp-1.1.1.tar.gz
+     wget https://github.com/llnl/lwgrp/releases/download/v1.0.4/lwgrp-1.0.4.tar.gz
+     wget https://github.com/llnl/dtcmp/releases/download/v1.1.4/dtcmp-1.1.4.tar.gz
      wget https://github.com/libarchive/libarchive/releases/download/3.5.1/libarchive-3.5.1.tar.gz
-     
+
      tar -zxf libcircle-0.3.0.tar.gz
      cd libcircle-0.3.0
        ./configure --prefix=$installdir
        make install
      cd ..
-     
-     tar -zxf lwgrp-1.0.3.tar.gz
-     cd lwgrp-1.0.3
+
+     tar -zxf lwgrp-1.0.4.tar.gz
+     cd lwgrp-1.0.4
        ./configure --prefix=$installdir
        make install
      cd ..
-     
-     tar -zxf dtcmp-1.1.1.tar.gz
-     cd dtcmp-1.1.1
+
+     tar -zxf dtcmp-1.1.4.tar.gz
+     cd dtcmp-1.1.4
        ./configure --prefix=$installdir --with-lwgrp=$installdir
        make install
      cd ..
@@ -75,78 +162,30 @@ Then to install the dependencies, run the following commands:
      cd ..
    cd ..
 
-To build on PowerPC, one may need to add :code:`--build=powerpc64le-redhat-linux-gnu`
-to the configure commands.
-
-Assuming the dependencies have been placed in
-an `install` directory as shown above, build mpiFileUtils from a release like v0.10:
+One can then clone, build, and install mpiFileUtils:
 
 .. code-block:: Bash
 
-   wget https://github.com/hpc/mpifileutils/archive/v0.10.tar.gz
-   tar -zxf v0.10.tar.gz
-   mkdir build install
-   cd build
-   cmake ../mpifileutils-0.10 \
-     -DWITH_DTCMP_PREFIX=../install \
-     -DWITH_LibCircle_PREFIX=../install \
-     -DCMAKE_INSTALL_PREFIX=../install
-   make install
-
-or to build the latest mpiFileUtils from the master branch:
-
-.. code-block:: Bash
-
-   git clone --depth 1 https://github.com/hpc/mpifileutils
-   mkdir build install
+   git clone https://github.com/hpc/mpifileutils
+   mkdir build
    cd build
    cmake ../mpifileutils \
      -DWITH_DTCMP_PREFIX=../install \
      -DWITH_LibCircle_PREFIX=../install \
+     -DWITH_LibArchive_PREFIX=../install \
      -DCMAKE_INSTALL_PREFIX=../install
-   make install
-
-build latest mpiFileUtils from the master branch with DAOS Support:
-
-.. code-block:: Bash
-
-   git clone --depth 1 https://github.com/hpc/mpifileutils
-   mkdir build install
-   cd build
-   cmake ../mpifileutils \
-     -DWITH_DTCMP_PREFIX=../install \
-     -DWITH_LibCircle_PREFIX=../install \
-     -DCMAKE_INSTALL_PREFIX=../install \
-     -DWITH_CART_PREFIX=</path/to/daos/> \
-     -DWITH_DAOS_PREFIX=</path/to/daos/> \
-     -DENABLE_DAOS=ON
-   make install
-
-The above build with DAOS option also assumes you have already installed DAOS. If
-CART and DAOS are installed under a standard system path then specifying the CART
-and DAOS paths is unnecessary. 
-
-To enable Lustre, GPFS, and experimental tools, add the following flags during CMake:
-
-.. code-block:: Bash
-
-    -DENABLE_LUSTRE=ON
-    -DENABLE_GPFS=ON
-    -DENABLE_EXPERIMENTAL=ON
-
-To disable linking against libarchive, and tools requiring libarchive, add the following flag during CMake:
-
-.. code-block:: Bash
+   make -j install
 
-    -DENABLE_LIBARCHIVE=OFF
+The same CMake options as described in earlier sections are available.
 
---------------------------------------------------------------
-Build mpiFileUtils directly, build its dependencies with Spack
---------------------------------------------------------------
+----------------------------
+Development build with Spack
+----------------------------
 
-One can use Spack to install mpiFileUtils dependencies using the `spack.yaml` file distributed with mpiFileUtils.
-From the root directory of mpiFileUtils, run the command `spack find` to determine which packages spack will install.
-Next, run `spack concretize` to have spack perform dependency analysis.
+One can also build from a clone of the mpiFileUtils repository
+after using Spack to install its dependencies via the `spack.yaml` file that is distributed with mpiFileUtils.
+From the root directory of mpiFileUtils, run the command `spack find` to determine which packages Spack will install.
+Next, run `spack concretize` to have Spack perform dependency analysis.
 Finally, run `spack install` to build the dependencies.
 
 There are two ways to tell CMake about the dependencies.
@@ -156,7 +195,7 @@ Thus, the commands to build become:
 
 .. code-block:: Bash
 
-   git clone --depth 1 https://github.com/hpc/mpifileutils
+   git clone https://github.com/hpc/mpifileutils
    mkdir build install
    cd mpifileutils
    spack install
diff --git a/doc/rst/conf.py b/doc/rst/conf.py
index 28c70e1..8c0b9d5 100644
--- a/doc/rst/conf.py
+++ b/doc/rst/conf.py
@@ -46,7 +46,7 @@ master_doc = 'index'
 
 # General information about the project.
 project = u'mpiFileUtils'
-copyright = u'2021, LLNL/LANL/UT-Battelle/DDN'
+copyright = u'2022, LLNL/LANL/UT-Battelle/DDN'
 author = u'HPC'
 
 # The version info for the project you're documenting, acts as replacement for
@@ -54,9 +54,9 @@ author = u'HPC'
 # built documents.
 #
 # The short X.Y version.
-version = u'0.11.0'
+version = u'0.11.1'
 # The full version, including alpha/beta/rc tags.
-release = u'0.11.0'
+release = u'0.11.1'
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
@@ -91,19 +91,20 @@ source_encoding = 'utf-8-sig'
 # The theme to use for HTML and HTML Help pages.  See the documentation for
 # a list of builtin themes.
 #
-import guzzle_sphinx_theme
+html_theme = 'default'
 
-html_theme_path = guzzle_sphinx_theme.html_theme_path()
-html_theme = 'guzzle_sphinx_theme'
-
-# Register the theme as an extension to generate a sitemap.xml
-extensions.append("guzzle_sphinx_theme")
-
-# Guzzle theme options (see theme.conf for more information)
-html_theme_options = {
-    # Set the name of the project to appear in the sidebar
-    "project_nav_name": "mpiFileUtils",
-}
+#import guzzle_sphinx_theme
+#html_theme_path = guzzle_sphinx_theme.html_theme_path()
+#html_theme = 'guzzle_sphinx_theme'
+#
+## Register the theme as an extension to generate a sitemap.xml
+#extensions.append("guzzle_sphinx_theme")
+#
+## Guzzle theme options (see theme.conf for more information)
+#html_theme_options = {
+#    # Set the name of the project to appear in the sidebar
+#    "project_nav_name": "mpiFileUtils",
+#}
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
diff --git a/doc/rst/daos-deserialize.1.rst b/doc/rst/daos-deserialize.1.rst
new file mode 100644
index 0000000..324b454
--- /dev/null
+++ b/doc/rst/daos-deserialize.1.rst
@@ -0,0 +1,51 @@
+daos-deserialize
+===
+
+SYNOPSIS
+--------
+
+**daos-deserialize [OPTION] [<file> <file> ...] || [</path/to/directory>]**
+
+DESCRIPTION
+-----------
+
+Parallel MPI application to deserialize HDF5 files into a DAOS container.
+
+daos-deserialize is a deserialization tool that will restore DAOS data written
+to an HDF5 file with daos-serialize. The tool will restore the data in the
+the HDF5 file into a DAOS container.
+
+OPTIONS
+-------
+.. option:: --pool UUID 
+
+   Specify the pool where the restored DAOS container will be created 
+
+.. option:: -v, --verbose
+
+   Run in verbose mode.
+
+.. option:: -q, --quiet
+
+   Run tool silently. No output is printed.
+
+.. option:: -h, --help
+
+   Print a brief message listing the :manpage:`daos-deserialize(1)` options and usage.
+
+EXAMPLES
+--------
+
+1. To deserialize a DAOS container by specifying individual files:
+
+``mpirun -np 128 daos-deserialize <file1> <file2>``
+
+2. To deserialize a DAOS container by specifying a directory with HDF5 files:
+
+``mpirun -np 128 daos-deserialize /path/to/dir``
+
+SEE ALSO
+--------
+
+The mpiFileUtils source code and all documentation may be downloaded
+from <https://github.com/hpc/mpifileutils>
diff --git a/doc/rst/daos-serialize.1.rst b/doc/rst/daos-serialize.1.rst
new file mode 100644
index 0000000..d9b46bc
--- /dev/null
+++ b/doc/rst/daos-serialize.1.rst
@@ -0,0 +1,54 @@
+daos-serialize
+===
+
+SYNOPSIS
+--------
+
+**daos-serialize [OPTION] /<pool>/<cont>**
+
+DESCRIPTION
+-----------
+
+Parallel MPI application to serialize a DAOS container to an HDF5 file.
+
+daos-serialize is a serialization tool that will allow storing any type
+of DAOS container on a POSIX filesystem. It allows DAOS container data to
+be stored outside of the DAOS system. The DAOS container is converted
+to a set of files in HDF5 format. The serialization format of the container is meant to
+be deserialized by daos-deserialize.
+
+OPTIONS
+-------
+.. option:: -o, --output-path PATH
+
+   Write the HDF5 files generated during serialization to the specified
+   output path.
+
+.. option:: -v, --verbose
+
+   Run in verbose mode.
+
+.. option:: -q, --quiet
+
+   Run tool silently. No output is printed.
+
+.. option:: -h, --help
+
+   Print a brief message listing the :manpage:`daos-serialize(1)` options and usage.
+
+EXAMPLES
+--------
+
+1. To serialize a DAOS container:
+
+``mpirun -np 128 daos-serialize /<pool>/<container>``
+
+2. To serialize a DAOS container to a specific directory:
+
+``mpirun -np 128 daos-serialize -o /path/to/output/dir /<pool>/<container>``
+
+SEE ALSO
+--------
+
+The mpiFileUtils source code and all documentation may be downloaded
+from <https://github.com/hpc/mpifileutils>
diff --git a/doc/rst/dbcast.1.rst b/doc/rst/dbcast.1.rst
index 80b33f2..ec5a21a 100644
--- a/doc/rst/dbcast.1.rst
+++ b/doc/rst/dbcast.1.rst
@@ -9,12 +9,15 @@ SYNOPSIS
 DESCRIPTION
 -----------
 
-Parallel MPI application to recursively broadcast a single file from a
+Parallel MPI application to broadcast a single file from a
 global file system to node-local storage, like ramdisk or an SSD.
 
 The file is logically sliced into chunks and collectively copied from a
-global file system to node-local storage. The source file SRC must be
-readable by all MPI processes. The destination file DEST should be the
+global file system to node-local storage. The bytes of the source file
+are read from the global file system once and the data are forwarded
+through MPI to be written to storage on all compute nodes.
+The source file SRC must be readable by all MPI processes.
+The destination file DEST should be the
 full path of the file in node-local storage. If needed, parent
 directories for the destination file will be created as part of the
 broadcast.
diff --git a/doc/rst/dcmp.1.rst b/doc/rst/dcmp.1.rst
index d81996c..acf51c6 100644
--- a/doc/rst/dcmp.1.rst
+++ b/doc/rst/dcmp.1.rst
@@ -49,12 +49,6 @@ OPTIONS
    "GB" can immediately follow the number without spaces (e.g. 64MB).
    The default chunksize is 4MB.
 
-.. option:: --daos-prefix PREFIX
-
-   Specify the DAOS prefix to be used. This is only necessary
-   if copying a subset of a POSIX container in DAOS using a
-   Unified Namespace path.
-
 .. option:: --daos-api API
 
    Specify the DAOS API to be used. By default, the API is automatically
diff --git a/doc/rst/dcp.1.rst b/doc/rst/dcp.1.rst
index 02e30f6..00f3d33 100644
--- a/doc/rst/dcp.1.rst
+++ b/doc/rst/dcp.1.rst
@@ -32,11 +32,15 @@ OPTIONS
    "GB" can immediately follow the number without spaces (e.g. 64MB).
    The default chunksize is 4MB.
 
-.. option:: --daos-prefix PREFIX
+.. option:: --xattrs WHICH
 
-   Specify the DAOS prefix to be used. This is only necessary
-   if copying a subset of a POSIX container in DAOS using a
-   Unified Namespace path.
+    Copy extended attributes ("xattrs") from source files to target files.
+    WHICH determines which xattrs are copied.  Options are to copy no xattrs,
+    all xattrs, xattrs not excluded by /etc/xattr.conf, or all xattrs except
+    those which have special meaning to Lustre.  Certain xattrs control Lustre
+    features on a file-by-file basis, such as how the file data is distributed
+    across Lustre servers.  Values must be in {none, all, libattr, non-lustre}.
+    The default is non-lustre.
 
 .. option:: --daos-api API
 
@@ -45,6 +49,15 @@ OPTIONS
    DFS API, and all other containers use the DAOS object API.
    Values must be in {DFS, DAOS}.
 
+.. option:: --daos-preserve FILENAME 
+
+   Option to turn on metadata preservation in DAOS. This should
+   be used in the case that data is being moved to/from DAOS. For instance,
+   if data is being moved from DAOS to Lustre, then the preserve option can
+   be used to write the DAOS container metadata to an HDF5 file. That way,
+   when data is moved back from Lustre to DAOS the container properties can
+   be preserved. A filename to write the metadata to must be specified.
+
 .. option:: -i, --input FILE
 
    Read source list from FILE. FILE must be generated by another tool
@@ -63,7 +76,7 @@ OPTIONS
 
 .. option:: -p, --preserve
 
-   Preserve permissions, group, timestamps, and extended attributes.
+   Preserve permissions, group, and timestamps.
 
 .. option:: -s, --direct
 
diff --git a/doc/rst/dcp_sierra.png b/doc/rst/dcp_sierra.png
new file mode 100644
index 0000000..cc5d435
Binary files /dev/null and b/doc/rst/dcp_sierra.png differ
diff --git a/doc/rst/dfind.1.rst b/doc/rst/dfind.1.rst
index 247d4c2..dd0e331 100644
--- a/doc/rst/dfind.1.rst
+++ b/doc/rst/dfind.1.rst
@@ -29,6 +29,11 @@ OPTIONS
 
    Write the processed list to a file.
 
+.. option:: -t, --text
+
+   Must be used with the --output option. Write processed list of files to
+   FILE in ascii text format.
+
 .. option:: -v, --verbose
 
    Run in verbose mode.
diff --git a/doc/rst/dsync.1.rst b/doc/rst/dsync.1.rst
index 2fe4715..402ee05 100644
--- a/doc/rst/dsync.1.rst
+++ b/doc/rst/dsync.1.rst
@@ -15,6 +15,9 @@ dsync makes DEST match SRC, adding missing entries from DEST, and updating
 existing entries in DEST as necessary so that SRC and DEST have identical
 content, ownership, timestamps, and permissions.
 
+dsync is similar to :manpage:`rsync(1)` archive mode for creating and
+then maintaining an identical copy of a source directory tree.
+
 OPTIONS
 -------
 
@@ -39,11 +42,15 @@ OPTIONS
    "GB" can immediately follow the number without spaces (e.g. 64MB).
    The default chunksize is 4MB.
 
-.. option:: --daos-prefix PREFIX
+.. option:: --xattrs WHICH
 
-   Specify the DAOS prefix to be used. This is only necessary
-   if copying a subset of a POSIX container in DAOS using a
-   Unified Namespace path.
+    Copy extended attributes ("xattrs") from source files to target files.
+    WHICH determines which xattrs are copied.  Options are to copy no xattrs,
+    all xattrs, xattrs not excluded by /etc/xattr.conf, or all xattrs except
+    those which have special meaning to Lustre.  Certain xattrs control Lustre
+    features on a file-by-file basis, such as how the file data is distributed
+    across Lustre servers.  Values must be in {none, all, libattr, non-lustre}.
+    The default is non-lustre.
 
 .. option:: --daos-api API
 
@@ -87,10 +94,10 @@ OPTIONS
    in /src.bak will instead be hardlinked to the file in /src.bak:
 
    # initial backup of /src
-   dsync /src /src.bak
+   ``dsync /src /src.bak``
 
    # incremental backup of /src
-   dsync --link-dest /src.bak /src /src.bak.inc
+   ``dsync --link-dest /src.bak /src /src.bak.inc``
 
 .. option:: -S, --sparse
 
diff --git a/doc/rst/index.rst b/doc/rst/index.rst
index 4cb02f4..0ab552d 100644
--- a/doc/rst/index.rst
+++ b/doc/rst/index.rst
@@ -3,60 +3,39 @@
    You can adapt this file completely to your liking, but it should at least
    contain the root `toctree` directive.
 
-===================================
-**Documentation for mpiFileUtils**
-===================================
+================
+**mpiFileUtils**
+================
 
 Overview
 *************
 
-mpiFileUtils provides both a library called libmfu and a suite of MPI-based
-tools to manage large datasets, which may vary from large directory trees to
-large files. High-performance computing users often generate large datasets with
-parallel applications that run with many processes (millions in some cases).
-However those users are then stuck with single-process tools like cp and rm to
-manage their datasets. This suite provides MPI-based tools to handle typical
-jobs like copy, remove, and compare for such datasets, providing speedups of up
-to 50x. The libmfu library simplifies the creation of new tools
-and it can be called directly from within HPC applications.
+High-performance computing users generate large datasets using parallel applications that can run with thousands of processes.
+However, users are often stuck managing those datasets using traditional single-process tools like cp and rm.
+This mismatch in scale makes it impractical for users to work with their data.
+
+The mpiFileUtils suite solves this problem by offering MPI-based tools for basic tasks like copy, remove, and compare for such datasets,
+delivering orders of magnitude in performance speedup over their single-process counterparts.
+Furthermore, the libmfu library packages common functionality to simplify the creation of new tools,
+and it can even be invoked directly from within HPC applications.
 
 Video Overview: `"Scalable Management of HPC Datasets with mpiFileUtils" <https://youtu.be/cxjPOUS-ZBY>`_, HPCKP'20.
 
+The figure below, taken from the above presentation, illustrates the potential performance improvement that one can achieve
+when scaling a tool like dcp to utilize more compute resources.
+
+.. figure:: dcp_sierra.png
+
+   dcp scaling performance on the Sierra cluster at LLNL using 40 processes/node.  Shows the time required to copy a single directory of 200k files totaling 24.4 TiB of data.  The minimum time of 93 seconds at 64 nodes is 495x faster than the 12.75 hours taken by the cp command.
+
 User Guide
 ***************************
 
 .. toctree::
-   :maxdepth: 3
+   :maxdepth: 2
 
+   build.rst
    proj-design.rst
    tools.rst
    libmfu.rst
-   build.rst
-
-Man Pages
-***************************
 
-.. toctree::
-   :maxdepth: 1
-
-   dbcast.1
-   dbz2.1
-   dchmod.1
-   dcmp.1
-   dcp.1
-   ddup.1
-   dfind.1
-   dreln.1
-   drm.1
-   dstripe.1
-   dsync.1
-   dwalk.1
-   dgrep.1
-   dparallel.1
-   dtar.1
-
-Indices and tables
-*********************
-
-* :ref:`genindex`
-* :ref:`search`
diff --git a/doc/rst/libmfu.rst b/doc/rst/libmfu.rst
index f0ca232..4b6aa73 100644
--- a/doc/rst/libmfu.rst
+++ b/doc/rst/libmfu.rst
@@ -1,5 +1,5 @@
 ========================
-libmfu
+Common Library - libmfu
 ========================
 
 Functionality that is common to multiple tools is moved to the common library,
diff --git a/doc/rst/tools.rst b/doc/rst/tools.rst
index 81683f0..2269a71 100644
--- a/doc/rst/tools.rst
+++ b/doc/rst/tools.rst
@@ -7,19 +7,19 @@ as MPI applications, e.g., within a compute allocation on a cluster using
 mpirun. The tools do not currently checkpoint, so one must be careful that an
 invocation of the tool has sufficient time to complete before it is killed.
 
-- dbcast - Broadcast a file to compute nodes.
-- dbz2 - Compress a file with bz2.
-- dchmod - Change owner, group, and permissions on files.
-- dcmp - Compare files.
-- dcp - Copy files.
-- ddup - Find duplicate files.
-- dfind - Filter files.
-- dreln - Update symlinks.
-- drm - Remove files.
-- dstripe - Restripe files.
-- dsync - Synchronize files.
-- dtar - Create and extract tape archive files.
-- dwalk - List, sort, and profile files.
+- :doc:`dbcast <dbcast.1>` - Broadcast a file to each compute node.
+- :doc:`dbz2 <dbz2.1>` - Compress and decompress a file with bz2.
+- :doc:`dchmod <dchmod.1>` - Change owner, group, and permissions on files.
+- :doc:`dcmp <dcmp.1>` - Compare contents between directories or files.
+- :doc:`dcp <dcp.1>` - Copy files.
+- :doc:`ddup <ddup.1>` - Find duplicate files.
+- :doc:`dfind <dfind.1>` - Filter files.
+- :doc:`dreln <dreln.1>` - Update symlinks to point to a new path.
+- :doc:`drm <drm.1>` - Remove files.
+- :doc:`dstripe <dstripe.1>` - Restripe files (Lustre).
+- :doc:`dsync <dsync.1>` - Synchronize source and destination directories or files.
+- :doc:`dtar <dtar.1>` - Create and extract tape archive files.
+- :doc:`dwalk <dwalk.1>` - List, sort, and profile files.
 
 ==============================
 Experimental Utilities
@@ -33,3 +33,98 @@ who are interested in developing them further or to provide additional examples.
 - dparallel - Perform commands in parallel.
 - dsh - List and remove files with interactive commands.
 - dfilemaker - Generate random files.
+
+==============================
+Usage tips
+==============================
+Since the tools are MPI applications, it helps to keep a few things in mind:
+
+- One typically needs to run the tools within a job allocation.  The sweet spot for most tools is about 2-4 nodes.  One can use more nodes for large datasets, so long as tools scale sufficiently well.
+- One must launch the job using the MPI job launcher like mpirun or mpiexec.  One should use most CPU cores, though leave a few cores idle on each node for the file system client processes.
+- Most tools do not checkpoint their progress.  Be sure to request sufficient time in your allocation to allow the job to complete.  One may need to start over from the beginning if a tool is interrupted.
+- One cannot pipe output of one tool to the input of another.  However, the --input and --output file options are good approximations.
+- One cannot easily check the return codes of tools.  Instead, inspect stdout and stderr output for errors.
+
+==============================
+Examples and frequently used commands
+==============================
+If your MPI library supports it, most tools can run as MPI singletons (w/o mpirun, which runs a single-task MPI job).
+For brevity, the examples in this section are shown as MPI singleton runs.
+In a real run, one would precede the command shown with an appropriate MPI launch command and options, e.g.,::
+
+  mpirun -np 128 dwalk /path/to/walk
+
+In addition to the man page, each tool provides a help screen for a brief reminder of available options.::
+
+  dwalk --help
+
+The normal output from dwalk shows a summary of item and byte counts.
+This is useful to determine the number of files and bytes under a path of interest::
+
+  dwalk /path/to/walk
+
+When walking large directory trees, you can write the list to an output file.
+Then you can read that list back without having to walk the file system again.::
+
+  dwalk --output list.mfu /path/to/walk
+  dwalk --input list.mfu
+
+The default file format is a binary file intended for use in other tools, not humans, but one can ask for a text-based output::
+
+ dwalk --text --output list.txt /path/to/walk
+
+The text-based output is lossy, and it cannot be read back in to a tool.
+If you want both, save to binary format first, then read the binary file to convert it to text.::
+
+  dwalk --output list.mfu /path/to/walk
+  dwalk --input list.mfu --text --output list.txt
+
+dwalk also provides a sort option to order items in the list in various ways,
+e.g., to order the list by username, then by access time::
+
+  dwalk --input list.mfu --sort user,atime --output user_atime.mfu
+
+To order items from largest to smallest number of bytes::
+
+  dwalk --input list.mfu --sort '-size' --output big_to_small.mfu
+
+dfind can be used to filter items with a string of find-like expressions,
+e.g., files owned by user1 that are bigger than 100GB::
+
+  dfind --input list.mfu --user user1 --size +100GB --output user1_over_100GB.mfu
+
+dchmod is like chmod and chgrp in one, so one can change uid/gid/mode with a single command::
+
+  dchmod --group grp1 --mode g+rw /path/to/walk
+
+drm is like "rm -rf" but in parallel::
+
+  drm /path/to/remove
+
+dbcast provides an efficient way to broadcast a file to all compute nodes,
+e.g., upload a tar file of a dataset to an SSD local to each compute node::
+
+  dbcast /path/to/file.dat /ssd/file.dat
+
+dsync is the recommended way to make a copy a large set of files::
+
+  dsync /path/src /path/dest
+
+For large directory trees, the --batch-files option offers a type of checkpoint.
+It moves files in batches, and if interrupted, a restart picks up from the last completed batch.::
+
+  dsync --batch-files 100000 /path/src /path/dest
+
+The tools can be composed in various ways using the --input and --output options.
+For example, the following sequence of commands executes a purge operation,
+which deletes any file that has not been accessed in the past 180 days.::
+
+  # walk directory to stat all files, record list in file
+  dwalk --output list.mfu /path/to/walk
+
+  # filter list to identify all regular files that were last accessed over 180 days ago
+  dfind --input list.mfu --type f --atime +180 --output purgelist.mfu
+
+  # delete all files in the purge list
+  drm --input purgelist.mfu
+
diff --git a/man/dbcast.1 b/man/dbcast.1
index 9934ac0..263de20 100644
--- a/man/dbcast.1
+++ b/man/dbcast.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DBCAST" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DBCAST" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dbcast \- distributed broadcast
 .
@@ -35,12 +35,15 @@ level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
 \fBdbcast [OPTION] SRC DEST\fP
 .SH DESCRIPTION
 .sp
-Parallel MPI application to recursively broadcast a single file from a
+Parallel MPI application to broadcast a single file from a
 global file system to node\-local storage, like ramdisk or an SSD.
 .sp
 The file is logically sliced into chunks and collectively copied from a
-global file system to node\-local storage. The source file SRC must be
-readable by all MPI processes. The destination file DEST should be the
+global file system to node\-local storage. The bytes of the source file
+are read from the global file system once and the data are forwarded
+through MPI to be written to storage on all compute nodes.
+The source file SRC must be readable by all MPI processes.
+The destination file DEST should be the
 full path of the file in node\-local storage. If needed, parent
 directories for the destination file will be created as part of the
 broadcast.
@@ -88,6 +91,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dbz2.1 b/man/dbz2.1
index 060d127..e09d49b 100644
--- a/man/dbz2.1
+++ b/man/dbz2.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DBZ2" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DBZ2" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dbz2 \- distributed bz2 compression
 .
@@ -107,6 +107,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dchmod.1 b/man/dchmod.1
index 4663a1a..7babf68 100644
--- a/man/dchmod.1
+++ b/man/dchmod.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DCHMOD" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DCHMOD" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dchmod \- distributed tool to set permissions and group
 .
@@ -160,6 +160,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dcmp.1 b/man/dcmp.1
index ab3d0c7..4ccf673 100644
--- a/man/dcmp.1
+++ b/man/dcmp.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DCMP" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DCMP" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dcmp \- distributed compare
 .
@@ -65,23 +65,16 @@ Enable base checks and normal stdout results when –output is used.
 .TP
 .B \-\-bufsize SIZE
 Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
+immediately follow the number without spaces (e.g. 8MB). The default
+bufsize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
 .B \-\-chunksize SIZE
 Multiple processes copy a large file in parallel by dividing it into chunks.
 Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-daos\-prefix PREFIX
-Specify the DAOS prefix to be used. This is only necessary
-if copying a subset of a POSIX container in DAOS using a
-Unified Namespace path.
+“GB” can immediately follow the number without spaces (e.g. 64MB).
+The default chunksize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
@@ -359,6 +352,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dcp.1 b/man/dcp.1
index 485339c..0123e7d 100644
--- a/man/dcp.1
+++ b/man/dcp.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DCP" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DCP" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dcp \- distributed copy
 .
@@ -47,23 +47,27 @@ file system, and it splits large file copies across multiple processes.
 .TP
 .B \-\-bufsize SIZE
 Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
+immediately follow the number without spaces (e.g. 8MB). The default
+bufsize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
 .B \-\-chunksize SIZE
 Multiple processes copy a large file in parallel by dividing it into chunks.
 Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
+“GB” can immediately follow the number without spaces (e.g. 64MB).
+The default chunksize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
-.B \-\-daos\-prefix PREFIX
-Specify the DAOS prefix to be used. This is only necessary
-if copying a subset of a POSIX container in DAOS using a
-Unified Namespace path.
+.B \-\-xattrs WHICH
+Copy extended attributes (“xattrs”) from source files to target files.
+WHICH determines which xattrs are copied.  Options are to copy no xattrs,
+all xattrs, xattrs not excluded by /etc/xattr.conf, or all xattrs except
+those which have special meaning to Lustre.  Certain xattrs control Lustre
+features on a file\-by\-file basis, such as how the file data is distributed
+across Lustre servers.  Values must be in {none, all, libattr, non\-lustre}.
+The default is non\-lustre.
 .UNINDENT
 .INDENT 0.0
 .TP
@@ -75,6 +79,16 @@ Values must be in {DFS, DAOS}.
 .UNINDENT
 .INDENT 0.0
 .TP
+.B \-\-daos\-preserve FILENAME
+Option to turn on metadata preservation in DAOS. This should
+be used in the case that data is being moved to/from DAOS. For instance,
+if data is being moved from DAOS to Lustre, then the preserve option can
+be used to write the DAOS container metadata to an HDF5 file. That way,
+when data is moved back from Lustre to DAOS the container properties can
+be preserved. A filename to write the metadata to must be specified.
+.UNINDENT
+.INDENT 0.0
+.TP
 .B \-i, \-\-input FILE
 Read source list from FILE. FILE must be generated by another tool
 from the mpiFileUtils suite.
@@ -95,7 +109,7 @@ or there is not permission to read the link’s target.
 .INDENT 0.0
 .TP
 .B \-p, \-\-preserve
-Preserve permissions, group, timestamps, and extended attributes.
+Preserve permissions, group, and timestamps.
 .UNINDENT
 .INDENT 0.0
 .TP
@@ -173,6 +187,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/ddup.1 b/man/ddup.1
index 0588988..f5fb304 100644
--- a/man/ddup.1
+++ b/man/ddup.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DDUP" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DDUP" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 ddup \- report files with identical content
 .
@@ -77,6 +77,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dfind.1 b/man/dfind.1
index c858e69..d76e449 100644
--- a/man/dfind.1
+++ b/man/dfind.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DFIND" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DFIND" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dfind \- distributed file filtering
 .
@@ -56,6 +56,12 @@ Write the processed list to a file.
 .UNINDENT
 .INDENT 0.0
 .TP
+.B \-t, \-\-text
+Must be used with the –output option. Write processed list of files to
+FILE in ascii text format.
+.UNINDENT
+.INDENT 0.0
+.TP
 .B \-v, \-\-verbose
 Run in verbose mode.
 .UNINDENT
@@ -273,6 +279,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dreln.1 b/man/dreln.1
index 099bc79..953adb4 100644
--- a/man/dreln.1
+++ b/man/dreln.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DRELN" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DRELN" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dreln \- distributed relink
 .
@@ -114,6 +114,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/drm.1 b/man/drm.1
index f023f6d..174f9dc 100644
--- a/man/drm.1
+++ b/man/drm.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DRM" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DRM" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 drm \- distributed remove
 .
@@ -174,6 +174,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dstripe.1 b/man/dstripe.1
index 4e5586a..cfe6589 100644
--- a/man/dstripe.1
+++ b/man/dstripe.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DSTRIPE" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DSTRIPE" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dstripe \- restripe files on underlying storage
 .
@@ -136,6 +136,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dsync.1 b/man/dsync.1
index 08d0164..39380bc 100644
--- a/man/dsync.1
+++ b/man/dsync.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DSYNC" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DSYNC" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dsync \- synchronize directory trees
 .
@@ -40,6 +40,9 @@ Parallel MPI application to synchronize two files or two directory trees.
 dsync makes DEST match SRC, adding missing entries from DEST, and updating
 existing entries in DEST as necessary so that SRC and DEST have identical
 content, ownership, timestamps, and permissions.
+.sp
+dsync is similar to \fBrsync(1)\fP archive mode for creating and
+then maintaining an identical copy of a source directory tree.
 .SH OPTIONS
 .INDENT 0.0
 .TP
@@ -55,23 +58,27 @@ Batch files into groups of up to size N during copy operation.
 .TP
 .B \-\-bufsize SIZE
 Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
+immediately follow the number without spaces (e.g. 8MB). The default
+bufsize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
 .B \-\-chunksize SIZE
 Multiple processes copy a large file in parallel by dividing it into chunks.
 Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
+“GB” can immediately follow the number without spaces (e.g. 64MB).
+The default chunksize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
-.B \-\-daos\-prefix PREFIX
-Specify the DAOS prefix to be used. This is only necessary
-if copying a subset of a POSIX container in DAOS using a
-Unified Namespace path.
+.B \-\-xattrs WHICH
+Copy extended attributes (“xattrs”) from source files to target files.
+WHICH determines which xattrs are copied.  Options are to copy no xattrs,
+all xattrs, xattrs not excluded by /etc/xattr.conf, or all xattrs except
+those which have special meaning to Lustre.  Certain xattrs control Lustre
+features on a file\-by\-file basis, such as how the file data is distributed
+across Lustre servers.  Values must be in {none, all, libattr, non\-lustre}.
+The default is non\-lustre.
 .UNINDENT
 .INDENT 0.0
 .TP
@@ -122,10 +129,10 @@ For example in the following, any file that would be copied from
 in /src.bak will instead be hardlinked to the file in /src.bak:
 .sp
 # initial backup of /src
-dsync /src /src.bak
+\fBdsync /src /src.bak\fP
 .sp
 # incremental backup of /src
-dsync –link\-dest /src.bak /src /src.bak.inc
+\fBdsync \-\-link\-dest /src.bak /src /src.bak.inc\fP
 .UNINDENT
 .INDENT 0.0
 .TP
@@ -170,6 +177,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dtar.1 b/man/dtar.1
index 77c0fa4..739f7d7 100644
--- a/man/dtar.1
+++ b/man/dtar.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DTAR" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DTAR" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dtar \- create and extract a tar archive
 .
@@ -144,16 +144,16 @@ Call fsync before closing files after writing.
 .TP
 .B \-\-bufsize SIZE
 Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
+immediately follow the number without spaces (e.g. 8MB). The default
+bufsize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
 .B \-\-chunksize SIZE
 Multiple processes copy a large file in parallel by dividing it into chunks.
 Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
+“GB” can immediately follow the number without spaces (e.g. 64MB).
+The default chunksize is 4MB.
 .UNINDENT
 .INDENT 0.0
 .TP
@@ -206,6 +206,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/dwalk.1 b/man/dwalk.1
index 41e9e40..3f58d2d 100644
--- a/man/dwalk.1
+++ b/man/dwalk.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "DWALK" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "DWALK" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 dwalk \- distributed walk and list
 .
@@ -173,6 +173,6 @@ from <\fI\%https://github.com/hpc/mpifileutils\fP>
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/man/mpifileutils.1 b/man/mpifileutils.1
index 224a8fe..bc9ab3d 100644
--- a/man/mpifileutils.1
+++ b/man/mpifileutils.1
@@ -1,6 +1,6 @@
 .\" Man page generated from reStructuredText.
 .
-.TH "MPIFILEUTILS" "1" "Jan 26, 2021" "0.11.0" "mpiFileUtils"
+.TH "MPIFILEUTILS" "1" "Feb 04, 2022" "0.11.1" "mpiFileUtils"
 .SH NAME
 mpifileutils \- mpiFileUtils Documentation
 .
@@ -32,192 +32,150 @@ level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
 ..
 .SH OVERVIEW
 .sp
-mpiFileUtils provides both a library called libmfu and a suite of MPI\-based
-tools to manage large datasets, which may vary from large directory trees to
-large files. High\-performance computing users often generate large datasets with
-parallel applications that run with many processes (millions in some cases).
-However those users are then stuck with single\-process tools like cp and rm to
-manage their datasets. This suite provides MPI\-based tools to handle typical
-jobs like copy, remove, and compare for such datasets, providing speedups of up
-to 50x. The libmfu library simplifies the creation of new tools
-and it can be called directly from within HPC applications.
+High\-performance computing users generate large datasets using parallel applications that can run with thousands of processes.
+However, users are often stuck managing those datasets using traditional single\-process tools like cp and rm.
+This mismatch in scale makes it impractical for users to work with their data.
 .sp
-Video Overview: \fI\%“Scalable Management of HPC Datasets with mpiFileUtils”\fP, HPCKP‘20.
-.SH USER GUIDE
-.SS Project Design Principles
-.sp
-The following principles drive design decisions in the project.
-.SS Scale
-.sp
-The library and tools should be designed such that running with more processes
-increases performance, provided there are sufficient data and parallelism
-available in the underlying file systems. The design of the tool should not
-impose performance scalability bottlenecks.
-.SS Performance
-.sp
-While it is tempting to mimic the interface, behavior, and file formats of
-familiar tools like cp, rm, and tar, when forced with a choice between
-compatibility and performance, mpiFileUtils chooses performance. For example,
-if an archive file format requires serialization that inhibits parallel
-performance, mpiFileUtils will opt to define a new file format that enables
-parallelism rather than being constrained to existing formats. Similarly,
-options in the tool command line interface may have different semantics from
-familiar tools in cases where performance is improved. Thus, one should be
-careful to learn the options of each tool.
-.SS Portability
-.sp
-The tools are intended to support common file systems used in HPC centers, like
-Lustre, GPFS, and NFS. Additionally, methods in the library should be portable
-and efficient across multiple file systems. Tool and library users can rely on
-mpiFileUtils to provide portable and performant implementations.
-.SS Composability
+The mpiFileUtils suite solves this problem by offering MPI\-based tools for basic tasks like copy, remove, and compare for such datasets,
+delivering orders of magnitude in performance speedup over their single\-process counterparts.
+Furthermore, the libmfu library packages common functionality to simplify the creation of new tools,
+and it can even be invoked directly from within HPC applications.
 .sp
-While the tools do not support chaining with Unix pipes, they do support
-interoperability through input and output files. One tool may process a dataset
-and generate an output file that another tool can read as input, e.g., to walk
-a directory tree with one tool, filter the list of file names with another, and
-perhaps delete a subset of matching files with a third. Additionally, when logic
-is deemed to be useful across multiple tools or is anticipated to be useful in
-future tools or applications, it should be provided in the common library.
-.SS Utilities
+Video Overview: \fI\%“Scalable Management of HPC Datasets with mpiFileUtils”\fP, HPCKP‘20.
 .sp
-The tools in mpiFileUtils are MPI applications. They must be launched
-as MPI applications, e.g., within a compute allocation on a cluster using
-mpirun. The tools do not currently checkpoint, so one must be careful that an
-invocation of the tool has sufficient time to complete before it is killed.
+The figure below, taken from the above presentation, illustrates the potential performance improvement that one can achieve
+when scaling a tool like dcp to utilize more compute resources.
 .INDENT 0.0
-.IP \(bu 2
-dbcast \- Broadcast a file to compute nodes.
-.IP \(bu 2
-dbz2 \- Compress a file with bz2.
-.IP \(bu 2
-dchmod \- Change owner, group, and permissions on files.
-.IP \(bu 2
-dcmp \- Compare files.
-.IP \(bu 2
-dcp \- Copy files.
-.IP \(bu 2
-ddup \- Find duplicate files.
-.IP \(bu 2
-dfind \- Filter files.
-.IP \(bu 2
-dreln \- Update symlinks.
-.IP \(bu 2
-drm \- Remove files.
-.IP \(bu 2
-dstripe \- Restripe files.
-.IP \(bu 2
-dsync \- Synchronize files.
-.IP \(bu 2
-dtar \- Create and extract tape archive files.
-.IP \(bu 2
-dwalk \- List, sort, and profile files.
+.INDENT 2.5
+[image]
+dcp scaling performance on the Sierra cluster at LLNL using 40 processes/node.  Shows the time required to copy a single directory of 200k files totaling 24.4 TiB of data.  The minimum time of 93 seconds at 64 nodes is 495x faster than the 12.75 hours taken by the cp command..UNINDENT
 .UNINDENT
-.SS Experimental Utilities
+.SH USER GUIDE
+.SS Build
 .sp
-Experimental utilities are under active development. They are not considered to
-be production worthy, but they are available in the distribution for those
-who are interested in developing them further or to provide additional examples.
+mpiFileUtils and its dependencies can be installed with CMake or Spack.
+Several build variations are described in this section:
 .INDENT 0.0
 .IP \(bu 2
-dgrep \- Run grep on files in parallel.
+CMake
 .IP \(bu 2
-dparallel \- Perform commands in parallel.
+Spack
 .IP \(bu 2
-dsh \- List and remove files with interactive commands.
+development build with CMake
 .IP \(bu 2
-dfilemaker \- Generate random files.
+development build with Spack
 .UNINDENT
-.SS libmfu
+.SS CMake
 .sp
-Functionality that is common to multiple tools is moved to the common library,
-libmfu. This goal of this library is to make it easy to develop new tools and
-to provide consistent behavior across tools in the suite. The library can also
-be useful to end applications, e.g., to efficiently create or remove a large
-directory tree in a portable way across different parallel file systems.
-.SS libmfu: the mpiFileUtils common library
+mpiFileUtils requires CMake 3.1 or higher.
+Before running cmake, ensure that the MPI wrapper scripts like mpicc are loaded in your environment.
 .sp
-The mpiFileUtils common library defines data structures and methods on those
-data structures that makes it easier to develop new tools or for use within HPC
-applications to provide portable, performant implementations across file
-systems common in HPC centers.
+The simplest mpiFileUtils install for many users is to build from a release package.
+This packages the source for a specific version of mpiFileUtils along with the corresponding source for several of its dependencies in a single tarball.
+mpiFileUtils release packages are available as attachments from their respective GitHub Releases page:
+.sp
+\fI\%https://github.com/hpc/mpifileutils/releases\fP
+.sp
+mpiFileUtils optionally depends on libarchive, version 3.5.1.
+If new enough, the system install of libarchive may be sufficient,
+though even newer versions may be incompatible with the required version.
+To be certain of compatibility, it is recommended that one install libarchive\-3.5.1 with commands like the following
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-#include "mfu.h"
+#!/bin/bash
+mkdir install
+installdir=\(gapwd\(ga/install
+
+wget https://github.com/libarchive/libarchive/releases/download/3.5.1/libarchive\-3.5.1.tar.gz
+tar \-zxf libarchive\-3.5.1.tar.gz
+cd libarchive\-3.5.1
+  ./configure \-\-prefix=$installdir
+  make install
+cd ..
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
 .sp
-This file includes all other necessary headers.
-.SS mfu_flist
-.sp
-The key data structure in libmfu is a distributed file list called mfu_flist.
-This structure represents a list of files, each with stat\-like metadata, that
-is distributed among a set of MPI ranks.
-.sp
-The library contains functions for creating and operating on these lists. For
-example, one may create a list by recursively walking an existing directory or
-by inserting new entries one at a time. Given a list as input, functions exist
-to create corresponding entries (inodes) on the file system or to delete the
-list of files. One may filter, sort, and remap entries. One can copy a list of
-entries from one location to another or compare corresponding entries across
-two different lists. A file list can be serialized and written to or read from
-a file.
-.sp
-Each MPI rank “owns” a portion of the list, and there are routines to step
-through the entries owned by that process. This portion is referred to as the
-“local” list. Functions exist to get and set properties of the items in the
-local list, for example to get the path name, type, and size of a file.
-Functions dealing with the local list can be called by the MPI process
-independently of other MPI processes.
+To build on PowerPC, one may need to add \fB\-\-build=powerpc64le\-redhat\-linux\-gnu\fP to the configure command.
 .sp
-Other functions operate on the global list in a collective fashion, such as
-deleting all items in a file list. All processes in the MPI job must invoke
-these functions simultaneously.
+Assuming libarchive has been installed to an \fIinstall\fP directory as shown above,
+one can then build mpiFileUtils from a release like v0.11.1 with commands like the following:
+.INDENT 0.0
+.INDENT 3.5
 .sp
-For full details, see \fI\%mfu_flist.h\fP
-and refer to its usage in existing tools.
-.SS mfu_path
+.nf
+.ft C
+wget https://github.com/hpc/mpifileutils/releases/download/v0.11.1/mpifileutils\-v0.11.1.tgz
+tar \-zxf mpifileutils\-v0.11.1.tgz
+cd mpifileutils\-v0.11.1
+  mkdir build
+  cd build
+    cmake .. \e
+      \-DWITH_LibArchive_PREFIX=../../install \e
+      \-DCMAKE_INSTALL_PREFIX=../../install
+    make \-j install
+  cd ..
+cd ..
+.ft P
+.fi
+.UNINDENT
+.UNINDENT
 .sp
-mpiFileUtils represents file paths with the \fI\%mfu_path\fP
-structure. Functions are available to manipulate paths to prepend and append
-entries, to slice paths into pieces, and to compute relative paths.
-.SS mfu_param_path
+Additional CMake options:
+.INDENT 0.0
+.IP \(bu 2
+\fB\-DENABLE_LIBARCHIVE=[ON/OFF]\fP : use libarchive and build tools requiring libarchive like dtar, defaults to \fBON\fP
+.IP \(bu 2
+\fB\-DENABLE_XATTRS=[ON/OFF]\fP : use extended attributes and libattr, defaults to \fBON\fP
+.IP \(bu 2
+\fB\-DENABLE_LUSTRE=[ON/OFF]\fP : specialization for Lustre, defaults to \fBOFF\fP
+.IP \(bu 2
+\fB\-DENABLE_GPFS=[ON/OFF]\fP : specialization for GPFS, defaults to \fBOFF\fP
+.IP \(bu 2
+\fB\-DENABLE_EXPERIMENTAL=[ON/OFF]\fP : build experimental tools, defaults to \fBOFF\fP
+.UNINDENT
+.SS DAOS support
 .sp
-Path names provided by the user on the command line (parameters) are handled
-through the \fI\%mfu_param_path\fP
-structure. Such paths may have to be checked for existence and to determine
-their type (file or directory). Additionally, the user may specify many such
-paths through invocations involving shell wildcards, so functions are available
-to check long lists of paths in parallel.
-.SS mfu_io
+To build with DAOS support, first install mpiFileUtils dependencies as mentioned above,
+and also make sure DAOS is installed. If DAOS is installed under a standard
+system path then specifying the DAOS path with \fB\-DWITH_DAOS_PREFIX\fP is unnecessary.
+.INDENT 0.0
+.INDENT 3.5
 .sp
-The \fI\%mfu_io.h\fP
-functions provide wrappers for many POSIX\-IO functions. This is helpful for
-checking error codes in a consistent manner and automating retries on failed
-I/O calls. One should use the wrappers in mfu_io if available, and if not, one
-should consider adding the missing wrapper.
-.SS mfu_util
+.nf
+.ft C
+cmake ../mpifileutils \e
+  \-DCMAKE_INSTALL_PREFIX=../install \e
+  \-DWITH_DAOS_PREFIX=</path/to/daos/> \e
+  \-DENABLE_DAOS=ON
+make \-j install
+.ft P
+.fi
+.UNINDENT
+.UNINDENT
 .sp
-The \fI\%mfu_util.h\fP
-functions provide wrappers for error reporting and memory allocation.
-.SS Build
+Some DAOS\-enabled tools require HDF5.
+To use the \fIdaos\-serialize\fP and \fIdaos\-deserialize\fP tools, HDF5 1.2+ is required.
+To copy HDF5 containers with \fIdcp\fP, HDF5 1.8+ is required, along with the daos\-vol.
 .sp
-mpiFileUtils and its dependencies can be installed with and without Spack.
-There are several common variations described here:
+To build with HDF5 support, add the following flags during CMake.
+If HDF5 is installed under a standard system path then specifying the HDF5 path with \fB\-DWITH_HDF5_PREFIX\fP is unnecessary.
 .INDENT 0.0
-.IP \(bu 2
-install both mpiFileUtils and its dependencies with Spack
-.IP \(bu 2
-install both mpiFileUtils and its dependencies directly
-.IP \(bu 2
-install mpiFileUtis directly after installing its dependencies with Spack
+.INDENT 3.5
+.sp
+.nf
+.ft C
+\-DENABLE_HDF5=ON \e
+\-DWITH_HDF5_PREFIX=</path/to/hdf5>
+.ft P
+.fi
 .UNINDENT
-.SS Build everything with Spack
+.UNINDENT
+.SS Spack
 .sp
 To use \fI\%Spack\fP, it is recommended that one first create a \fIpackages.yaml\fP file to list system\-provided packages, like MPI.
 Without doing this, Spack will fetch and install an MPI library that may not work on your system.
@@ -246,11 +204,11 @@ spack install mpifileutils +lustre +gpfs +experimental
 .fi
 .UNINDENT
 .UNINDENT
-.SS Build everything directly
+.SS Development build with CMake
 .sp
-To build directly, mpiFileUtils requires CMake 3.1 or higher.
-First ensure MPI wrapper scripts like mpicc are loaded in your environment.
-Then to install the dependencies, run the following commands:
+To make changes to mpiFileUtils, one may wish to build from a clone of the repository.
+This requires that one installs the mpiFileUtils dependencies separately,
+which can be done with the following commands:
 .INDENT 0.0
 .INDENT 3.5
 .sp
@@ -263,8 +221,8 @@ installdir=\(gapwd\(ga/install
 mkdir deps
 cd deps
   wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle\-0.3.0.tar.gz
-  wget https://github.com/llnl/lwgrp/releases/download/v1.0.3/lwgrp\-1.0.3.tar.gz
-  wget https://github.com/llnl/dtcmp/releases/download/v1.1.1/dtcmp\-1.1.1.tar.gz
+  wget https://github.com/llnl/lwgrp/releases/download/v1.0.4/lwgrp\-1.0.4.tar.gz
+  wget https://github.com/llnl/dtcmp/releases/download/v1.1.4/dtcmp\-1.1.4.tar.gz
   wget https://github.com/libarchive/libarchive/releases/download/3.5.1/libarchive\-3.5.1.tar.gz
 
   tar \-zxf libcircle\-0.3.0.tar.gz
@@ -273,14 +231,14 @@ cd deps
     make install
   cd ..
 
-  tar \-zxf lwgrp\-1.0.3.tar.gz
-  cd lwgrp\-1.0.3
+  tar \-zxf lwgrp\-1.0.4.tar.gz
+  cd lwgrp\-1.0.4
     ./configure \-\-prefix=$installdir
     make install
   cd ..
 
-  tar \-zxf dtcmp\-1.1.1.tar.gz
-  cd dtcmp\-1.1.1
+  tar \-zxf dtcmp\-1.1.4.tar.gz
+  cd dtcmp\-1.1.4
     ./configure \-\-prefix=$installdir \-\-with\-lwgrp=$installdir
     make install
   cd ..
@@ -296,105 +254,33 @@ cd ..
 .UNINDENT
 .UNINDENT
 .sp
-To build on PowerPC, one may need to add \fB\-\-build=powerpc64le\-redhat\-linux\-gnu\fP
-to the configure commands.
-.sp
-Assuming the dependencies have been placed in
-an \fIinstall\fP directory as shown above, build mpiFileUtils from a release like v0.10:
-.INDENT 0.0
-.INDENT 3.5
-.sp
-.nf
-.ft C
-wget https://github.com/hpc/mpifileutils/archive/v0.10.tar.gz
-tar \-zxf v0.10.tar.gz
-mkdir build install
-cd build
-cmake ../mpifileutils\-0.10 \e
-  \-DWITH_DTCMP_PREFIX=../install \e
-  \-DWITH_LibCircle_PREFIX=../install \e
-  \-DCMAKE_INSTALL_PREFIX=../install
-make install
-.ft P
-.fi
-.UNINDENT
-.UNINDENT
-.sp
-or to build the latest mpiFileUtils from the master branch:
+One can then clone, build, and install mpiFileUtils:
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-git clone \-\-depth 1 https://github.com/hpc/mpifileutils
-mkdir build install
+git clone https://github.com/hpc/mpifileutils
+mkdir build
 cd build
 cmake ../mpifileutils \e
   \-DWITH_DTCMP_PREFIX=../install \e
   \-DWITH_LibCircle_PREFIX=../install \e
+  \-DWITH_LibArchive_PREFIX=../install \e
   \-DCMAKE_INSTALL_PREFIX=../install
-make install
-.ft P
-.fi
-.UNINDENT
-.UNINDENT
-.sp
-build latest mpiFileUtils from the master branch with DAOS Support:
-.INDENT 0.0
-.INDENT 3.5
-.sp
-.nf
-.ft C
-git clone \-\-depth 1 https://github.com/hpc/mpifileutils
-mkdir build install
-cd build
-cmake ../mpifileutils \e
-  \-DWITH_DTCMP_PREFIX=../install \e
-  \-DWITH_LibCircle_PREFIX=../install \e
-  \-DCMAKE_INSTALL_PREFIX=../install \e
-  \-DWITH_CART_PREFIX=</path/to/daos/> \e
-  \-DWITH_DAOS_PREFIX=</path/to/daos/> \e
-  \-DENABLE_DAOS=ON
-make install
-.ft P
-.fi
-.UNINDENT
-.UNINDENT
-.sp
-The above build with DAOS option also assumes you have already installed DAOS. If
-CART and DAOS are installed under a standard system path then specifying the CART
-and DAOS paths is unnecessary.
-.sp
-To enable Lustre, GPFS, and experimental tools, add the following flags during CMake:
-.INDENT 0.0
-.INDENT 3.5
-.sp
-.nf
-.ft C
-\-DENABLE_LUSTRE=ON
-\-DENABLE_GPFS=ON
-\-DENABLE_EXPERIMENTAL=ON
+make \-j install
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
 .sp
-To disable linking against libarchive, and tools requiring libarchive, add the following flag during CMake:
-.INDENT 0.0
-.INDENT 3.5
-.sp
-.nf
-.ft C
-\-DENABLE_LIBARCHIVE=OFF
-.ft P
-.fi
-.UNINDENT
-.UNINDENT
-.SS Build mpiFileUtils directly, build its dependencies with Spack
+The same CMake options as described in earlier sections are available.
+.SS Development build with Spack
 .sp
-One can use Spack to install mpiFileUtils dependencies using the \fIspack.yaml\fP file distributed with mpiFileUtils.
-From the root directory of mpiFileUtils, run the command \fIspack find\fP to determine which packages spack will install.
-Next, run \fIspack concretize\fP to have spack perform dependency analysis.
+One can also build from a clone of the mpiFileUtils repository
+after using Spack to install its dependencies via the \fIspack.yaml\fP file that is distributed with mpiFileUtils.
+From the root directory of mpiFileUtils, run the command \fIspack find\fP to determine which packages Spack will install.
+Next, run \fIspack concretize\fP to have Spack perform dependency analysis.
 Finally, run \fIspack install\fP to build the dependencies.
 .sp
 There are two ways to tell CMake about the dependencies.
@@ -406,7 +292,7 @@ Thus, the commands to build become:
 .sp
 .nf
 .ft C
-git clone \-\-depth 1 https://github.com/hpc/mpifileutils
+git clone https://github.com/hpc/mpifileutils
 mkdir build install
 cd mpifileutils
 spack install
@@ -422,1853 +308,386 @@ cmake ../mpifileutils
 .sp
 The other way to use spack is to create a “view” to the installed dependencies.
 Details on this are coming soon.
-.SH MAN PAGES
-.SS dbcast
-.SS SYNOPSIS
-.sp
-\fBdbcast [OPTION] SRC DEST\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to recursively broadcast a single file from a
-global file system to node\-local storage, like ramdisk or an SSD.
-.sp
-The file is logically sliced into chunks and collectively copied from a
-global file system to node\-local storage. The source file SRC must be
-readable by all MPI processes. The destination file DEST should be the
-full path of the file in node\-local storage. If needed, parent
-directories for the destination file will be created as part of the
-broadcast.
-.sp
-In the current implementation, dbcast requires at least two MPI
-processes per compute node, and all compute nodes must run an equal
-number of MPI processes.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-s, \-\-size SIZE
-The chunk size in bytes used to segment files during the broadcast.
-Units like “MB” and “GB” should be immediately follow the number
-without spaces (ex. 2MB). The default size is 1MB. It is recommended
-to use the stripe size of a file if this is known.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print the command usage, and the list of options available.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To broadcast a file to /ssd on each node:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dbcast /global/path/to/filenane /ssd/filename\fP
-.INDENT 0.0
-.IP 2. 3
-Same thing, but slicing at 10MB chunks:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dbcast \-s 10MB /global/path/to/filenane /ssd/filename\fP
-.INDENT 0.0
-.IP 3. 3
-To read the current striping parameters of a file on Lustre:
-.UNINDENT
-.sp
-\fBlfs getstripe /global/path/to/filename\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dbz2
-.SS SYNOPSIS
+.SS Project Design Principles
 .sp
-\fBdbz2 [OPTIONS] [\-z|\-d] FILE\fP
-.SS DESCRIPTION
+The following principles drive design decisions in the project.
+.SS Scale
 .sp
-Parallel MPI application to compress or decompress a file.
+The library and tools should be designed such that running with more processes
+increases performance, provided there are sufficient data and parallelism
+available in the underlying file systems. The design of the tool should not
+impose performance scalability bottlenecks.
+.SS Performance
 .sp
-When compressing, a new file will be created with a .dbz2 extension.
-When decompressing, the .dbz2 extension will be dropped from the file name.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-z, \-\-compress
-Compress the file
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-d, \-\-decompress
-Decompress the file
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-k, \-\-keep
-Keep the input file.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-f, \-\-force
-Overwrite the output file, if it exists.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-b, \-\-blocksize SIZE
-Set the compression block size, from 1 to 9.
-Where 1=100kB … and 9=900kB. Default is 9.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Verbose output (optional).
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Quiet output
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print usage.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To compress a file:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dbz2 \-\-compress /path/to/file\fP
-.INDENT 0.0
-.IP 2. 3
-To compress a file and overwrite any existing output file:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dbz2 \-\-force \-\-compress /path/to/file\fP
-.INDENT 0.0
-.IP 3. 3
-To decompress a file:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dbz2 \-\-decompress /path/to/file.dbz2\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dchmod
-.SS SYNOPSIS
+While it is tempting to mimic the interface, behavior, and file formats of
+familiar tools like cp, rm, and tar, when forced with a choice between
+compatibility and performance, mpiFileUtils chooses performance. For example,
+if an archive file format requires serialization that inhibits parallel
+performance, mpiFileUtils will opt to define a new file format that enables
+parallelism rather than being constrained to existing formats. Similarly,
+options in the tool command line interface may have different semantics from
+familiar tools in cases where performance is improved. Thus, one should be
+careful to learn the options of each tool.
+.SS Portability
 .sp
-\fBdchmod [OPTION] PATH …\fP
-.SS DESCRIPTION
+The tools are intended to support common file systems used in HPC centers, like
+Lustre, GPFS, and NFS. Additionally, methods in the library should be portable
+and efficient across multiple file systems. Tool and library users can rely on
+mpiFileUtils to provide portable and performant implementations.
+.SS Composability
 .sp
-Parallel MPI application to recursively change permissions and/or group
-from a top level directory.
+While the tools do not support chaining with Unix pipes, they do support
+interoperability through input and output files. One tool may process a dataset
+and generate an output file that another tool can read as input, e.g., to walk
+a directory tree with one tool, filter the list of file names with another, and
+perhaps delete a subset of matching files with a third. Additionally, when logic
+is deemed to be useful across multiple tools or is anticipated to be useful in
+future tools or applications, it should be provided in the common library.
+.SS Utilities
 .sp
-dchmod provides functionality similar to \fBchmod(1)\fP, \fBchown(1)\fP, and \fBchgrp(1)\fP\&.
-Like \fBchmod(1)\fP, the tool supports the use of octal or symbolic mode to
-change the permissions.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-i, \-\-input FILE
-Read source list from FILE. FILE must be generated by another tool
-from the mpiFileUtils suite.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-u, \-\-owner USER
-Change owner to specified USER name or numeric user id.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-g, \-\-group GROUP
-Change group to specified GROUP name or numeric group id.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-m, \-\-mode MODE
-The mode to apply to each item. MODE may be octal or symbolic syntax
-similar to \fBchmod(1)\fP\&. In symbolic notation, “ugoa” are supported
-as are “rwxX”. As with chmod, if no leading letter “ugoa” is provided,
-mode bits are combined with umask to determine the actual mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-f, \-\-force
-Attempt to change every item.  By default, dchmod avoids unncessary
-chown and chmod calls, for example trying to change the group
-on an item that already has the correct group, or trying to change
-the group on an item that is not owned by the user running the tool.
-With –force, dchmod executes chown/chmod calls on every item.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-s, \-\-silent
-Suppress EPERM error messages, which is useful when running dchmod
-on large directories with files owned by other users.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-exclude REGEX
-Do not modify items whose full path matches REGEX, processed by
-\fBregexec(3)\fP\&.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-match REGEX
-Only modify items whose full path matches REGEX, processed by
-\fBregexec(3)\fP\&.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-n, \-\-name
-Change –exclude and –match to apply to item name rather than its
-full path.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode. Prints a list of statistics including the
-number of files walked, the number of levels there are in the
-directory tree, and the number of files the command operated on, and
-the files/sec rate for each of those.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print the command usage, and the list of options available.
-.UNINDENT
-.SS EXAMPLES
+The tools in mpiFileUtils are MPI applications. They must be launched
+as MPI applications, e.g., within a compute allocation on a cluster using
+mpirun. The tools do not currently checkpoint, so one must be careful that an
+invocation of the tool has sufficient time to complete before it is killed.
 .INDENT 0.0
-.IP 1. 3
-Use octal mode to change permissions:
+.IP \(bu 2
+dbcast \- Broadcast a file to each compute node.
+.IP \(bu 2
+dbz2 \- Compress and decompress a file with bz2.
+.IP \(bu 2
+dchmod \- Change owner, group, and permissions on files.
+.IP \(bu 2
+dcmp \- Compare contents between directories or files.
+.IP \(bu 2
+dcp \- Copy files.
+.IP \(bu 2
+ddup \- Find duplicate files.
+.IP \(bu 2
+dfind \- Filter files.
+.IP \(bu 2
+dreln \- Update symlinks to point to a new path.
+.IP \(bu 2
+drm \- Remove files.
+.IP \(bu 2
+dstripe \- Restripe files (Lustre).
+.IP \(bu 2
+dsync \- Synchronize source and destination directories or files.
+.IP \(bu 2
+dtar \- Create and extract tape archive files.
+.IP \(bu 2
+dwalk \- List, sort, and profile files.
 .UNINDENT
+.SS Experimental Utilities
 .sp
-\fBmpirun \-np 128 dchmod \-\-mode 755 /directory\fP
+Experimental utilities are under active development. They are not considered to
+be production worthy, but they are available in the distribution for those
+who are interested in developing them further or to provide additional examples.
 .INDENT 0.0
-.IP 2. 3
-Set group and mode in a single command using symbolic mode:
+.IP \(bu 2
+dgrep \- Run grep on files in parallel.
+.IP \(bu 2
+dparallel \- Perform commands in parallel.
+.IP \(bu 2
+dsh \- List and remove files with interactive commands.
+.IP \(bu 2
+dfilemaker \- Generate random files.
 .UNINDENT
+.SS Usage tips
 .sp
-\fBmpirun \-np 128 dchmod \-\-group mygroup \-\-mode u+r,g+rw /directory\fP
+Since the tools are MPI applications, it helps to keep a few things in mind:
 .INDENT 0.0
-.IP 3. 3
-Set owner and group, leaving permissions the same:
+.IP \(bu 2
+One typically needs to run the tools within a job allocation.  The sweet spot for most tools is about 2\-4 nodes.  One can use more nodes for large datasets, so long as tools scale sufficiently well.
+.IP \(bu 2
+One must launch the job using the MPI job launcher like mpirun or mpiexec.  One should use most CPU cores, though leave a few cores idle on each node for the file system client processes.
+.IP \(bu 2
+Most tools do not checkpoint their progress.  Be sure to request sufficient time in your allocation to allow the job to complete.  One may need to start over from the beginning if a tool is interrupted.
+.IP \(bu 2
+One cannot pipe output of one tool to the input of another.  However, the –input and –output file options are good approximations.
+.IP \(bu 2
+One cannot easily check the return codes of tools.  Instead, inspect stdout and stderr output for errors.
 .UNINDENT
+.SS Examples and frequently used commands
 .sp
-\fBmpirun \-np 128 dchmod \-\-owner user1 \-\-group mygroup /directory\fP
+If your MPI library supports it, most tools can run as MPI singletons (w/o mpirun, which runs a single\-task MPI job).
+For brevity, the examples in this section are shown as MPI singleton runs.
+In a real run, one would precede the command shown with an appropriate MPI launch command and options, e.g.,:
 .INDENT 0.0
-.IP 4. 3
-Change permissions to u+rw on all items EXCEPT those whose name match
-regex:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dchmod \-\-name \-\-exclude ‘afilename’ \-\-mode u+rw /directory\fP
-.sp
-Note: You can use –match to change file permissions on all of the
-files/directories that match the regex.
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dcmp
-.SS SYNOPSIS
-.sp
-\fBdcmp [OPTION] SRC DEST\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to compare two files or to recursively compare
-files with same relative paths within two different directories.
-.sp
-dcmp provides functionality similar to a recursive \fBcmp(1)\fP\&. It reports
-how many files in two different directories are the same or different.
+.INDENT 3.5
 .sp
-dcmp can be configured to compare a number of different file properties.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-o, \-\-output EXPR:FILE
-Writes list of files matching expression EXPR to specified FILE.
-The expression consists of a set of fields and states described below.
-More than one \-o option is allowed in a single invocation,
-in which case, each option should provide a different output file name.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-t, \-\-text
-Change –output to write files in text format rather than binary.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-b, \-\-base
-Enable base checks and normal stdout results when –output is used.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-bufsize SIZE
-Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-chunksize SIZE
-Multiple processes copy a large file in parallel by dividing it into chunks.
-Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-daos\-prefix PREFIX
-Specify the DAOS prefix to be used. This is only necessary
-if copying a subset of a POSIX container in DAOS using a
-Unified Namespace path.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-daos\-api API
-Specify the DAOS API to be used. By default, the API is automatically
-determined based on the container type, where POSIX containers use the
-DFS API, and all other containers use the DAOS object API.
-Values must be in {DFS, DAOS}.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-s, \-\-direct
-Use O_DIRECT to avoid caching file data.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode. Prints a list of statistics/timing data for the
-command. Files walked, started, completed, seconds, files, bytes
-read, byte rate, and file rate.
+.nf
+.ft C
+mpirun \-np 128 dwalk /path/to/walk
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
 .UNINDENT
+.sp
+In addition to the man page, each tool provides a help screen for a brief reminder of available options.:
 .INDENT 0.0
-.TP
-.B \-l, \-\-lite
-lite mode does a comparison of file modification time and size. If
-modification time and size are the same, then the contents are assumed
-to be the same. Similarly, if the modification time or size is different,
-then the contents are assumed to be different. The lite mode does no comparison
-of data/content in the file.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+dwalk \-\-help
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print the command usage, and the list of options available.
 .UNINDENT
-.SS EXPRESSIONS
-.sp
-An expression is made up of one or more conditions, where each condition specifies a field and a state.
-A single condition consists of a field name, an ‘=’ sign, and a state name.
-.sp
-Valid fields are listed below, along with the property of the entry that is checked.
-.TS
-center;
-|l|l|.
-_
-T{
-Field
-T}	T{
-Property of entry
-T}
-_
-T{
-EXIST
-T}	T{
-whether entry exists
-T}
-_
-T{
-TYPE
-T}	T{
-type of entry, e.g., regular file, directory, symlink
-T}
-_
-T{
-SIZE
-T}	T{
-size of entry in bytes, if a regular file
-T}
-_
-T{
-UID
-T}	T{
-user id of entry
-T}
-_
-T{
-GID
-T}	T{
-group id of entry
-T}
-_
-T{
-ATIME
-T}	T{
-time of last access
-T}
-_
-T{
-MTIME
-T}	T{
-time of last modification
-T}
-_
-T{
-CTIME
-T}	T{
-time of last status change
-T}
-_
-T{
-PERM
-T}	T{
-permission bits of entry
-T}
-_
-T{
-ACL
-T}	T{
-ACLs associated with entry, if any
-T}
-_
-T{
-CONTENT
-T}	T{
-file contents of entry, byte\-for\-byte comparision, if a regular file
-T}
-_
-.TE
-.sp
-Valid conditions for the EXIST field are:
-.TS
-center;
-|l|l|.
-_
-T{
-Condition
-T}	T{
-Meaning
-T}
-_
-T{
-EXIST=ONLY_SRC
-T}	T{
-entry exists only in source path
-T}
-_
-T{
-EXIST=ONLY_DEST
-T}	T{
-entry exists only in destination path
-T}
-_
-T{
-EXIST=DIFFER
-T}	T{
-entry exists in either source or destination, but not both
-T}
-_
-T{
-EXIST=COMMON
-T}	T{
-entry exists in both source and destination
-T}
-_
-.TE
-.sp
-All other fields may only specify the DIFFER and COMMON states.
-.sp
-Conditions can be joined together with AND (@) and OR (,) operators without spaces to build complex expressions.
-For example, the following expression reports entries that exist in both source and destination paths, but are of different types:
+.sp
+The normal output from dwalk shows a summary of item and byte counts.
+This is useful to determine the number of files and bytes under a path of interest:
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-EXIST=COMMON@TYPE=DIFFER
+dwalk /path/to/walk
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
 .sp
-The AND operator binds with higher precedence than the OR operator.
-For example, the following expression matches on entries which either (exist in both source and destination and whose types differ) or (only exist in the source):
+When walking large directory trees, you can write the list to an output file.
+Then you can read that list back without having to walk the file system again.:
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-EXIST=COMMON@TYPE=DIFFER,EXIST=ONLY_SRC
+dwalk \-\-output list.mfu /path/to/walk
+dwalk \-\-input list.mfu
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
 .sp
-Some conditions imply others.
-For example, for CONTENT to be considered the same,
-the entry must exist in both source and destination, the types must match, the sizes must match, and finally the contents must match:
+The default file format is a binary file intended for use in other tools, not humans, but one can ask for a text\-based output:
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-SIZE=COMMON    => EXISTS=COMMON@TYPE=COMMON@SIZE=COMMON
-CONTENT=COMMON => EXISTS=COMMON@TYPE=COMMON@SIZE=COMMON@CONTENT=COMMON
+dwalk \-\-text \-\-output list.txt /path/to/walk
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
 .sp
-A successful check on any other field also implies that EXIST=COMMON.
-.sp
-When used with the \-o option, one must also specify a file name at the end of the expression, separated with a ‘:’.
-The list of any entries that match the expression are written to the named file.
-For example, to list any entries matching the above expression to a file named outfile1,
-one should use the following option:
+The text\-based output is lossy, and it cannot be read back in to a tool.
+If you want both, save to binary format first, then read the binary file to convert it to text.:
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-\-o EXIST=COMMON@TYPE=DIFFER:outfile1
+dwalk \-\-output list.mfu /path/to/walk
+dwalk \-\-input list.mfu \-\-text \-\-output list.txt
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
 .sp
-If the –base option is given or when no output option is specified,
-the following expressions are checked and numeric results are reported to stdout:
+dwalk also provides a sort option to order items in the list in various ways,
+e.g., to order the list by username, then by access time:
 .INDENT 0.0
 .INDENT 3.5
 .sp
 .nf
 .ft C
-EXIST=COMMON
-EXIST=DIFFER
-EXIST=COMMON@TYPE=COMMON
-EXIST=COMMON@TYPE=DIFFER
-EXIST=COMMON@CONTENT=COMMON
-EXIST=COMMON@CONTENT=DIFFER
+dwalk \-\-input list.mfu \-\-sort user,atime \-\-output user_atime.mfu
 .ft P
 .fi
 .UNINDENT
 .UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-Compare two files in different directories:
-.UNINDENT
 .sp
-\fBmpirun \-np 128 dcmp /src1/file1 /src2/file2\fP
+To order items from largest to smallest number of bytes:
 .INDENT 0.0
-.IP 2. 3
-Compare two directories with verbose output. The verbose output prints timing and number of bytes read:
-.UNINDENT
+.INDENT 3.5
 .sp
-\fBmpirun \-np 128 dcmp \-v /src1 /src2\fP
-.INDENT 0.0
-.IP 3. 3
-Write list of entries to outfile1 that are only in src1 or whose names exist in both src1 and src2 but whose types differ:
+.nf
+.ft C
+dwalk \-\-input list.mfu \-\-sort \(aq\-size\(aq \-\-output big_to_small.mfu
+.ft P
+.fi
 .UNINDENT
-.sp
-\fBmpirun \-np 128 dcmp \-o EXIST=COMMON@TYPE=DIFFER,EXIST=ONLY_SRC:outfile1 /src1 /src2\fP
-.INDENT 0.0
-.IP 4. 3
-Same as above but also write list of entries to outfile2 that exist in either src1 or src2 but not both:
 .UNINDENT
 .sp
-\fBmpirun \-np 128 dcmp \-o EXIST=COMMON@TYPE=DIFFER,EXIST=ONLY_SRC:outfile1 \-o EXIST=DIFFER:outfile2 /src1 /src2\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dcp
-.SS SYNOPSIS
-.sp
-\fBdcp [OPTION] SRC DEST\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to recursively copy files and directories.
-.sp
-dcp is a file copy tool in the spirit of \fBcp(1)\fP that evenly
-distributes the work of scanning the directory tree, and copying file
-data across a large cluster without any centralized state.  It is
-designed for copying files that are located on a distributed parallel
-file system, and it splits large file copies across multiple processes.
-.SS OPTIONS
+dfind can be used to filter items with a string of find\-like expressions,
+e.g., files owned by user1 that are bigger than 100GB:
 .INDENT 0.0
-.TP
-.B \-\-bufsize SIZE
-Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+dfind \-\-input list.mfu \-\-user user1 \-\-size +100GB \-\-output user1_over_100GB.mfu
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-chunksize SIZE
-Multiple processes copy a large file in parallel by dividing it into chunks.
-Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
 .UNINDENT
+.sp
+dchmod is like chmod and chgrp in one, so one can change uid/gid/mode with a single command:
 .INDENT 0.0
-.TP
-.B \-\-daos\-prefix PREFIX
-Specify the DAOS prefix to be used. This is only necessary
-if copying a subset of a POSIX container in DAOS using a
-Unified Namespace path.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+dchmod \-\-group grp1 \-\-mode g+rw /path/to/walk
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-daos\-api API
-Specify the DAOS API to be used. By default, the API is automatically
-determined based on the container type, where POSIX containers use the
-DFS API, and all other containers use the DAOS object API.
-Values must be in {DFS, DAOS}.
 .UNINDENT
+.sp
+drm is like “rm \-rf” but in parallel:
 .INDENT 0.0
-.TP
-.B \-i, \-\-input FILE
-Read source list from FILE. FILE must be generated by another tool
-from the mpiFileUtils suite.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+drm /path/to/remove
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-L, \-\-dereference
-Dereference symbolic links and copy the target file or directory
-that each symbolic link refers to.
 .UNINDENT
+.sp
+dbcast provides an efficient way to broadcast a file to all compute nodes,
+e.g., upload a tar file of a dataset to an SSD local to each compute node:
 .INDENT 0.0
-.TP
-.B \-P, \-\-no\-dereference
-Do not follow symbolic links in source paths. Effectviely allows
-symbolic links to be copied when the link target is not valid
-or there is not permission to read the link’s target.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+dbcast /path/to/file.dat /ssd/file.dat
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-p, \-\-preserve
-Preserve permissions, group, timestamps, and extended attributes.
 .UNINDENT
+.sp
+dsync is the recommended way to make a copy a large set of files:
 .INDENT 0.0
-.TP
-.B \-s, \-\-direct
-Use O_DIRECT to avoid caching file data.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+dsync /path/src /path/dest
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-S, \-\-sparse
-Create sparse files when possible.
 .UNINDENT
+.sp
+For large directory trees, the –batch\-files option offers a type of checkpoint.
+It moves files in batches, and if interrupted, a restart picks up from the last completed batch.:
 .INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+dsync \-\-batch\-files 100000 /path/src /path/dest
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
 .UNINDENT
+.sp
+The tools can be composed in various ways using the –input and –output options.
+For example, the following sequence of commands executes a purge operation,
+which deletes any file that has not been accessed in the past 180 days.:
 .INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
+.INDENT 3.5
+.sp
+.nf
+.ft C
+# walk directory to stat all files, record list in file
+dwalk \-\-output list.mfu /path/to/walk
+
+# filter list to identify all regular files that were last accessed over 180 days ago
+dfind \-\-input list.mfu \-\-type f \-\-atime +180 \-\-output purgelist.mfu
+
+# delete all files in the purge list
+drm \-\-input purgelist.mfu
+.ft P
+.fi
 .UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdcp(1)\fP options and usage.
 .UNINDENT
-.SS RESTRICTIONS
+.SS Common Library \- libmfu
 .sp
-If a long\-running copy is interrupted, one should delete the partial
-copy and run dcp again from the beginning. One may use drm to quickly
-remove a partial copy of a large directory tree.
+Functionality that is common to multiple tools is moved to the common library,
+libmfu. This goal of this library is to make it easy to develop new tools and
+to provide consistent behavior across tools in the suite. The library can also
+be useful to end applications, e.g., to efficiently create or remove a large
+directory tree in a portable way across different parallel file systems.
+.SS libmfu: the mpiFileUtils common library
 .sp
-To ensure the copy is successful, one should run dcmp after dcp
-completes to verify the copy, especially if dcp was not run with the \-s
-option.
-.SS EXAMPLES
+The mpiFileUtils common library defines data structures and methods on those
+data structures that makes it easier to develop new tools or for use within HPC
+applications to provide portable, performant implementations across file
+systems common in HPC centers.
 .INDENT 0.0
-.IP 1. 3
-To copy dir1 as dir2:
-.UNINDENT
+.INDENT 3.5
 .sp
-\fBmpirun \-np 128 dcp /source/dir1 /dest/dir2\fP
-.INDENT 0.0
-.IP 2. 3
-To copy contents of dir1 into dir2:
+.nf
+.ft C
+#include "mfu.h"
+.ft P
+.fi
 .UNINDENT
-.sp
-\fBmkdir /dest/dir2 mpirun \-np 128 dcp /source/dir1/\e* /dest/dir2\fP
-.INDENT 0.0
-.IP 3. 3
-To copy while preserving permissions, group, timestamps, and
-attributes:
 .UNINDENT
 .sp
-\fBmpirun \-np 128 dcp \-p /source/dir1/ /dest/dir2\fP
-.SS KNOWN BUGS
-.sp
-Using the \-S option for sparse files does not work yet at LLNL. If you
-try to use it then dcp will default to a normal copy.
-.sp
-The maximum supported file name length for any file transferred is
-approximately 4068 characters. This may be less than the number of
-characters that your operating system supports.
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS ddup
-.SS SYNOPSIS
+This file includes all other necessary headers.
+.SS mfu_flist
 .sp
-\fBddup [OPTION] PATH\fP
-.SS DESCRIPTION
+The key data structure in libmfu is a distributed file list called mfu_flist.
+This structure represents a list of files, each with stat\-like metadata, that
+is distributed among a set of MPI ranks.
 .sp
-Parallel MPI application to report files under a directory tree having identical content.
-.sp
-ddup reports path names to files having identical content (duplicate files).
-A top\-level directory is specified, and the path name to any file that is a duplicate
-of another anywhere under that same directory tree is reported.
-The path to each file is reported, along with a final hash representing its content.
-Multiple sets of duplicate files can be matched using this final reported hash.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-d, \-\-debug LEVEL
-Set verbosity level.  LEVEL can be one of: fatal, err, warn, info, dbg.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print the command usage, and the list of options available.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To report any duplicate files under a directory tree:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 ddup /path/to/haystack\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dfind
-.SS SYNOPSIS
-.sp
-\fBdfind [OPTION] [EXPRESSION] PATH …\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to filter a list of files according to an expression.
-.sp
-dfind provides functionality similar to \fBfind(1)\fP\&.
-.sp
-The file list can be obtained by either walking one or more paths provided on the command line or through an input list.
-.sp
-The filtered list can be written to an output file.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-i, \-\-input FILE
-Read source list from FILE. FILE must be generated by another tool
-from the mpiFileUtils suite.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-o, \-\-output FILE
-Write the processed list to a file.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdfind(1)\fP options and usage.
-.UNINDENT
-.SS EXPRESSIONS
-.sp
-Numeric arguments can be specified as:
-.INDENT 0.0
-.INDENT 3.5
-.TS
-center;
-|l|l|.
-_
-T{
-+N
-T}	T{
-more than N
-T}
-_
-T{
-\-N
-T}	T{
-less than N
-T}
-_
-T{
-N
-T}	T{
-exactly N
-T}
-_
-.TE
-.UNINDENT
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-amin N
-File was last accessed N minutes ago.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-anewer FILE
-File was last accessed more recently than FILE was modified.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-atime N
-File was last accessed N days ago.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-cmin N
-File’s status was last changed N minutes ago.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-cnewer FILE
-File’s status was last changed more recently than FILE was modified.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-ctime N
-File’s status was last changed N days ago.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-mmin N
-File’s data was last modified N minutes ago.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-newer FILE
-File was modified more recently than FILE.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-mtime N
-File’s data was last modified N days ago.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-gid N
-File’s numeric group ID is N.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-group NAME
-File belongs to group NAME.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-uid N
-File’s numeric user ID is N.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-user NAME
-File is owned by user NAME.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-name PATTERN
-Base of file name matches shell pattern PATTERN.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-path PATTERN
-Full path to file matches shell pattern PATTERN.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-regex REGEX
-Full path to file matches POSIX regular expression REGEX.  Regular expressions processed by \fBregexec(3)\fP\&.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-size N
-File size is N bytes.  Units can be used like ‘KB’, ‘MB’, ‘GB’.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-type C
-File is of type C:
-.TS
-center;
-|l|l|.
-_
-T{
-b
-T}	T{
-block device
-T}
-_
-T{
-c
-T}	T{
-char device
-T}
-_
-T{
-d
-T}	T{
-directory
-T}
-_
-T{
-f
-T}	T{
-regular file
-T}
-_
-T{
-l
-T}	T{
-symbolic link
-T}
-_
-T{
-p
-T}	T{
-pipe
-T}
-_
-T{
-s
-T}	T{
-socket
-T}
-_
-.TE
-.UNINDENT
-.SS ACTIONS
-.INDENT 0.0
-.TP
-.B \-\-print
-Print file name to stdout.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-exec CMD ;
-Execute command CMD on file.  All following arguments are taken as arguments to the command until ‘;’ is encountered.  The string ‘{}’ is replaced by the current file name.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-Print all files owner by user1 under given path:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dfind \-v \-\-user user1 \-\-print /path/to/target\fP
-.INDENT 0.0
-.IP 2. 3
-To find all files less than 1GB and write them to a file:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dfind \-v \-o outfile \-\-size \-1GB /path/to/target\fP
-.INDENT 0.0
-.IP 3. 3
-Filter list in infile to find all regular files not changed in the past 180 days and write new list to outfile:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dfind \-v \-i infile \-o outfile \-\-type f \-\-mtime +180\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dreln
-.SS SYNOPSIS
-.sp
-\fBdreln [OPTION] OLDPATH NEWPATH PATH …\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to recursively update symlinks within a
-directory.
-.sp
-dreln walks the specified PATH and updates any symlink whose target
-includes an absolute path to OLDPATH and replaces that symlink
-with a new link whose target points to NEWPATH instead.
-.sp
-This is useful to update symlinks after migrating a large
-directory from one file system to another, whose links specify
-absolute paths to the original file system.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-i, \-\-input FILE
-Read source list from FILE. FILE must be generated by another tool
-from the mpiFileUtils suite.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-p, \-\-preserve
-Preserve existing modification times on links.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-r, \-\-relative
-Replace links using target paths that are relative to NEWPATH.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdrm(1)\fP options and usage.
-.UNINDENT
-.SS EXAMPLES
-.sp
-1. To update all links under /walk/path whose targets point to /orig/path
-and replace them with targets that point to /new/path:
-.sp
-\fBmpirun \-np 128 dreln \-v /orig/path /new/path /walk/path\fP
-.sp
-2. Same as above, but replace each link target with a relative path
-from the link to its new target under /new/path:
-.sp
-\fBmpirun \-np 128 dreln \-v \-\-relative /orig/path /new/path /walk/path\fP
-.INDENT 0.0
-.IP 3. 3
-One can preserve existing modification times on links:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dreln \-v \-\-preserve /orig/path /new/path /walk/path\fP
-.INDENT 0.0
-.IP 4. 3
-One can specifiy multiple paths to walk:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dreln \-v /orig/path /new/path /walk/path1 /walk/path2\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS drm
-.SS SYNOPSIS
-.sp
-\fBdrm [OPTION] PATH…\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to recursively delete a directory and its
-contents.
-.sp
-drm is a tool for removing files recursively in parallel.
-drm behaves like \fIrm \-rf\fP, but it is faster.
-.sp
-\fBNOTE:\fP
-.INDENT 0.0
-.INDENT 3.5
-DO NOT USE SHELL REGEX!!!
-The –match and –exclude options use POSIX regex syntax. Because of
-this make sure that the shell does not try to interpret your regex before
-it gets passed to the program. You can generally use quotes around your
-regex to prevent the shell from expanding. An example of this using the
-–match option with –dryrun would be:
-.sp
-\fBmpirun \-np 128 drm \-\-dryrun \-v \-\-name \-\-match \(aqfile_.*\(aq /path/to/dir/*\fP
-.UNINDENT
-.UNINDENT
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-i, \-\-input FILE
-Read source list from FILE. FILE must be generated by another tool
-from the mpiFileUtils suite.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-o, \-\-output FILE
-Write the list of items drm attempts to delete to FILE in mpiFileUtils format.
-Format can be changed with –text option.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-t, \-\-text
-Must be used with the –output option. Write list of items drm attempts
-to delete to FILE in ascii text format.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-l, \-\-lite
-Walk file system without stat.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-stat
-Walk file system with stat.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-exclude REGEX
-Do not remove items whose full path matches REGEX, processed by \fBregexec(3)\fP\&.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-match REGEX
-Only remove items whose full path matches REGEX, processed by
-\fBregexec(3)\fP\&.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-name
-Change –exclude and match to apply to item name rather than its
-full path.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-dryrun
-Print a list of files that \fBwould\fP be deleted without deleting
-them. This is useful to check list of items satisfying –exclude or
-–match options before actually deleting anything.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-aggressive
-This option will delete files during the walk phase, and then
-delete directories by level after the walk in drm. You cannot
-use this option with –dryrun.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-T, \-\-traceless
-Delete child items without updating the mtime on their parent directory.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdrm(1)\fP options and usage.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To delete a directory and its contents:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 drm \-v /dir/to/delete\fP
-.INDENT 0.0
-.IP 2. 3
-Delete all items (files and directories) ending with .core from
-directory tree:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 drm \-\-match \(aq.core$\(aq /dir/to/delete/from\fP
-.INDENT 0.0
-.IP 3. 3
-List items that would be deleted without removing them:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 drm \-\-dryrun \-\-match \(aq.core$\(aq /dir/to/delete/from\fP
-.INDENT 0.0
-.IP 4. 3
-Delete all items named foo:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 drm \-\-name \-\-match \(aq^foo$\(aq /dir/to/delete/from\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dstripe
-.SS SYNOPSIS
-.sp
-\fBdstripe [OPTION] PATH…\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to restripe files.
-.sp
-This tool is in active development. It currently only works on Lustre.
-.sp
-dstripe enables one to restripe file(s) across the underlying storage
-devices. One must specify a list of paths. All files in those paths can
-be restriped. By default, stripe size is 1MB and stripe count is \-1
-allowing dstripe to use all available stripes.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-c, \-\-count STRIPE_COUNT
-The number of stripes to use during file restriping. If STRIPE_COUNT
-is \-1, then all available stripes are used. If STRIPE_COUNT is 0,
-the lustre file system default is used. The default stripe count is
-\-1.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-s, \-\-size STRIPE_SIZE
-The stripe size to use during file restriping. Units like “MB” and
-“GB” can immediately follow the number without spaces (ex. 2MB). The
-default stripe size is 1MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-m, \-\-minsize SIZE
-The minimum size a file must be to be a candidate for restriping.
-Files smaller than SIZE will not be restriped. Units like “MB” and
-“GB” can immediately follow the number without spaces (ex. 2MB). The
-default minimum file size is 0MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-r, \-\-report
-Display the file size, stripe count, and stripe size of all files
-found in PATH. No restriping is performed when using this option.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print the command usage, and the list of options available.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To stripe a file on all storage devices using a 1MB stripe size:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dstripe \-s 1MB /path/to/file\fP
-.INDENT 0.0
-.IP 2. 3
-To stripe a file across 20 storage devices with a 1GB stripe size:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dstripe \-c 20 \-s 1GB /path/to/file\fP
-.INDENT 0.0
-.IP 3. 3
-To restripe all files in /path/to/files/ that are at least 1GB in
-size:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dstripe \-m 1GB /path/to/files/\fP
-.INDENT 0.0
-.IP 4. 3
-To restripe all files in /path/to/files/ across 10 storage devices
-with 2MB stripe size:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dstripe \-c 10 \-s 2MB /path/to/files/\fP
-.INDENT 0.0
-.IP 5. 3
-To display the current stripe count and stripe size of all files in
-/path/to/files/:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dstripe \-r /path/to/files/\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dsync
-.SS SYNOPSIS
-.sp
-\fBdsync [OPTION] SRC DEST\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to synchronize two files or two directory trees.
-.sp
-dsync makes DEST match SRC, adding missing entries from DEST, and updating
-existing entries in DEST as necessary so that SRC and DEST have identical
-content, ownership, timestamps, and permissions.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-\-dryrun
-Show differences without changing anything.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-b, \-\-batch\-files N
-Batch files into groups of up to size N during copy operation.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-bufsize SIZE
-Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-chunksize SIZE
-Multiple processes copy a large file in parallel by dividing it into chunks.
-Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-daos\-prefix PREFIX
-Specify the DAOS prefix to be used. This is only necessary
-if copying a subset of a POSIX container in DAOS using a
-Unified Namespace path.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-daos\-api API
-Specify the DAOS API to be used. By default, the API is automatically
-determined based on the container type, where POSIX containers use the
-DFS API, and all other containers use the DAOS object API.
-Values must be in {DFS, DAOS}.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-c, \-\-contents
-Compare files byte\-by\-byte rather than checking size and mtime
-to determine whether file contents are different.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-D, \-\-delete
-Delete extraneous files from destination.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-L, \-\-dereference
-Dereference symbolic links and copy the target file or directory
-that each symbolic link refers to.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-P, \-\-no\-dereference
-Do not follow symbolic links in source paths. Effectviely allows
-symbolic links to be copied when the link target is not valid
-or there is not permission to read the link’s target.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-s, \-\-direct
-Use O_DIRECT to avoid caching file data.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-link\-dest DIR
-Create hardlink in DEST to files in DIR when file is unchanged
-rather than create a new file. One can use this option to conserve
-storage space during an incremental backup.
-.sp
-For example in the following, any file that would be copied from
-/src to /src.bak.inc that is the same as the file already existing
-in /src.bak will instead be hardlinked to the file in /src.bak:
-.sp
-# initial backup of /src
-dsync /src /src.bak
-.sp
-# incremental backup of /src
-dsync –link\-dest /src.bak /src /src.bak.inc
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-S, \-\-sparse
-Create sparse files when possible.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode. Prints a list of statistics/timing data for the
-command. Files walked, started, completed, seconds, files, bytes
-read, byte rate, and file rate.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print the command usage, and the list of options available.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-Synchronize dir2 to match dir1:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dsync /path/to/dir1 /path/to/dir2\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dwalk
-.SS SYNOPSIS
-.sp
-\fBdwalk [OPTION] PATH …\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to recursively walk and list contents in a
-directory.
-.sp
-dwalk provides functionality similar to \fBls(1)\fP and \fBdu(1)\fP\&. Like
-\fBdu(1)\fP, the tool reports a summary of the total number of files and
-bytes. Like \fBls(1)\fP, the tool sorts and prints information about
-individual files.
-.sp
-The output can be sorted on different fields (e.g, name, user, group,
-size, etc). A histogram of file sizes can be computed listing the number
-of files that fall into user\-defined bins.
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-i, \-\-input FILE
-Read source list from FILE. FILE must be generated by another tool
-from the mpiFileUtils suite.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-o, \-\-output FILE
-Write the processed list to FILE in binary format. Format can be changed
-With –text option.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-t, \-\-text
-Must be used with the –output option. Write processed list of files to
-FILE in ascii text format.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-l, \-\-lite
-Walk file system without stat.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-s, \-\-sort FIELD
-Sort output by comma\-delimited fields (see below).
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-d, \-\-distribution size:SEPARATORS
-Print the distribution of file sizes. For example, specifying
-size:0,80,100 will report the number of files that have size 0
-bytes, between 1\-80 bytes, between 81\-99 bytes, and 100 bytes or
-greater.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-f, \-\-file\-histogram
-Creates a file histogram without requiring the user to provide
-the bin sizes. The bins are created dynamically based on the
-max file size. The first bin is always for only zero byte
-files, and the rest go up until the max file size is included
-in the very last bin. It always goes up by orders of magnitude
-in powers of two. So, an example of bin separators would be:
-0, 2^10, 2^20, 2^30. Assuming the max file size was somewhere
-within the 2^20 \- 2^30 range. The histogram also includes both
-files and directories.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-p, \-\-print
-Print files to the screen.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-L, \-\-dereference
-Dereference symbolic links and walk the target file or directory
-that each symbolic link refers to.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print usage.
-.UNINDENT
-.SS SORT FIELDS
-.sp
-By default, the list of files dwalk captures is not sorted. To sort the
-list, one or more fields can be specified in a comma\-delimited list:
-.sp
-name,user,group,uid,gid,atime,mtime,ctime,size
-.sp
-A field name can be preceded with ‘\-’ to sort by that field in reverse
-order.
-.sp
-A lexicographic sort is executed if more than one field is given.
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To print summary information for a directory:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dwalk \-v /dir/to/walk\fP
-.INDENT 0.0
-.IP 2. 3
-To print a list of files, sorted by file size, then by file name:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dwalk –print –sort size,name /dir/to/walk\fP
-.INDENT 0.0
-.IP 3. 3
-To save the list of files:
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dwalk –output out.dwalk /dir/to/walk\fP
-.INDENT 0.0
-.IP 4. 3
-Print the file distribution for specified histogram based on the size
-field from the top level directory.
-.UNINDENT
-.sp
-\fBmpirun \-np 128 dwalk \-v –print \-d size:0,20,1G src/\fP
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dgrep
-.SS SYNOPSIS
-.sp
-dgrep …
-.SS DESCRIPTION
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdgrep(1)\fP options and usage.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-version
-Print version information and exit.
-.UNINDENT
-.SS Known bugs
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dparallel
-.SS SYNOPSIS
-.sp
-dparallel …
-.SS DESCRIPTION
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdparallel(1)\fP options and usage.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-version
-Print version information and exit.
-.UNINDENT
-.SS Known bugs
-.SS SEE ALSO
-.sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SS dtar
-.SS SYNOPSIS
-.sp
-\fBdtar [OPTION] \-c \-f ARCHIVE SOURCE…\fP
-.sp
-\fBdtar [OPTION] \-x \-f ARCHIVE\fP
-.SS DESCRIPTION
-.sp
-Parallel MPI application to create and extract tar files.
-.sp
-dtar writes archives in pax file format.
-In addition to the archive file, dtar creates an index to record
-the number of items and the starting byte offset of each entry within the archive.
-This index enables faster parallel extraction.
-By default, dtar appends its index as the last entry of the archive.
-Optionally, the index may be written as a separate file (with a .dtaridx extension)
-or as an extended attribute (named user.dtar.idx) of the archive file.
-.sp
-dtar can extract archives in various tar formats, including archive files that were created by other tools like tar.
-dtar can also extract archives that have been compressed with gzip, bz2, or compress.
-Compressed archives are significantly slower to extract than uncompressed archives,
-because decompression inhibits available parallelism.
-.sp
-Archives are extracted fastest when a dtar index exists.
-If an index does not exist, dtar can create and record an index
-during extraction to benefit subsequent extractions of the same archive file.
-.sp
-When extracting an archive, dtar skips the entry corresponding to its index.
-If other tools, like tar, are used to extract the archive, the index
-entry is extracted as a regular file that is placed in the current working directory
-with a file extension of “.dtaridx” and having the same basename as the original archive file.
-For an archive that was named “file.tar” when it was created, the dtar index file is named “file.tar.dtaridx”.
-.SS LIMITATIONS
-.sp
-dtar only supports directories, regular files, and symlinks.
-.sp
-dtar works best on Lustre and GPFS.
-There are no known restrictions for creating or extracting archives on these file systems.
-These file systems also deliver the highest bandwidth and file create rates.
-.sp
-dtar can be used on NFS, but there is one key restriction.
-Namely, one should not create an archive file in NFS.
-To create an archive of NFS files, the archive file itself should be written to a directory in Lustre or GPFS.
-The dtar tool writes to an archive file from multiple processes in parallel,
-and the algorithms used to write the archive are not valid for NFS.
-.sp
-dtar can be used to extract an archive file into NFS.
-The archive file that is being extracted may be on any file system.
-.sp
-The target items to be archived must be under the current working directory where dtar is running, so commands like these work.
-.sp
-\fBdtar \-cf foo.tar foo/\fP
+The library contains functions for creating and operating on these lists. For
+example, one may create a list by recursively walking an existing directory or
+by inserting new entries one at a time. Given a list as input, functions exist
+to create corresponding entries (inodes) on the file system or to delete the
+list of files. One may filter, sort, and remap entries. One can copy a list of
+entries from one location to another or compare corresponding entries across
+two different lists. A file list can be serialized and written to or read from
+a file.
 .sp
-\fBdtar \-cf foo.tar dir/foo/\fP
+Each MPI rank “owns” a portion of the list, and there are routines to step
+through the entries owned by that process. This portion is referred to as the
+“local” list. Functions exist to get and set properties of the items in the
+local list, for example to get the path name, type, and size of a file.
+Functions dealing with the local list can be called by the MPI process
+independently of other MPI processes.
 .sp
-But commands like the following are not supported:
+Other functions operate on the global list in a collective fashion, such as
+deleting all items in a file list. All processes in the MPI job must invoke
+these functions simultaneously.
 .sp
-\fBdtar \-cf foo.tar ../foo/\fP
+For full details, see \fI\%mfu_flist.h\fP
+and refer to its usage in existing tools.
+.SS mfu_path
 .sp
-\fBdtar \-cf foo.tar /some/other/absolute/path/to/foo/\fP
-.SS OPTIONS
-.INDENT 0.0
-.TP
-.B \-c, \-\-create
-Create a tar archive.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-x, \-\-extract
-Extract a tar archive.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-f, \-\-file NAME
-Name of archive file.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-C, \-\-chdir DIR
-Change directory to DIR before executing.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-preserve\-owner
-Apply recorded owner and group to extracted files.
-Default uses effective uid/gid of the running process.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-preserve\-times
-Apply recorded atime and mtime to extracted files.
-Default uses current system times.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-preserve\-perms
-Apply recorded permissions to extracted files.
-Default subtracts umask from file permissions.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-preserve\-xattrs
-Record extended attributes (xattrs) when creating archive.
-Apply recorded xattrs to extracted files.
-Default does not record or extract xattrs.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-fsync
-Call fsync before closing files after writing.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-bufsize SIZE
-Set the I/O buffer to be SIZE bytes.  Units like “MB” and “GB” may
-immediately follow the number without spaces (eg. 8MB). The default
-bufsize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-chunksize SIZE
-Multiple processes copy a large file in parallel by dividing it into chunks.
-Set chunk to be at minimum SIZE bytes.  Units like “MB” and
-“GB” can immediately follow the number without spaces (eg. 64MB).
-The default chunksize is 64MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-memsize SIZE
-Set the memory limit to be SIZE bytes when reading archive files.
-For some archives, dtar can distribute the file across processes
-to store segments of the archive in memory for faster processing.
-Units like “MB” and “GB” may immediately follow the number
-without spaces (eg. 8MB). The default is 256MB.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-\-progress N
-Print progress message to stdout approximately every N seconds.
-The number of seconds must be a non\-negative integer.
-A value of 0 disables progress messages.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-v, \-\-verbose
-Run in verbose mode.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-q, \-\-quiet
-Run tool silently. No output is printed.
-.UNINDENT
-.INDENT 0.0
-.TP
-.B \-h, \-\-help
-Print a brief message listing the \fBdtar(1)\fP options and usage.
-.UNINDENT
-.SS EXAMPLES
-.INDENT 0.0
-.IP 1. 3
-To create an archive of dir named dir.tar:
-.UNINDENT
+mpiFileUtils represents file paths with the \fI\%mfu_path\fP
+structure. Functions are available to manipulate paths to prepend and append
+entries, to slice paths into pieces, and to compute relative paths.
+.SS mfu_param_path
 .sp
-\fBmpirun \-np 128 dtar \-c \-f dir.tar dir/\fP
-.INDENT 0.0
-.IP 2. 3
-To extract an archive named dir.tar:
-.UNINDENT
+Path names provided by the user on the command line (parameters) are handled
+through the \fI\%mfu_param_path\fP
+structure. Such paths may have to be checked for existence and to determine
+their type (file or directory). Additionally, the user may specify many such
+paths through invocations involving shell wildcards, so functions are available
+to check long lists of paths in parallel.
+.SS mfu_io
 .sp
-\fBmpirun \-np 128 dtar \-x \-f dir.tar\fP
-.SS SEE ALSO
+The \fI\%mfu_io.h\fP
+functions provide wrappers for many POSIX\-IO functions. This is helpful for
+checking error codes in a consistent manner and automating retries on failed
+I/O calls. One should use the wrappers in mfu_io if available, and if not, one
+should consider adding the missing wrapper.
+.SS mfu_util
 .sp
-The mpiFileUtils source code and all documentation may be downloaded
-from <\fI\%https://github.com/hpc/mpifileutils\fP>
-.SH INDICES AND TABLES
-.INDENT 0.0
-.IP \(bu 2
-genindex
-.IP \(bu 2
-search
-.UNINDENT
+The \fI\%mfu_util.h\fP
+functions provide wrappers for error reporting and memory allocation.
 .SH AUTHOR
 HPC
 .SH COPYRIGHT
-2021, LLNL/LANL/UT-Battelle/DDN
+2022, LLNL/LANL/UT-Battelle/DDN
 .\" Generated by docutils manpage writer.
 .
diff --git a/mpifileutils.spec b/mpifileutils.spec
index dfbdcb9..b3a0f81 100644
--- a/mpifileutils.spec
+++ b/mpifileutils.spec
@@ -1,5 +1,5 @@
 Name:		mpifileutils
-Version:	0.10.1
+Version:	0.11.1
 Release:	1%{?dist}
 Summary:	File utilities designed for scalability and performance.
 
diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 2f83d2d..775c70f 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -15,6 +15,10 @@ ADD_SUBDIRECTORY(dreln)
 ADD_SUBDIRECTORY(drm)
 ADD_SUBDIRECTORY(dstripe)
 ADD_SUBDIRECTORY(dsync)
+IF(ENABLE_DAOS AND ENABLE_HDF5)
+  ADD_SUBDIRECTORY(daos-serialize)
+  ADD_SUBDIRECTORY(daos-deserialize)
+ENDIF(ENABLE_DAOS AND ENABLE_HDF5)
 IF(ENABLE_LIBARCHIVE)
   ADD_SUBDIRECTORY(dtar)
 ENDIF(ENABLE_LIBARCHIVE)
@@ -22,6 +26,9 @@ ADD_SUBDIRECTORY(dwalk)
 
 # experimental tools
 IF(ENABLE_EXPERIMENTAL)
+  IF(ENABLE_DAOS)
+    ADD_SUBDIRECTORY(daos-gen)
+  ENDIF(ENABLE_DAOS)
   ADD_SUBDIRECTORY(dfilemaker)
   ADD_SUBDIRECTORY(dgrep)
   ADD_SUBDIRECTORY(dparallel)
diff --git a/src/common/CMakeLists.txt b/src/common/CMakeLists.txt
index 4d5b7d1..509ba35 100644
--- a/src/common/CMakeLists.txt
+++ b/src/common/CMakeLists.txt
@@ -1,5 +1,5 @@
 # Version for the shared mfu library
-set(MFU_VERSION_MAJOR 3) # Incompatible API changes
+set(MFU_VERSION_MAJOR 4) # Incompatible API changes
 set(MFU_VERSION_MINOR 0) # Backwards-compatible functionality
 set(MFU_VERSION_PATCH 0) # Backwards-compatible fixes
 set(MFU_VERSION ${MFU_VERSION_MAJOR}.${MFU_VERSION_MINOR}.${MFU_VERSION_PATCH})
diff --git a/src/common/mfu_bz2_static.c b/src/common/mfu_bz2_static.c
index 84c62a6..62293ba 100644
--- a/src/common/mfu_bz2_static.c
+++ b/src/common/mfu_bz2_static.c
@@ -476,10 +476,10 @@ int mfu_decompress_bz2_static(const char* src_name, const char* dst_name)
     int64_t block_meta  = (int64_t)footer[0]; /* offset to start of block metadata */
     int64_t block_total = (int64_t)footer[1]; /* number of blocks */
     int64_t block_size  = (int64_t)footer[2]; /* max uncompressed size of a block */
-    int64_t data_size   = (int64_t)footer[3]; /* uncompressed size of all blocks */
+                                              /* (int64_t)footer[3] - unused - uncompressed size of all blocks */
     uint64_t version    = footer[4];          /* dbz2 file format footer version */
     uint64_t magic      = footer[5];          /* dbz2 file format magic value */
-    int64_t filesize    = (int64_t)footer[6]; /* file size of compressed file */
+                                              /* (int64_t)footer[6] - unused - file size of compressed file */
 
     /* check that we got correct magic value */
     if (magic != 0x3141314131413141) {
diff --git a/src/common/mfu_compress_bz2_libcircle.c b/src/common/mfu_compress_bz2_libcircle.c
index c5862af..3e37680 100644
--- a/src/common/mfu_compress_bz2_libcircle.c
+++ b/src/common/mfu_compress_bz2_libcircle.c
@@ -182,7 +182,6 @@ static void find_wave_size(int64_t size, int opts_memory)
      * memory required to do compression, keep 128B for variables,etc.
      * The block itself must be in memory before compression. */
     int64_t wave_size_approx = mem_limit - (int64_t)info.totalram * 2 / 100 - 8 * size - 400 * 1024 - 128 - size;
-    int64_t waves_blocks_approx = wave_size_approx / comp_buff_size;
     int64_t wave_size = wave_size_approx - 2 * tot_blocks * sizeof(struct block_info);
     blocks_pn_pw = (int64_t)(0.4 * wave_size / comp_buff_size);
     if (blocks_pn_pw > 800) {
diff --git a/src/common/mfu_daos.c b/src/common/mfu_daos.c
index c9ebc02..b870978 100644
--- a/src/common/mfu_daos.c
+++ b/src/common/mfu_daos.c
@@ -7,18 +7,182 @@
 
 #include <sys/types.h>
 #include <sys/stat.h>
+#include <sys/wait.h>
 #include <unistd.h>
 
 #include <daos_fs.h>
 #include <daos_uns.h>
 #include <gurt/common.h>
 #include <gurt/hash.h>
+#include <libgen.h>
+
+#ifdef HDF5_SUPPORT
+#include <hdf5.h>
+#if H5_VERS_MAJOR == 1 && H5_VERS_MINOR < 12
+#define H5Sencode1 H5Sencode
+#endif
+#endif
+
+#if defined(DAOS_API_VERSION_MAJOR) && defined(DAOS_API_VERSION_MINOR)
+#define CHECK_DAOS_API_VERSION(major, minor)                            \
+        ((DAOS_API_VERSION_MAJOR > (major))                             \
+         || (DAOS_API_VERSION_MAJOR == (major) && DAOS_API_VERSION_MINOR >= (minor)))
+#else
+#define CHECK_DAOS_API_VERSION(major, minor) 0
+#endif
 
 /*
  * Private definitions.
+ * TODO - Need to reorganize some functions in this file.
  */
 
-static bool daos_uuid_valid(const uuid_t uuid)
+void mfu_daos_stats_init(mfu_daos_stats_t* stats)
+{
+    stats->total_oids = 0;
+    stats->total_dkeys = 0;
+    stats->total_dkeys = 0;
+    stats->total_akeys = 0;
+    stats->bytes_read = 0;
+    stats->bytes_written = 0;
+    stats->wtime_started = 0;
+    stats->wtime_ended = 0;
+}
+
+void mfu_daos_stats_start(mfu_daos_stats_t* stats)
+{
+    time(&stats->time_started);
+    stats->wtime_started = MPI_Wtime();
+}
+
+void mfu_daos_stats_end(mfu_daos_stats_t* stats)
+{
+    time(&stats->time_ended);
+    stats->wtime_ended = MPI_Wtime();
+}
+
+void mfu_daos_stats_sum(mfu_daos_stats_t* stats, mfu_daos_stats_t* stats_sum)
+{
+    int num_values = 5;
+    /* put local values into buffer */
+    uint64_t values[num_values];
+    values[0] = stats->total_oids;
+    values[1] = stats->total_dkeys;
+    values[2] = stats->total_akeys;
+    values[3] = stats->bytes_read;
+    values[4] = stats->bytes_written;
+
+    /* sum the values */
+    MPI_Allreduce(MPI_IN_PLACE, values, num_values, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);
+
+    /* store summed values */
+    stats_sum->total_oids = values[0];
+    stats_sum->total_dkeys = values[1];
+    stats_sum->total_akeys = values[2];
+    stats_sum->bytes_read = values[3];
+    stats_sum->bytes_written = values[4];
+
+    /* copy times */
+    stats_sum->time_started = stats->time_started;
+    stats_sum->time_ended = stats->time_ended;
+    stats_sum->wtime_started = stats->wtime_started;
+    stats_sum->wtime_ended = stats->wtime_ended;
+}
+
+void mfu_daos_stats_print(
+    mfu_daos_stats_t* stats,
+    bool print_read,
+    bool print_write,
+    bool print_read_rate,
+    bool print_write_rate)
+{
+    /* format start time */
+    char starttime_str[256];
+    struct tm* localstart = localtime(&stats->time_started);
+    strftime(starttime_str, 256, "%b-%d-%Y,%H:%M:%S", localstart);
+
+    /* format end time */
+    char endtime_str[256];
+    struct tm* localend = localtime(&stats->time_ended);
+    strftime(endtime_str, 256, "%b-%d-%Y,%H:%M:%S", localend);
+
+    /* compute relative time elapsed */
+    double rel_time = stats->wtime_ended - stats->wtime_started;
+
+    /* convert read size to units */
+    double read_size_tmp;
+    const char* read_size_units;
+    mfu_format_bytes(stats->bytes_read, &read_size_tmp, &read_size_units);
+
+    /* convert write size to units */
+    double write_size_tmp;
+    const char* write_size_units;
+    mfu_format_bytes(stats->bytes_written, &write_size_tmp, &write_size_units);
+
+    MFU_LOG(MFU_LOG_INFO, "Started       : %s", starttime_str);
+    MFU_LOG(MFU_LOG_INFO, "Completed     : %s", endtime_str);
+    MFU_LOG(MFU_LOG_INFO, "Seconds       : %.3lf", rel_time);
+    MFU_LOG(MFU_LOG_INFO, "Objects       : %" PRId64, stats->total_oids);
+    MFU_LOG(MFU_LOG_INFO, "  D-Keys      : %" PRId64, stats->total_dkeys);
+    MFU_LOG(MFU_LOG_INFO, "  A-Keys      : %" PRId64, stats->total_akeys);
+
+    if (print_read) {
+        MFU_LOG(MFU_LOG_INFO, "Bytes read    : %.3lf %s (%" PRId64 " bytes)",
+                read_size_tmp, read_size_units, stats->bytes_read);
+    }
+    if (print_write) {
+        MFU_LOG(MFU_LOG_INFO, "Bytes written : %.3lf %s (%" PRId64 " bytes)",
+                write_size_tmp, write_size_units, stats->bytes_written);
+    }
+
+    if (print_read_rate) {
+        /* Compute read rate */
+        double read_rate = 0;
+        if (rel_time > 0) {
+            read_rate = (double) stats->bytes_read / rel_time;
+        }
+
+        /* Convert read rate to units */
+        double read_rate_tmp;
+        const char* read_rate_units;
+        mfu_format_bw(read_rate, &read_rate_tmp, &read_rate_units);
+
+        MFU_LOG(MFU_LOG_INFO, "Read rate     : %.3lf %s",
+                read_rate_tmp, read_rate_units);
+    }
+
+    if (print_write_rate) {
+        /* Compute write rate */
+        double write_rate = 0;
+        if (rel_time > 0) {
+            write_rate = (double) stats->bytes_written / rel_time;
+        }
+
+        /* Convert write rate to units */
+        double write_rate_tmp;
+        const char* write_rate_units;
+        mfu_format_bw(write_rate, &write_rate_tmp, &write_rate_units);
+
+        MFU_LOG(MFU_LOG_INFO, "Write rate    : %.3lf %s",
+                write_rate_tmp, write_rate_units);
+    }
+}
+
+void mfu_daos_stats_print_sum(
+    int rank,
+    mfu_daos_stats_t* stats,
+    bool print_read,
+    bool print_write,
+    bool print_read_rate,
+    bool print_write_rate)
+{
+    mfu_daos_stats_t stats_sum;
+    mfu_daos_stats_sum(stats, &stats_sum);
+    if (rank == 0) {
+        mfu_daos_stats_print(&stats_sum, print_read, print_write, print_read_rate, print_write_rate);
+    }
+}
+
+bool daos_uuid_valid(const uuid_t uuid)
 {
     return uuid && !uuid_is_null(uuid);
 }
@@ -27,19 +191,26 @@ static bool daos_uuid_valid(const uuid_t uuid)
 static int daos_check_args(
     int rank,
     char** argpaths,
+    int numpaths,
     daos_args_t* da,
     int* flag_daos_args)
 {
-    char* src_path = argpaths[0];
-    char* dst_path = argpaths[1];
+    char* src_path = NULL;
+    char* dst_path = NULL;
+    if (numpaths > 0) {
+        src_path = argpaths[0];
+    }
+    if (numpaths > 1) {
+        dst_path = argpaths[1];
+    }
 
-    bool have_src_path  = src_path != NULL;
-    bool have_dst_path  = dst_path != NULL;
-    bool have_src_pool  = daos_uuid_valid(da->src_pool_uuid);
-    bool have_src_cont  = daos_uuid_valid(da->src_cont_uuid);
-    bool have_dst_pool  = daos_uuid_valid(da->dst_pool_uuid);
-    bool have_dst_cont  = daos_uuid_valid(da->dst_cont_uuid);
-    bool have_prefix    = da->dfs_prefix != NULL;
+    bool have_src_path  = (src_path != NULL);
+    bool have_dst_path  = (dst_path != NULL);
+    bool have_src_pool  = strlen(da->src_pool) ? true : false;
+    bool have_src_cont  = strlen(da->src_cont) ? true : false;
+    bool have_dst_pool  = strlen(da->dst_pool) ? true : false;
+    bool have_dst_cont  = strlen(da->dst_cont) ? true : false;
+    bool have_prefix    = (da->dfs_prefix != NULL);
 
     /* Determine whether any DAOS arguments are supplied. 
      * If not, then there is nothing to check. */
@@ -56,15 +227,31 @@ static int daos_check_args(
         return 0;
     }
     
+    /* if passed in values for src/dst are both uuids ignore the case when comparing */
+
     /* Determine whether the source and destination
      * use the same pool and container */
+    int rc1, rc2;
     bool same_pool = false;
+    uuid_t src_pool;
+    uuid_t dst_pool;
+    rc1 = uuid_parse(da->src_pool, src_pool); 
+    rc2 = uuid_parse(da->dst_pool, dst_pool); 
+    if (rc1 == 0 && rc2 == 0) {
+        same_pool = (strcasecmp(da->src_pool, da->dst_pool) == 0);
+    } else {
+        same_pool = (strcmp(da->src_pool, da->dst_pool) == 0);
+    }
+
     bool same_cont = false;
-    if (uuid_compare(da->src_pool_uuid, da->dst_pool_uuid) == 0) {
-        same_pool = true;
-        if (uuid_compare(da->src_cont_uuid, da->dst_cont_uuid) == 0) {
-            same_cont = true;
-        }
+    uuid_t src_cont;
+    uuid_t dst_cont;
+    rc1 = uuid_parse(da->src_cont, src_cont); 
+    rc2 = uuid_parse(da->dst_cont, dst_cont); 
+    if (rc1 == 0 && rc2 == 0) {
+        same_cont = same_pool && (strcasecmp(da->src_cont, da->dst_cont) == 0);
+    } else {
+        same_cont = same_pool && (strcmp(da->src_cont, da->dst_cont) == 0);
     }
 
     /* Determine whether the source and destination paths are the same.
@@ -129,36 +316,91 @@ static bool daos_check_prefix(
  * Returns 1 if a daos path was not parsed.
  * Returns -1 for actual errors.
  */
-static int daos_parse_path(
+int daos_parse_path(
     char* path,
     size_t path_len,
-    uuid_t* p_uuid,
-    uuid_t* c_uuid)
+    char (*pool_str)[],
+    char (*cont_str)[])
 {
     struct duns_attr_t  dattr = {0};
     int                 rc;
+    char*               tmp_path1 = NULL;
+    char*               path_dirname = NULL;
+    char*               tmp_path2 = NULL;
+    char*               path_basename = NULL;
+    char*               tmp = NULL;
 
+    /* check first if duns_resolve_path succeeds on regular path */
     rc = duns_resolve_path(path, &dattr);
     if (rc == 0) {
         /* daos:// or UNS path */
-        uuid_copy(*p_uuid, dattr.da_puuid);
-        uuid_copy(*c_uuid, dattr.da_cuuid);
+        snprintf(*pool_str, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", dattr.da_pool);
+        snprintf(*cont_str, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", dattr.da_cont);
         if (dattr.da_rel_path == NULL) {
             strncpy(path, "/", path_len);
         } else {
             strncpy(path, dattr.da_rel_path, path_len);
         }
-    } else if (strncmp(path, "daos:", 5) == 0) {
-        /* Actual error, since we expect a daos path */
-        rc = -1;
     } else {
-        /* We didn't parse a daos path,
-         * but we weren't necessarily looking for one */
-        rc = 1;
-    }
+        /* If basename does not exist yet then duns_resolve_path will fail even
+        * if dirname is a UNS path */
 
-    mfu_free(&dattr.da_rel_path);
+        /* get dirname */
+        tmp_path1 = strdup(path);
+        if (tmp_path1 == NULL) {
+            rc = -ENOMEM;
+            goto out;
+        }
+        path_dirname = dirname(tmp_path1);
+
+        /* reset before calling duns_resolve_path with new string */
+        memset(&dattr, 0, sizeof(struct duns_attr_t));
+
+        /* Check if this path represents a daos pool and/or container. */
+        rc = duns_resolve_path(path_dirname, &dattr);
+        /* if it succeeds get the basename and append it to the rel_path */
+        if (rc == 0) {
+            /* if duns_resolve_path succeeds then concat basename to 
+            * da_rel_path */
+            tmp_path2 = strdup(path);
+            if (tmp_path2 == NULL) {
+                rc = -ENOMEM;
+                goto out;
+            }
+            path_basename = basename(tmp_path2);
+   
+            /* dirname might be root uns path, if that is the case,
+             * then da_rel_path might be NULL */
+            if (dattr.da_rel_path == NULL) {
+                tmp = MFU_CALLOC(path_len, sizeof(char));
+            } else {
+                tmp = realloc(dattr.da_rel_path, path_len);
+            }
+            if (tmp == NULL) {
+                rc = -ENOMEM;
+                goto out;
+            }
+            dattr.da_rel_path = tmp;
+            strcat(dattr.da_rel_path, "/");
+            strcat(dattr.da_rel_path, path_basename);
 
+            snprintf(*pool_str, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", dattr.da_pool);
+            snprintf(*cont_str, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", dattr.da_cont);
+            strncpy(path, dattr.da_rel_path, path_len);
+        } else if (strncmp(path, "daos:", 5) == 0) {
+            /* Actual error, since we expect a daos path */
+            rc = -1;
+        } else {
+            /* We didn't parse a daos path,
+            * but we weren't necessarily looking for one */
+            rc = 1;
+        }
+    }
+out:
+    mfu_free(&tmp_path1);
+    mfu_free(&tmp_path2);
+    mfu_free(&dattr.da_rel_path);
+    duns_destroy_attr(&dattr);
     return rc;
 }
 
@@ -167,37 +409,43 @@ static int daos_parse_path(
 static int daos_set_paths(
   int rank,
   char** argpaths,
-  daos_args_t* da)
+  int numpaths,
+  daos_args_t* da,
+  bool *dst_cont_passed)
 {
     int     rc = 0;
+    bool    have_dst = (numpaths > 1);
     bool    prefix_on_src = false;
     bool    prefix_on_dst = false;
+    char*   prefix_path = NULL;
+    char*   src_path = NULL;
+    char*   dst_path = NULL;
 
     /* find out if a dfs_prefix is being used,
      * if so, then that means that the container
      * is not being copied from the root of the
      * UNS path  */
     if (da->dfs_prefix != NULL) {
-        uuid_t  prefix_p_uuid;
-        uuid_t  prefix_c_uuid;
-        int     prefix_rc;
+        char prefix_pool[DAOS_PROP_LABEL_MAX_LEN + 1];
+        char prefix_cont[DAOS_PROP_LABEL_MAX_LEN + 1];
+        int prefix_rc;
 
         size_t prefix_len = strlen(da->dfs_prefix);
-        char* prefix_path = strndup(da->dfs_prefix, prefix_len);
+        prefix_path = strndup(da->dfs_prefix, prefix_len);
         if (prefix_path == NULL) {
             MFU_LOG(MFU_LOG_ERR, "Unable to allocate space for DAOS prefix.");
             rc = 1;
             goto out;
         }
 
-        uuid_clear(prefix_p_uuid);
-        uuid_clear(prefix_c_uuid);
+        memset(prefix_pool, '\0', DAOS_PROP_LABEL_MAX_LEN + 1);
+        memset(prefix_cont, '\0', DAOS_PROP_LABEL_MAX_LEN + 1);
 
         /* Get the pool/container uuids from the prefix */
-        prefix_rc = daos_parse_path(prefix_path, prefix_len, &prefix_p_uuid, &prefix_c_uuid);
-        if (prefix_rc != 0 || prefix_p_uuid == NULL || prefix_c_uuid == NULL) {
+        prefix_rc = daos_parse_path(prefix_path, prefix_len,
+                                    &prefix_pool, &prefix_cont);
+        if (prefix_rc != 0) {
             MFU_LOG(MFU_LOG_ERR, "Failed to resolve DAOS Prefix UNS path");
-            mfu_free(&prefix_path);
             rc = 1;
             goto out;
         }
@@ -205,7 +453,6 @@ static int daos_set_paths(
         /* In case the user tries to give a sub path in the UNS path */
         if (strcmp(prefix_path, "/") != 0) {
             MFU_LOG(MFU_LOG_ERR, "DAOS prefix must be a UNS path");
-            mfu_free(&prefix_path);
             rc = 1;
             goto out;
         }
@@ -213,22 +460,23 @@ static int daos_set_paths(
         /* Check if the prefix matches the source */
         prefix_on_src = daos_check_prefix(argpaths[0], da->dfs_prefix, &da->src_path);
         if (prefix_on_src) {
-            uuid_copy(da->src_pool_uuid, prefix_p_uuid);
-            uuid_copy(da->src_cont_uuid, prefix_c_uuid);
+            snprintf(da->src_pool, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", prefix_pool);
+            snprintf(da->src_cont, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", prefix_cont);
             argpaths[0] = da->src_path;
         }
 
-        /* Check if the prefix matches the destination */
-        prefix_on_dst = daos_check_prefix(argpaths[1], da->dfs_prefix, &da->dst_path);
-        if (prefix_on_dst) {
-            uuid_copy(da->dst_pool_uuid, prefix_p_uuid);
-            uuid_copy(da->dst_cont_uuid, prefix_c_uuid);
-            argpaths[1] = da->dst_path;
+        if (have_dst) {
+            /* Check if the prefix matches the destination */
+            prefix_on_dst = daos_check_prefix(argpaths[1], da->dfs_prefix, &da->dst_path);
+            if (prefix_on_dst) {
+                snprintf(da->dst_pool, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", prefix_pool);
+                snprintf(da->dst_cont, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", prefix_cont);
+                argpaths[1] = da->dst_path;
+            }
         }
 
         if (!prefix_on_src && !prefix_on_dst) {
             MFU_LOG(MFU_LOG_ERR, "DAOS prefix does not match source or destination");
-            mfu_free(&prefix_path);
             rc = 1;
             goto out;
         }
@@ -241,40 +489,67 @@ static int daos_set_paths(
      */
     if (!prefix_on_src) {
         size_t src_len = strlen(argpaths[0]);
-        char* src_path = strndup(argpaths[0], src_len);
+        src_path = strndup(argpaths[0], src_len);
         if (src_path == NULL) {
             MFU_LOG(MFU_LOG_ERR, "Unable to allocate space for source path.");
             rc = 1;
             goto out;
         }
-        int src_rc = daos_parse_path(src_path, src_len, &da->src_pool_uuid, &da->src_cont_uuid);
+        int src_rc = daos_parse_path(src_path, src_len, &da->src_pool, &da->src_cont);
         if (src_rc == 0) {
+            if (strlen(da->src_cont) == 0) {
+                MFU_LOG(MFU_LOG_ERR, "Source pool requires a source container.");
+                rc = 1;
+                goto out;
+            }
             argpaths[0] = da->src_path = strdup(src_path);
-            mfu_free(&src_path);
+            if (argpaths[0] == NULL) {
+                MFU_LOG(MFU_LOG_ERR, "Unable to allocate space for source path.");
+                rc = 1;
+                goto out;
+            }
         } else if (src_rc == -1) {
             MFU_LOG(MFU_LOG_ERR, "Failed to parse DAOS source path: daos://<pool>/<cont>[/<path>]");
-            mfu_free(&src_path);
             rc = 1;
             goto out;
         }
     }
-
-    if (!prefix_on_dst) {
+    if (have_dst && !prefix_on_dst) {
         size_t dst_len = strlen(argpaths[1]);
-        char* dst_path = strndup(argpaths[1], dst_len);
-        int dst_rc = daos_parse_path(dst_path, dst_len, &da->dst_pool_uuid, &da->dst_cont_uuid);
+        dst_path = strndup(argpaths[1], dst_len);
+        if (dst_path == NULL) {
+            MFU_LOG(MFU_LOG_ERR, "Unable to allocate space for destination path.");
+            rc = 1;
+            goto out;
+        }
+        int dst_rc = daos_parse_path(dst_path, dst_len, &da->dst_pool, &da->dst_cont);
         if (dst_rc == 0) {
             argpaths[1] = da->dst_path = strdup(dst_path);
-            mfu_free(&dst_path);
+            if (argpaths[1] == NULL) {
+                MFU_LOG(MFU_LOG_ERR, "Unable to allocate space for destination path.");
+                rc = 1;
+                goto out;
+            }
         } else if (dst_rc == -1) {
             MFU_LOG(MFU_LOG_ERR, "Failed to parse DAOS destination path: daos://<pool>/<cont>[/<path>]");
-            mfu_free(&dst_path);
             rc = 1;
             goto out;
         }
     }
 
+    if (have_dst) {
+        int dst_cont_len = strlen(da->dst_cont);
+        *dst_cont_passed = dst_cont_len > 0 ? true : false;
+        /* Generate a new container uuid if only a pool was given. */
+        if (!*dst_cont_passed) {
+            uuid_generate(da->dst_cont_uuid);
+        }
+    }
+
 out:
+    mfu_free(&prefix_path);
+    mfu_free(&src_path);
+    mfu_free(&dst_path);
     return rc;
 }
 
@@ -282,9 +557,8 @@ static int daos_get_cont_type(
     daos_handle_t coh,
     enum daos_cont_props* type)
 {
-    daos_prop_t*            prop = daos_prop_alloc(1);
-    struct daos_prop_entry  entry;
-    int                     rc;
+    daos_prop_t*    prop = daos_prop_alloc(1);
+    int             rc;
 
     if (prop == NULL) {
         MFU_LOG(MFU_LOG_ERR, "Failed to allocate prop (%d)", rc);
@@ -305,13 +579,49 @@ static int daos_get_cont_type(
     return 0;
 }
 
+/* check if cont status is unhealthy */
+static int daos_check_cont_status(daos_handle_t coh, bool *status_healthy)
+{
+    daos_prop_t*            prop = daos_prop_alloc(1);
+    struct daos_prop_entry  *entry;
+    struct daos_co_status   stat = {0};
+    int                     rc = 0;
+    
+    if (prop == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to allocate prop: "DF_RC, DP_RC(rc));
+        rc = ENOMEM;
+        goto out;
+    }
+
+    prop->dpp_entries[0].dpe_type = DAOS_PROP_CO_STATUS;
+
+    rc = daos_cont_query(coh, NULL, prop, NULL);
+    if (rc) {
+        MFU_LOG(MFU_LOG_ERR, "daos_cont_query() failed "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[0];
+    daos_prop_val_2_co_status(entry->dpe_val, &stat);                            
+    if (stat.dcs_status == DAOS_PROP_CO_HEALTHY) {
+        *status_healthy = true;
+    } else {
+        *status_healthy = false;
+    }
+
+out:
+    daos_prop_free(prop);
+    return rc;
+}
+
 /*
  * Try to set the file type based on the container type,
  * using api as a guide.
  */
 static int daos_set_api_cont_type(
     mfu_file_t* mfu_file,
-    daos_handle_t coh,
+    enum daos_cont_props cont_type,
     daos_api_t api)
 {
     /* If explicitly using DAOS, just set the type to DAOS */
@@ -320,15 +630,6 @@ static int daos_set_api_cont_type(
         return 0;
     }
 
-    /* Otherwise, query the container type, and use DFS for POSIX containers. */
-    enum daos_cont_props cont_type;
-
-    int rc = daos_get_cont_type(coh, &cont_type);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "Failed to get DAOS container type.");
-        return rc;
-    }
-
     if (cont_type == DAOS_PROP_CO_LAYOUT_POSIX) {
         mfu_file->type = DFS;
     } else {
@@ -348,44 +649,33 @@ static int daos_set_api_cont_type(
  * Set the mfu_file types to either DAOS or DFS,
  * and make sure they are compatible.
  */
-static int daos_set_api(
+static int daos_set_api_compat(
     mfu_file_t* mfu_src_file,
     mfu_file_t* mfu_dst_file,
     daos_args_t* da,
     char** argpaths)
 {
-    /* Check whether we have pool/cont uuids */
-    bool have_src_pool  = daos_uuid_valid(da->src_pool_uuid);
-    bool have_src_cont  = daos_uuid_valid(da->src_cont_uuid);
-    bool have_dst_pool  = daos_uuid_valid(da->dst_pool_uuid);
-    bool have_dst_cont  = daos_uuid_valid(da->dst_cont_uuid);
+    bool have_dst = (mfu_dst_file != NULL);
 
-    int rc;
+    /* Check whether we have pool/cont uuids */
+    bool have_src_cont  = strlen(da->src_cont) ? true : false;
+    bool have_dst_cont  = strlen(da->dst_cont) ? true : false;
 
-    /* If the user explicitly wants to use the DAOS API,
-     * then set both types to DAOS.
-     * Otherwise, query the container type and set to DFS
-     * for POSIX containers. */
-    if (have_src_pool && have_src_cont) {
-        rc = daos_set_api_cont_type(mfu_src_file, da->src_coh, da->api);
-        if (rc) {
-            return rc;
-        }
-    }
-    if (have_dst_pool && have_dst_cont) {
-        rc = daos_set_api_cont_type(mfu_dst_file, da->dst_coh, da->api);
-        if (rc) {
-            return rc;
+    /* Containers must be the same type */
+    if (have_src_cont && have_dst_cont) {
+        if (da->src_cont_type != da->dst_cont_type) {
+            MFU_LOG(MFU_LOG_ERR, "Containers must be the same type.");
+            return 1;
         }
     }
 
     /* Check whether we have source and destination paths */
     char* src_path = argpaths[0];
     char* dst_path = argpaths[1];
-    bool have_src_path = src_path != NULL;
-    bool have_dst_path = dst_path != NULL;
-    bool src_path_is_root = have_src_path && (strcmp(src_path, "/") == 0);
-    bool dst_path_is_root = have_dst_path && (strcmp(dst_path, "/") == 0);
+    bool have_src_path = (src_path != NULL);
+    bool have_dst_path = (dst_path != NULL);
+    bool src_path_is_root = (have_src_path && (strcmp(src_path, "/") == 0));
+    bool dst_path_is_root = (have_dst_path && (strcmp(dst_path, "/") == 0));
 
     /* If either type is DAOS:
      * Both paths must be root.
@@ -396,13 +686,15 @@ static int daos_set_api(
             MFU_LOG(MFU_LOG_ERR, "Cannot use path with non-POSIX container.");
             return 1;
         }
-        if (mfu_dst_file->type != DAOS && mfu_dst_file->type != DFS) {
-            MFU_LOG(MFU_LOG_ERR, "Cannot copy non-POSIX container outside DAOS.");
-            return 1;
+        if (have_dst) {
+            if (mfu_dst_file->type != DAOS && mfu_dst_file->type != DFS) {
+                MFU_LOG(MFU_LOG_ERR, "Cannot copy non-POSIX container outside DAOS.");
+                return 1;
+            }
+            mfu_dst_file->type = DAOS;
         }
-        mfu_dst_file->type = DAOS;
     }
-    if (mfu_dst_file->type == DAOS) {
+    if (have_dst && mfu_dst_file->type == DAOS) {
         if (!dst_path_is_root || !src_path_is_root) {
             MFU_LOG(MFU_LOG_ERR, "Cannot use path with non-POSIX container.");
             return 1;
@@ -413,7 +705,6 @@ static int daos_set_api(
         }
         mfu_src_file->type = DAOS;
     }
-
     return 0;
 }
 
@@ -438,8 +729,329 @@ static int daos_any_error(int rank, bool local_daos_error, int flag_daos_args)
     return 0;
 }
 
+/*
+ * Get the user attributes for a container in a format similar
+ * to what daos_cont_set_prop expects.
+ * The last entry is the ACL and is conditionally set only if
+ * the user has permissions.
+ * The properties should remain in the order expected by serialization.
+ * Returns 1 on error, 0 on success.
+ */
+static int cont_get_props(daos_handle_t coh, daos_prop_t** _props,
+                          bool get_oid, bool get_label, bool get_roots)
+{
+    int             rc;
+    daos_prop_t*    props = NULL;
+    daos_prop_t*    prop_acl = NULL;
+    daos_prop_t*    props_merged = NULL;
+    /* total amount of properties to allocate */
+    uint32_t        total_props = 15;
+    /* minimum number of properties that are always allocated/used to start
+     * count */
+    int             prop_index = 15;
+
+    if (get_oid) {
+        total_props++;
+    }
+
+    /* container label is required to be unique, so do not
+     * retrieve it for copies. The label is retrieved for
+     * serialization, but only deserialized if the label
+     * no longer exists in the pool */
+    if (get_label) {
+        total_props++;
+    }
+
+    if (get_roots) {
+        total_props++;
+    }
+
+    /* Allocate space for all props except ACL. */
+    props = daos_prop_alloc(total_props);
+    if (props == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to allocate container properties.");
+        rc = 1;
+        goto out;
+    }
+
+    /* The order of properties MUST match the order expected by serialization. */
+    props->dpp_entries[0].dpe_type = DAOS_PROP_CO_LAYOUT_TYPE;
+    props->dpp_entries[1].dpe_type = DAOS_PROP_CO_LAYOUT_VER;
+    props->dpp_entries[2].dpe_type = DAOS_PROP_CO_CSUM;
+    props->dpp_entries[3].dpe_type = DAOS_PROP_CO_CSUM_CHUNK_SIZE;
+    props->dpp_entries[4].dpe_type = DAOS_PROP_CO_CSUM_SERVER_VERIFY;
+    props->dpp_entries[5].dpe_type = DAOS_PROP_CO_REDUN_FAC;
+    props->dpp_entries[6].dpe_type = DAOS_PROP_CO_REDUN_LVL;
+    props->dpp_entries[7].dpe_type = DAOS_PROP_CO_SNAPSHOT_MAX;
+    props->dpp_entries[8].dpe_type = DAOS_PROP_CO_COMPRESS;
+    props->dpp_entries[9].dpe_type = DAOS_PROP_CO_ENCRYPT;
+    props->dpp_entries[10].dpe_type = DAOS_PROP_CO_OWNER;
+    props->dpp_entries[11].dpe_type = DAOS_PROP_CO_OWNER_GROUP;
+    props->dpp_entries[12].dpe_type = DAOS_PROP_CO_DEDUP;
+    props->dpp_entries[13].dpe_type = DAOS_PROP_CO_DEDUP_THRESHOLD;
+    props->dpp_entries[14].dpe_type = DAOS_PROP_CO_EC_CELL_SZ;
+
+    /* Conditionally get the OID. Should always be true for serialization. */
+    if (get_oid) {
+        props->dpp_entries[prop_index].dpe_type = DAOS_PROP_CO_ALLOCED_OID;
+        prop_index++;
+    }
+
+    if (get_label) {
+        props->dpp_entries[prop_index].dpe_type = DAOS_PROP_CO_LABEL;
+        prop_index++;
+    }
+
+    if (get_roots) {
+        props->dpp_entries[prop_index].dpe_type = DAOS_PROP_CO_ROOTS;
+    }
+
+    /* Get all props except ACL first. */
+    rc = daos_cont_query(coh, NULL, props, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to query container: "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    /* Fetch the ACL separately in case user doesn't have access */
+    rc = daos_cont_get_acl(coh, &prop_acl, NULL);
+    if (rc == 0) {
+        /* ACL will be appended to the end */
+        props_merged = daos_prop_merge(props, prop_acl);
+        if (props_merged == NULL) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to set container ACL: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto out;
+        }
+        daos_prop_free(props);
+        props = props_merged;
+    } else if (rc && rc != -DER_NO_PERM) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to query container ACL: "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    rc = 0;
+    *_props = props;
+
+out:
+    daos_prop_free(prop_acl);
+    if (rc != 0) {
+        daos_prop_free(props);
+    }
+
+    return rc;
+}
+
+/*
+ * Free the user attribute buffers created by cont_get_usr_attrs.
+ */
+static void cont_free_usr_attrs(int n, char*** _names, void*** _buffers,
+                               size_t** _sizes)
+{
+    char**  names = *_names;
+    void**  buffers = *_buffers;
+
+    if (names != NULL) {
+        for (size_t i = 0; i < n; i++) {
+            mfu_free(&names[i]);
+        }
+        mfu_free(_names);
+    }
+    if (buffers != NULL) {
+        for (size_t i = 0; i < n; i++) {
+            mfu_free(&buffers[i]);
+        }
+        mfu_free(_buffers);
+    }
+    mfu_free(_sizes);
+}
+
+/*
+ * Get the user attributes for a container in a format similar
+ * to what daos_cont_set_attr expects.
+ * cont_free_usr_attrs should be called to free the allocations.
+ * Returns 1 on error, 0 on success.
+ */
+static int cont_get_usr_attrs(daos_handle_t coh,
+                              int* _n, char*** _names, void*** _buffers,
+                              size_t** _sizes)
+{
+    int         rc = 0;
+    uint64_t    total_size = 0;
+    uint64_t    cur_size = 0;
+    uint64_t    num_attrs = 0;
+    uint64_t    name_len = 0;
+    char*       name_buf = NULL;
+    char**      names = NULL;
+    void**      buffers = NULL;
+    size_t*     sizes = NULL;
+
+    /* Get the total size needed to store all names */
+    rc = daos_cont_list_attr(coh, NULL, &total_size, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to list user attributes "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    /* no attributes found */
+    if (total_size == 0) {
+        *_n = 0;
+        goto out;
+    }
+
+    /* Allocate a buffer to hold all attribute names */
+    name_buf = MFU_CALLOC(total_size, sizeof(char));
+    if (name_buf == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attribute buffer");
+        rc = 1;
+        goto out;
+    }
+
+    /* Get the attribute names */
+    rc = daos_cont_list_attr(coh, name_buf, &total_size, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to list user attributes "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    /* Figure out the number of attributes */
+    while (cur_size < total_size) {
+        name_len = strnlen(name_buf + cur_size, total_size - cur_size);
+        if (name_len == total_size - cur_size) {
+            /* end of buf reached but no end of string, ignoring */
+            break;
+        }
+        num_attrs++;
+        cur_size += name_len + 1;
+    }
+
+    /* Sanity check */
+    if (num_attrs == 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to parse user attributes");
+        rc = 1;
+        goto out;
+    }
+
+    /* Allocate arrays for attribute names, buffers, and sizes */
+    names = MFU_CALLOC(num_attrs, sizeof(char*));
+    if (names == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attribute buffer");
+        rc = 1;
+        goto out;
+    }
+    sizes = MFU_CALLOC(num_attrs, sizeof(size_t));
+    if (sizes == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attribute buffer");
+        rc = 1;
+        goto out;
+    }
+    buffers = MFU_CALLOC(num_attrs, sizeof(void*));
+    if (buffers == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attribute buffer");
+        rc = 1;
+        goto out;
+    }
+
+    /* Create the array of names */
+    cur_size = 0;
+    for (uint64_t i = 0; i < num_attrs; i++) {
+        name_len = strnlen(name_buf + cur_size, total_size - cur_size);
+        if (name_len == total_size - cur_size) {
+            /* end of buf reached but no end of string, ignoring */
+            break;
+        }
+        names[i] = strndup(name_buf + cur_size, name_len + 1);
+        cur_size += name_len + 1;
+    }
+
+    /* Get the buffer sizes */
+    rc = daos_cont_get_attr(coh, num_attrs,
+                            (const char* const*)names,
+                            NULL, sizes, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get user attribute sizes "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    /* Allocate space for each value */
+    for (uint64_t i = 0; i < num_attrs; i++) {
+        buffers[i] = MFU_CALLOC(sizes[i], sizeof(size_t));
+        if (buffers[i] == NULL) {
+            MFU_LOG(MFU_LOG_ERR, "failed to allocate user attribute buffer");
+            rc = 1;
+            goto out;
+        }
+    }
+
+    /* Get the attribute values */
+    rc = daos_cont_get_attr(coh, num_attrs,
+                            (const char* const*)names,
+                            (void * const*)buffers, sizes,
+                            NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get user attribute values "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+    /* Return values to the caller */
+    *_n = num_attrs;
+    *_names = names;
+    *_buffers = buffers;
+    *_sizes = sizes;
+out:
+    if (rc != 0) {
+        cont_free_usr_attrs(num_attrs, &names, &buffers, &sizes);
+    }
+
+    mfu_free(&name_buf);
+    return rc;
+}
+
+/* Copy all user attributes from one container to another.
+ * Returns 1 on error, 0 on success. */
+static int copy_usr_attrs(daos_handle_t src_coh, daos_handle_t dst_coh)
+{
+    int         num_attrs = 0;
+    char**      names = NULL;
+    void**      buffers = NULL;
+    size_t*     sizes = NULL;
+    int         rc;
+
+    /* Get all user attributes */
+    rc = cont_get_usr_attrs(src_coh, &num_attrs, &names, &buffers, &sizes);
+    if (rc != 0) {
+        rc = 1;
+        goto out;
+    }
+
+    if (num_attrs == 0) {
+        rc = 0;
+        goto out;
+    }
+
+    rc = daos_cont_set_attr(dst_coh, num_attrs,
+                            (char const* const*) names,
+                            (void const* const*) buffers,
+                            sizes, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to set user attrs: "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+out:
+    cont_free_usr_attrs(num_attrs, &names, &buffers, &sizes);
+    return rc;
+}
+
 /* Distribute process 0's pool or container handle to others. */
-static void daos_bcast_handle(
+void daos_bcast_handle(
   int rank,              /* root rank for broadcast */
   daos_handle_t* handle, /* handle value to be broadcasted */
   daos_handle_t* poh,    /* daos pool for global2local conversion of container handle */
@@ -503,34 +1115,61 @@ static void daos_bcast_handle(
 }
 
 /* connect to DAOS pool, and then open container */
-static int daos_connect(
+int daos_connect(
   int rank,
-  uuid_t pool_uuid,
-  uuid_t cont_uuid,
+  daos_args_t* da,
+  char (*pool)[],
+  char (*cont)[],
   daos_handle_t* poh,
   daos_handle_t* coh,
+  bool force_serialize,
   bool connect_pool,
-  bool create_cont)
+  bool create_cont,
+  bool require_new_cont,
+  bool preserve,
+  mfu_file_t* mfu_src_file,
+  bool dst_cont_passed)
 {
-    /* assume failure until otherwise */
-    int valid = 0;
-    int rc;
+    /* sanity check */
+    if (require_new_cont && !create_cont) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "create_cont must be true when require_new_cont is true");
+        }
+        return -1;
+    }
+
+    int                         valid = 0; /* assume failure until otherwise */
+    int                         rc;
+    daos_prop_t                 *props = NULL;
+    struct daos_prop_co_roots   roots = {0};
+#ifdef HDF5_SUPPORT
+    struct hdf5_args hdf5;
+#endif
 
     /* have rank 0 connect to the pool and container,
      * we'll then broadcast the handle ids from rank 0 to everyone else */
     if (rank == 0) {
+#ifdef HDF5_SUPPORT
+        /* initialization for hdf5 file for preserve option */
+        if (preserve) {
+            hdf5.file = H5Fopen(da->daos_preserve_path, H5F_ACC_RDONLY, H5P_DEFAULT);
+            if (hdf5.file < 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to open hdf5 file");
+                rc = 1;
+                goto bcast;
+            }
+        }
+#endif
         /* Connect to DAOS pool */
         if (connect_pool) {
             daos_pool_info_t pool_info = {0};
 #if DAOS_API_VERSION_MAJOR < 1
-            rc = daos_pool_connect(pool_uuid, NULL, NULL, DAOS_PC_RW,
-                    poh, &pool_info, NULL);
+            rc = daos_pool_connect(*pool, NULL, NULL, DAOS_PC_RW, poh, &pool_info, NULL);
 #else
-            rc = daos_pool_connect(pool_uuid, NULL, DAOS_PC_RW,
-                    poh, &pool_info, NULL);
+            rc = daos_pool_connect(*pool, NULL, DAOS_PC_RW, poh, &pool_info, NULL);
 #endif
             if (rc != 0) {
-                MFU_LOG(MFU_LOG_ERR, "Failed to connect to pool");
+                MFU_LOG(MFU_LOG_ERR, "Failed to connect to pool: "DF_RC, DP_RC(rc));
                 goto bcast;
             }
         }
@@ -538,32 +1177,174 @@ static int daos_connect(
         /* Try to open the container
          * If NOEXIST we create it */
         daos_cont_info_t co_info = {0};
-        rc = daos_cont_open(*poh, cont_uuid, DAOS_COO_RW, coh, &co_info, NULL);
+        if (!dst_cont_passed && create_cont) {
+            /* Use uuid if container was created by mpifileutils.
+             * If nothing is passed in for destination a uuid is always generated
+             * unless user passed one in, because destination container labels are
+             * not generated */
+            char cont_str[130];
+            uuid_unparse(da->dst_cont_uuid, cont_str);
+            rc = daos_cont_open(*poh, cont_str, DAOS_COO_RW, coh, &co_info, NULL);
+        } else {
+            rc = daos_cont_open(*poh, *cont, DAOS_COO_RW, coh, &co_info, NULL);
+        }
         if (rc != 0) {
-            if (!create_cont) {
-                MFU_LOG(MFU_LOG_ERR, "Failed to open DFS container");
+            if (rc != -DER_NONEXIST || !create_cont) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to open container: "DF_RC, DP_RC(rc));
                 goto bcast;
             }
 
-            rc = dfs_cont_create(*poh, cont_uuid, NULL, NULL, NULL);
-            if (rc != 0) {
-                MFU_LOG(MFU_LOG_ERR, "Failed to create DFS container");
-                goto bcast;
+            bool have_src_cont = daos_handle_is_valid(da->src_coh);
+
+            /* Get the src container properties. */
+            if (have_src_cont) {
+                if ((mfu_src_file != NULL) && (mfu_src_file->type == DFS)) {
+                    /* Don't get the allocated OID */
+                    rc = cont_get_props(da->src_coh, &props, false, false, false);
+                } else {
+                    rc = cont_get_props(da->src_coh, &props, true, false, true);
+                }
+                if (rc != 0) {
+                    goto bcast;
+                }
             }
 
-            /* try to open it again */
-            rc = daos_cont_open(*poh, cont_uuid, DAOS_COO_RW, coh, &co_info, NULL);
-            if (rc != 0) {
-                MFU_LOG(MFU_LOG_ERR, "Failed to open DFS container");
-                goto bcast;
+            /* if a destination container string was passed in we need to
+             * check if it is a uuid or cont label string. This is necessary
+             * because a user can generate a uuid then pass it as destination,
+             * which should use uuid to create/open the container */
+            bool is_uuid;
+            if (dst_cont_passed) {
+                rc = uuid_parse(*cont, da->dst_cont_uuid); 
+                if (rc == 0) {
+                    is_uuid = true;
+                } else {
+                    is_uuid = false;
+                }
             }
-        }
 
-        /* everything looks good so far */
-        valid = 1;
-    }
+            /* Create a new container. If we don't have a source container,
+             * create a POSIX container. Otherwise, create a container of
+             * the same type as the source. */
+            if (!have_src_cont || (da->src_cont_type == DAOS_PROP_CO_LAYOUT_POSIX)) {
+#ifdef HDF5_SUPPORT
+                /* read container properties in hdf5 file when moving data
+                 * back to DAOS if the preserve option has been set */
+                if (preserve) {
+                    daos_cont_layout_t ctype = DAOS_PROP_CO_LAYOUT_POSIX;
+                    MFU_LOG(MFU_LOG_INFO, "Reading metadata file: %s", da->daos_preserve_path);
+                    rc = cont_deserialize_all_props(&hdf5, &props, &roots, &ctype, *poh);
+                    if (rc != 0) {
+                        MFU_LOG(MFU_LOG_ERR, "Failed to read cont props: "DF_RC, DP_RC(rc));
+                        goto bcast;
+                    }
+                }
+#endif
+                dfs_attr_t attr = {0};
+                attr.da_props = props;
+                if (dst_cont_passed && !is_uuid) {
+                    rc = dfs_cont_create_with_label(*poh, *cont, &attr, &da->dst_cont_uuid, NULL, NULL);
+                } else {
+                    /* if nothing is passed in for destination a uuid is always
+                     * generated unless user passed one in, destination container
+                     * labels are not generated */
+                    rc = dfs_cont_create(*poh, da->dst_cont_uuid, &attr, NULL, NULL);
+                }
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Failed to create container: (%d %s)", rc, strerror(rc));
+                    goto bcast;
+                }
+            } else {
+                if (dst_cont_passed && !is_uuid) {
+                    rc = daos_cont_create_with_label(*poh, *cont, props, &da->dst_cont_uuid, NULL);
+                } else {
+                    rc = daos_cont_create(*poh, da->dst_cont_uuid, props, NULL);
+                }
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Failed to create container: "DF_RC, DP_RC(rc));
+                    goto bcast;
+                }
+            }
+
+            if (dst_cont_passed && !is_uuid) {
+                MFU_LOG(MFU_LOG_INFO, "Successfully created container %s", *cont);
+            } else {
+                char uuid_str[130];
+                uuid_unparse_lower(da->dst_cont_uuid, uuid_str);
+                MFU_LOG(MFU_LOG_INFO, "Successfully created container %s", uuid_str);
+            }
+
+            /* try to open it again */
+            if (dst_cont_passed && !is_uuid) {
+                rc = daos_cont_open(*poh, *cont, DAOS_COO_RW, coh, &co_info, NULL);
+            } else {
+                char cont_str[130];
+                uuid_unparse(da->dst_cont_uuid, cont_str);
+                rc = daos_cont_open(*poh, cont_str, DAOS_COO_RW, coh, &co_info, NULL);
+            }
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to open container: "DF_RC, DP_RC(rc));
+                goto bcast;
+            }
+
+#ifdef HDF5_SUPPORT
+            /* need to wait until destination container is valid to set user
+             * attributes, preserve is only set true when destination is a DFS
+             * copy */
+            if (preserve) { 
+                /* deserialize and set the user attributes if they exist */
+                htri_t usr_attrs_exist = H5Lexists(hdf5.file, "User Attributes", H5P_DEFAULT);
+                if (usr_attrs_exist > 0) {
+                    rc = cont_deserialize_usr_attrs(&hdf5, *coh);
+                    if (rc != 0) {
+                        goto bcast;
+                    }
+                }
+            }
+#endif
+
+            /* Copy user attributes from source container */
+            if (have_src_cont) {
+                rc = copy_usr_attrs(da->src_coh, *coh);
+                if (rc != 0) {
+                    goto bcast;
+                }
+            }
+        } else if (require_new_cont) {
+            /* We successfully opened the container, but it should not exist */
+            MFU_LOG(MFU_LOG_ERR, "Destination container already exists");
+            goto bcast;
+        }
+
+        /* check container status, and if unhealthy do not continue unless --force
+         * option is used (only for serialization, never for copies) */
+        bool status_healthy;
+        rc = daos_check_cont_status(*coh, &status_healthy);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Checking DAOS container status failed\n");
+            goto bcast;
+        } else if (!status_healthy && !force_serialize) {
+            MFU_LOG(MFU_LOG_ERR, "Container status is unhealthy, stopping\n");
+            goto bcast;
+        }
 
+        /* everything looks good so far */
+        valid = 1;
+    }
 bcast:
+    if (rank == 0) {
+        daos_prop_free(props);
+        mfu_free(&roots);
+#ifdef HDF5_SUPPORT
+        if (preserve) {
+            /* only close if handle is open */
+            if (hdf5.file > 0) {
+                H5Fclose(hdf5.file);
+            }
+        }
+#endif
+    }
+
     /* broadcast valid from rank 0 */
     MPI_Bcast(&valid, 1, MPI_INT, 0, MPI_COMM_WORLD);
 
@@ -580,42 +1361,64 @@ bcast:
 
     /* broadcast container handle from rank 0 */
     daos_bcast_handle(rank, coh, poh, CONT_HANDLE);
-
     return 0;
 }
 
 /* Mount DAOS dfs */
-static int daos_mount(
+static int mfu_dfs_mount(
   mfu_file_t* mfu_file,
   daos_handle_t* poh,
   daos_handle_t* coh)
 {
-    /* Mount dfs */
-    int rc = dfs_mount(*poh, *coh, O_RDWR, &mfu_file->dfs);
+    /* Mount dfs_sys */
+    int rc = dfs_sys_mount(*poh, *coh, O_RDWR, DFS_SYS_NO_LOCK, &mfu_file->dfs_sys);
+    if (rc !=0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to mount DAOS filesystem (DFS): %s", strerror(rc));
+        rc = -1;
+        goto out;
+    }
+
+    /* Get underlying DFS base for DFS API calls */
+    rc = dfs_sys2base(mfu_file->dfs_sys, &mfu_file->dfs);
     if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "Failed to mount DAOS filesystem (DFS): "
-                MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+        MFU_LOG(MFU_LOG_ERR, "Failed to get DAOS filesystem (DFS) base: %s", strerror(rc));
         rc = -1;
+        dfs_sys_umount(mfu_file->dfs_sys);
+        mfu_file->dfs_sys = NULL;
     }
 
+out:
     return rc;
 }
 
-/* Unmount DAOS dfs.
- * Cleanup up hash */
-static int daos_umount(
+/* Unmount DAOS dfs */
+static int mfu_dfs_umount(
   mfu_file_t* mfu_file)
 {
-    /* Unmount dfs */
-    int rc = dfs_umount(mfu_file->dfs);
+    if ((mfu_file == NULL) || (mfu_file->dfs_sys == NULL)) {
+        return 0;
+    }
+
+    /* Unmount dfs_sys */
+    int rc = dfs_sys_umount(mfu_file->dfs_sys);
     if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "Failed to unmount DFS namespace");
+        MFU_LOG(MFU_LOG_ERR, "Failed to unmount DAOS filesystem (DFS): %s", strerror(rc));
         rc = -1;
     }
+    mfu_file->dfs_sys = NULL;
+
+    return rc;
+}
+
+static inline int mfu_daos_destroy_snap(daos_handle_t coh, daos_epoch_t epoch)
+{
+    daos_epoch_range_t epr;
+    epr.epr_lo = epoch;
+    epr.epr_hi = epoch;
 
-    /* Clean up the hash */
-    if (mfu_file->dfs_hash != NULL) {
-        d_hash_table_destroy(mfu_file->dfs_hash, true);
+    int rc = daos_cont_destroy_snap(coh, epr, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "DAOS destroy snapshot failed: "DF_RC, DP_RC(rc));
     }
 
     return rc;
@@ -637,16 +1440,28 @@ daos_args_t* daos_args_new(void)
     da->src_path   = NULL;
     da->dst_path   = NULL;
 
-    /* initalize value of DAOS UUID's to NULL with uuid_clear */
-    uuid_clear(da->src_pool_uuid);
-    uuid_clear(da->dst_pool_uuid);
-    uuid_clear(da->src_cont_uuid);
-    uuid_clear(da->dst_cont_uuid);
+    memset(da->src_pool, '\0', DAOS_PROP_LABEL_MAX_LEN + 1);
+    memset(da->src_cont, '\0', DAOS_PROP_LABEL_MAX_LEN + 1);
+    memset(da->dst_pool, '\0', DAOS_PROP_LABEL_MAX_LEN + 1);
+    memset(da->dst_cont, '\0', DAOS_PROP_LABEL_MAX_LEN + 1);
 
     /* By default, try to automatically determine the API */
+    /* By default, try to automatically determine the API */
     da->api = DAOS_API_AUTO;
 
-    da->epc = 0;
+    /* Default to 0 for "no epoch" */
+    da->src_epc = 0;
+    da->dst_epc = 0;
+
+    /* Default does not allow the destination container to exist for DAOS API */
+    da->allow_exist_dst_cont = false;
+
+    da->src_cont_type = DAOS_PROP_CO_LAYOUT_UNKOWN;
+    da->dst_cont_type = DAOS_PROP_CO_LAYOUT_UNKOWN;
+
+    /* by default do not preserve daos metadata */
+    da->daos_preserve = false;
+    da->daos_preserve_path = NULL;
 
     return da;
 }
@@ -658,6 +1473,7 @@ void daos_args_delete(daos_args_t** pda)
         mfu_free(&da->dfs_prefix);
         mfu_free(&da->src_path);
         mfu_free(&da->dst_path);
+        mfu_free(&da->daos_preserve_path);
         mfu_free(pda);
     }
 }
@@ -674,6 +1490,8 @@ int daos_parse_api_str(
         *api = DAOS_API_DFS;
     } else if (strcasecmp(api_str, "DAOS") == 0) {
         *api = DAOS_API_DAOS;
+    } else if (strcasecmp(api_str, "HDF5") == 0) {
+        *api = DAOS_API_HDF5;
     } else {
         rc = 1;
     }
@@ -693,14 +1511,110 @@ int daos_parse_epc_str(
     return 0;
 }
 
+#ifdef HDF5_SUPPORT
+static int serialize_daos_metadata(char *filename,
+                                   daos_args_t* da)
+{
+    int    rc = 0;
+    hid_t  status = 0;
+    struct hdf5_args hdf5 = {0};
+
+    MFU_LOG(MFU_LOG_INFO, "Writing metadata file: %s", da->daos_preserve_path);
+
+    /* TODO: much of this HDF5 setup should get moved into the HDF5 user
+     * interface */
+    hdf5.file = H5Fcreate(da->daos_preserve_path, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
+    if (hdf5.file < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to create hdf5 file");
+        goto out;
+    }
+    rc = cont_serialize_props(&hdf5, da->src_coh);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to serialize cont layout: "DF_RC, DP_RC(rc));
+        goto out;
+    }
+
+    /* create User Attributes Dataset in daos_metadata file */
+    hdf5.usr_attr_memtype = H5Tcreate(H5T_COMPOUND, sizeof(usr_attr_t));
+    if (hdf5.usr_attr_memtype < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr memtype");
+        goto out;
+    }
+    hdf5.usr_attr_name_vtype = H5Tcopy(H5T_C_S1);
+    if (hdf5.usr_attr_name_vtype < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr name vtype");
+        goto out;
+    }
+    status = H5Tset_size(hdf5.usr_attr_name_vtype, H5T_VARIABLE);
+    if (status < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr name vtype");
+        goto out;
+    }
+    hdf5.usr_attr_val_vtype = H5Tvlen_create(H5T_NATIVE_OPAQUE);
+    if (hdf5.usr_attr_val_vtype < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr val vtype");
+        goto out;
+    }
+    status = H5Tinsert(hdf5.usr_attr_memtype, "Attribute Name",
+                       HOFFSET(usr_attr_t, attr_name), hdf5.usr_attr_name_vtype);
+    if (status < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to insert user attr name");
+        goto out;
+    }
+    status = H5Tinsert(hdf5.usr_attr_memtype, "Attribute Value",
+                       HOFFSET(usr_attr_t, attr_val), hdf5.usr_attr_val_vtype);
+    if (status < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to insert user attr val");
+        goto out;
+    }
+
+    /* serialize the DAOS user attributes*/
+    rc = cont_serialize_usr_attrs(&hdf5, da->src_coh);
+    if (rc != 0) {
+       MFU_LOG(MFU_LOG_ERR, "failed to serialize user attributes: "DF_RC, DP_RC(rc));
+       goto out;
+    }
+    status = H5Tclose(hdf5.usr_attr_name_vtype);
+    if (status < 0) {
+       rc = 1;
+       MFU_LOG(MFU_LOG_ERR, "failed to close user attr name datatype");
+        
+    }
+    status = H5Tclose(hdf5.usr_attr_val_vtype);
+    if (status < 0) {
+       rc = 1;
+       MFU_LOG(MFU_LOG_ERR, "failed to close user attr value datatype");
+    }
+out:
+    mfu_free(&filename);
+    return rc;
+}
+#endif
+
 int daos_setup(
     int rank,
     char** argpaths,
+    int numpaths,
     daos_args_t* da,
     mfu_file_t* mfu_src_file,
     mfu_file_t* mfu_dst_file)
 {
     int tmp_rc;
+    bool have_src = ((numpaths > 0) && (mfu_src_file != NULL));
+    bool have_dst = ((numpaths > 1) && (mfu_dst_file != NULL));
+    bool dst_cont_passed = false;
+
+    /* Sanity check that we have paths */
+    if (!have_src && !have_dst) {
+        return 0;
+    }
 
     /* Each process keeps track of whether it had any DAOS errors.
      * If there weren't any daos args, then ignore daos_init errors.
@@ -712,16 +1626,15 @@ int daos_setup(
      * Later, ignore if no daos args supplied */
     tmp_rc = daos_init();
     if (tmp_rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "Failed to initialize daos");
+        MFU_LOG(MFU_LOG_ERR, "Failed to initialize daos: "DF_RC, DP_RC(tmp_rc));
         local_daos_error = true;
     }
 
     /* Do a preliminary check on the DAOS args */
     if (!local_daos_error) {
-        tmp_rc = daos_check_args(rank, argpaths, da, &flag_daos_args);
+        tmp_rc = daos_check_args(rank, argpaths, numpaths, da, &flag_daos_args);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Invalid DAOS args: "
-                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS_INVAL_ARG));
+            MFU_LOG(MFU_LOG_ERR, "Invalid DAOS args: " MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS_INVAL_ARG));
             local_daos_error = true;
         }
     }
@@ -731,20 +1644,18 @@ int daos_setup(
      * prefix since the path is mapped to the root
      * of the container in the DAOS DFS mount */
     if (!local_daos_error) {
-        tmp_rc = daos_set_paths(rank, argpaths, da);
+        tmp_rc = daos_set_paths(rank, argpaths, numpaths, da, &dst_cont_passed);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Invalid DAOS args: "
-                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS_INVAL_ARG));
+            MFU_LOG(MFU_LOG_ERR, "Invalid DAOS args: " MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS_INVAL_ARG));
             local_daos_error = true;
         }
     }
 
     /* Re-check the required DAOS arguments (if any) */
     if (!local_daos_error) {
-        tmp_rc = daos_check_args(rank, argpaths, da, &flag_daos_args);
+        tmp_rc = daos_check_args(rank, argpaths, numpaths, da, &flag_daos_args);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Invalid DAOS args: "
-                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS_INVAL_ARG));
+            MFU_LOG(MFU_LOG_ERR, "Invalid DAOS args: " MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS_INVAL_ARG));
             local_daos_error = true;
         }
     }
@@ -757,76 +1668,192 @@ int daos_setup(
         return 1;
     }
 
+    /* At this point, we don't have any errors */
+    local_daos_error = false;
+
     /* Check whether we have pool/cont uuids */
-    bool have_src_pool  = daos_uuid_valid(da->src_pool_uuid);
-    bool have_src_cont  = daos_uuid_valid(da->src_cont_uuid);
-    bool have_dst_pool  = daos_uuid_valid(da->dst_pool_uuid);
-    bool have_dst_cont  = daos_uuid_valid(da->dst_cont_uuid);
+    bool have_src_pool  = strlen(da->src_pool) > 0 ? true : false;
+    bool have_src_cont  = strlen(da->src_cont) > 0 ? true : false;
+    bool have_dst_pool  = strlen(da->dst_pool) > 0 ? true : false;
 
     /* Check if containers are in the same pool */
-    bool same_pool = (uuid_compare(da->src_pool_uuid, da->dst_pool_uuid) == 0);
+    bool same_pool = (strcmp(da->src_pool, da->dst_pool) == 0);
+
+    bool connect_pool = true;
+    bool create_cont = false;
+    bool require_new_cont = false;
+    bool preserve = false;
 
     /* connect to DAOS source pool if uuid is valid */
-    if (!local_daos_error && have_src_pool && have_src_cont) {
-        /* Open pool connection, but do not create container if non-existent */
-        tmp_rc = daos_connect(rank, da->src_pool_uuid,
-                da->src_cont_uuid, &da->src_poh, &da->src_coh, true, false);
+    if (have_src_cont) {
+        /* Sanity check */
+        if (!have_src_pool) {
+            if (rank == 0) {
+                MFU_LOG(MFU_LOG_ERR, "Source container requires source pool");
+            }
+            local_daos_error = true;
+            goto out;
+        }
+        tmp_rc = daos_connect(rank, da, &da->src_pool, &da->src_cont,
+                              &da->src_poh, &da->src_coh, false,
+                              connect_pool, create_cont, require_new_cont,
+                              preserve, mfu_src_file, dst_cont_passed);
         if (tmp_rc != 0) {
             /* tmp_rc from daos_connect is collective */
             local_daos_error = true;
+            goto out;
+        }
+        /* Get the container type */
+        tmp_rc = daos_get_cont_type(da->src_coh, &da->src_cont_type);
+        if (tmp_rc != 0) {
+            /* ideally, this should be the same for each process */
+            local_daos_error = true;
+            goto out;
+        }
+        /* Set the src api based on the container type */
+        tmp_rc = daos_set_api_cont_type(mfu_src_file, da->src_cont_type, da->api);
+        if (tmp_rc != 0) {
+            local_daos_error = true;
+            goto out;
+        }
+        /* Sanity check before we create a new container */
+        if (da->src_cont_type != DAOS_PROP_CO_LAYOUT_POSIX) {
+            if (strcmp(da->src_path, "/") != 0) {
+                if (rank == 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Cannot use path with non-POSIX container.");
+                }
+                local_daos_error = true;
+                goto out;
+            }
         }
     }
 
+    /* If we're using the DAOS API, the destination container cannot
+     * exist already, unless overriden by allow_exist_dst_cont. */
+    if (mfu_src_file->type != POSIX && mfu_src_file->type != DFS && !da->allow_exist_dst_cont) {
+        require_new_cont = true;
+    }
+
     /* If the source and destination are in the same pool,
      * then open the container in that pool.
      * Otherwise, connect to the second pool and open the container */
-    if (!local_daos_error && have_dst_pool && have_dst_cont) {
+    if (have_dst_pool) {
+        /* Sanity check before we create a new container */
+        if (require_new_cont && da->api == DAOS_API_DAOS) {
+            if (strcmp(da->dst_path, "/") != 0) {
+                if (rank == 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Cannot use path with non-POSIX container.");
+                }
+                local_daos_error = true;
+                goto out;
+            }
+        }
+
+        /* TODO: pass daos_preserve bool here to daos_connect=true */
+        create_cont = true;
+        /* do check that src is POSIX and since dst has a pool,
+         * then the dst *should* always be DFS, but this is not set
+         * on the destintaion until after daos_connect is called, and
+         * we should read the properties *before* the container is
+         * created. We do not have the destination container type
+         * here yet which is why extra check is used then passed
+         * to daos_connect */
+        if (da->daos_preserve) {
+            if (mfu_src_file->type == POSIX) {
+                preserve = true; 
+            } else {
+                local_daos_error = true;
+                MFU_LOG(MFU_LOG_ERR, "Cannot use daos-preserve if source and destination"
+                                     " are DAOS. DAOS metadata is copied by default "
+                                     "when both the source and destination is DAOS");
+                goto out;
+            }
+        }
         if (same_pool) {
-            /* Don't reconnect to pool, but do create container if non-existent */
-            tmp_rc = daos_connect(rank, da->dst_pool_uuid,
-                    da->dst_cont_uuid, &da->src_poh, &da->dst_coh, false, true);
+            connect_pool = false;
+            tmp_rc = daos_connect(rank, da, &da->dst_pool, &da->dst_cont,
+                                  &da->src_poh, &da->dst_coh, false,
+                                  connect_pool, create_cont, require_new_cont,
+                                  preserve, mfu_src_file, dst_cont_passed);
         } else {
-            /* Open pool connection, and create container if non-existent */
-            tmp_rc = daos_connect(rank, da->dst_pool_uuid,
-                    da->dst_cont_uuid, &da->dst_poh, &da->dst_coh, true, true);
+            connect_pool = true;
+            tmp_rc = daos_connect(rank, da, &da->dst_pool, &da->dst_cont,
+                                  &da->dst_poh, &da->dst_coh, false,
+                                  connect_pool, create_cont, require_new_cont,
+                                  preserve, mfu_src_file, dst_cont_passed);
         }
         if (tmp_rc != 0) {
             /* tmp_rc from daos_connect is collective */
             local_daos_error = true;
+            goto out;
         }
-    }
-
-    /* Figure out if we should use the DFS or DAOS API */
-    if (!local_daos_error) {
-        tmp_rc = daos_set_api(mfu_src_file, mfu_dst_file, da, argpaths);
+        /* Get the container type */
+        tmp_rc = daos_get_cont_type(da->dst_coh, &da->dst_cont_type);
         if (tmp_rc != 0) {
+            /* ideally, this should be the same for each process */
             local_daos_error = true;
+            goto out;
+        }
+        if (have_dst) {
+            /* Set the dst api based on the container type */
+            tmp_rc = daos_set_api_cont_type(mfu_dst_file, da->dst_cont_type, da->api);
+            if (tmp_rc != 0) {
+                local_daos_error = true;
+                goto out;
+            }
         }
     }
 
+    /* Figure out if we should use the DFS or DAOS API */
+    tmp_rc = daos_set_api_compat(mfu_src_file, mfu_dst_file, da, argpaths);
+    if (tmp_rc != 0) {
+        local_daos_error = true;
+        goto out;
+    }
+
     /* Mount source DFS container */
-    if (!local_daos_error && mfu_src_file->type == DFS) {
-        tmp_rc = daos_mount(mfu_src_file, &da->src_poh, &da->src_coh);
+    if (mfu_src_file->type == DFS) {
+        tmp_rc = mfu_dfs_mount(mfu_src_file, &da->src_poh, &da->src_coh);
         if (tmp_rc != 0) {
             local_daos_error = true;
+            goto out;
         }
     }
 
-    /* Mount destination DFS container */
-    if (!local_daos_error && mfu_dst_file->type == DFS) {
-        if (same_pool) {
-            tmp_rc = daos_mount(mfu_dst_file, &da->src_poh, &da->dst_coh);
-        } else {
-            tmp_rc = daos_mount(mfu_dst_file, &da->dst_poh, &da->dst_coh);
-        }
-        if (tmp_rc != 0) {
-            local_daos_error = true;
+    if (have_dst) {
+        /* Mount destination DFS container */
+        if (mfu_dst_file->type == DFS) {
+            if (same_pool) {
+                tmp_rc = mfu_dfs_mount(mfu_dst_file, &da->src_poh, &da->dst_coh);
+            } else {
+                tmp_rc = mfu_dfs_mount(mfu_dst_file, &da->dst_poh, &da->dst_coh);
+            }
+            if (tmp_rc != 0) {
+                local_daos_error = true;
+                goto out;
+            }
         }
     }
 
+    /* check daos_preserve
+     * if src cont is DFS and dst is POSIX, then write 
+     * cont props and user attrs */
+#ifdef HDF5_SUPPORT
+    char *filename = NULL;
+    if (da->daos_preserve && rank == 0) {
+        if (mfu_src_file->type == DFS && mfu_dst_file->type == POSIX) {
+            tmp_rc = serialize_daos_metadata(filename, da);
+            if (tmp_rc != 0) {
+                local_daos_error = true;
+                MFU_LOG(MFU_LOG_ERR, "failed serialize DAOS metadata: "DF_RC,
+                        DP_RC(tmp_rc));
+            }
+        }
+    }
+#endif
+out:
     /* Return if any process had a daos error */
     if (daos_any_error(rank, local_daos_error, flag_daos_args)) {
-        tmp_rc = daos_fini();
         return 1;
     }
 
@@ -843,70 +1870,92 @@ int daos_cleanup(
     int rc = 0;
     int tmp_rc;
 
-    bool same_pool = false;
-    if (daos_uuid_valid(da->src_pool_uuid) && daos_uuid_valid(da->dst_pool_uuid)
-            && uuid_compare(da->src_pool_uuid, da->dst_pool_uuid) == 0) {
-        same_pool = true;
-    }
+    /* get our rank */
+    int rank;
+    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
 
-    /* Unmount source DFS container */
-    if (mfu_src_file->type == DFS) {
-        tmp_rc = daos_umount(mfu_src_file);
-        MPI_Barrier(MPI_COMM_WORLD);
+    bool same_pool = (strcmp(da->src_pool, da->dst_pool) == 0);
+
+    /* Destroy source snapshot */
+    if (rank == 0 && da->src_epc != 0) {
+        tmp_rc = mfu_daos_destroy_snap(da->src_coh, da->src_epc);
         if (tmp_rc != 0) {
             rc = 1;
         }
     }
 
-    /* Close source container */
-    if (mfu_src_file->type == DFS || mfu_src_file->type == DAOS) {
-        tmp_rc = daos_cont_close(da->src_coh, NULL);
-        MPI_Barrier(MPI_COMM_WORLD);
+    /* Destroy destination snapshot */
+    if (rank == 0 && da->dst_epc != 0) {
+        tmp_rc = mfu_daos_destroy_snap(da->dst_coh, da->dst_epc);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Failed to close container (%d)", rc);
             rc = 1;
         }
     }
 
+    /* Don't close containers until snapshots are destroyed */
+    MPI_Barrier(MPI_COMM_WORLD);
+
+    /* Unmount source DFS container */
+    tmp_rc = mfu_dfs_umount(mfu_src_file);
+    if (tmp_rc != 0) {
+        rc = 1;
+    }
+
     /* Unmount destination DFS container */
-    if (mfu_dst_file->type == DFS) {
-        tmp_rc = daos_umount(mfu_dst_file);
-        MPI_Barrier(MPI_COMM_WORLD);
+    tmp_rc = mfu_dfs_umount(mfu_dst_file);
+    if (tmp_rc != 0) {
+        rc = 1;
+    }
+
+    /* Wait for unmount */
+    MPI_Barrier(MPI_COMM_WORLD);
+
+    /* Close source container */
+    if (daos_handle_is_valid(da->src_coh)) {
+        tmp_rc = daos_cont_close(da->src_coh, NULL);
         if (tmp_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to close source container "DF_RC, DP_RC(tmp_rc));
             rc = 1;
         }
+        da->src_coh = DAOS_HDL_INVAL;
     }
 
     /* Close destination container */
-    if (mfu_dst_file->type == DFS || mfu_dst_file->type == DAOS) {
+    if (daos_handle_is_valid(da->dst_coh)) {
         tmp_rc = daos_cont_close(da->dst_coh, NULL);
-        MPI_Barrier(MPI_COMM_WORLD);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Failed to close container (%d)", rc);
+            MFU_LOG(MFU_LOG_ERR, "Failed to close destination container "DF_RC, DP_RC(tmp_rc));
             rc = 1;
         }
+        da->dst_coh = DAOS_HDL_INVAL;
     }
 
+    /* Wait for container close */
+    MPI_Barrier(MPI_COMM_WORLD);
+
     /* Close source pool */
-    if (mfu_src_file->type == DFS || mfu_src_file->type == DAOS) {
+    if (daos_handle_is_valid(da->src_poh)) {
         tmp_rc = daos_pool_disconnect(da->src_poh, NULL);
-        MPI_Barrier(MPI_COMM_WORLD);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Failed to disconnect from source pool");
+            MFU_LOG(MFU_LOG_ERR, "Failed to disconnect from source pool "DF_RC, DP_RC(tmp_rc));
             rc = 1;
         }
+        da->src_poh = DAOS_HDL_INVAL;
     }
 
     /* Close destination pool */
-    if ((mfu_dst_file->type == DFS || mfu_dst_file->type == DAOS) && !same_pool) {
+    if (daos_handle_is_valid(da->dst_poh) && !same_pool) {
         tmp_rc = daos_pool_disconnect(da->dst_poh, NULL);
-        MPI_Barrier(MPI_COMM_WORLD);
         if (tmp_rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "Failed to disconnect from destination pool");
+            MFU_LOG(MFU_LOG_ERR, "Failed to disconnect from destination pool "DF_RC, DP_RC(tmp_rc));
             rc = 1;
         }
+        da->dst_poh = DAOS_HDL_INVAL;
     }
 
+    /* Wait for pool close */
+    MPI_Barrier(MPI_COMM_WORLD);
+
     /* Finalize DAOS */
     tmp_rc = daos_fini();
     if (tmp_rc != 0) {
@@ -919,118 +1968,301 @@ int daos_cleanup(
     return rc;
 }
 
-static int daos_copy_recx_single(daos_key_t *dkey,
-                            daos_handle_t *src_oh,
-                            daos_handle_t *dst_oh,
-                            daos_iod_t *iod)
+/* Copy a single recx from a src obj to dst obj for a given dkey/akey.
+ * returns -1 on error, 0 if same, 1 if different. */
+static int mfu_daos_obj_sync_recx_single(
+    daos_key_t *dkey,
+    daos_handle_t *src_oh,
+    daos_handle_t *dst_oh,
+    daos_iod_t *iod,
+    bool compare_dst,
+    bool write_dst,
+    mfu_daos_stats_t* stats)
 {
-    /* if iod_type is single value just fetch iod size from source
-     * and update in destination object */
-    int buf_len = (int)(*iod).iod_size;
-    char buf[buf_len];
-    d_sg_list_t sgl;
-    d_iov_t iov;
-    int rc;
-
-    /* set sgl values */
-    sgl.sg_nr     = 1;
-    sgl.sg_nr_out = 0;
-    sgl.sg_iovs   = &iov;
-    d_iov_set(&iov, buf, buf_len);
-    rc = daos_obj_fetch(*src_oh, DAOS_TX_NONE, 0, dkey, 1, iod, &sgl, NULL, NULL);
+    bool        dst_equal = false;  /* equal until found otherwise */
+    uint64_t    src_buf_len = (*iod).iod_size;
+    char        src_buf[src_buf_len];
+    d_sg_list_t src_sgl;
+    d_iov_t     src_iov;
+    int         rc;
+
+    /* set src_sgl values */
+    src_sgl.sg_nr     = 1;
+    src_sgl.sg_nr_out = 0;
+    src_sgl.sg_iovs   = &src_iov;
+    d_iov_set(&src_iov, src_buf, src_buf_len);
+
+    /* Fetch the source */
+    rc = daos_obj_fetch(*src_oh, DAOS_TX_NONE, 0, dkey, 1, iod, &src_sgl, NULL, NULL);
     if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS object fetch returned with errors: ", MFU_ERRF,
-	        MFU_ERRP(-MFU_ERR_DAOS));
-        return 1;
+        MFU_LOG(MFU_LOG_ERR, "DAOS object fetch returned with errors "DF_RC, DP_RC(rc));
+        goto out_err;
     }
-    rc = daos_obj_update(*dst_oh, DAOS_TX_NONE, 0, dkey, 1, iod, &sgl, NULL);
-    if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS object update returned with errors: ", MFU_ERRF,
-	        MFU_ERRP(-MFU_ERR_DAOS));
-        return 1;
+    stats->bytes_read += src_buf_len;
+
+    /* Sanity check */
+    if (src_sgl.sg_nr_out != 1) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to fetch single recx.");
+        goto out_err;
+    }
+
+    /* Conditionally compare the destination before writing */
+    if (compare_dst) {
+        uint64_t    dst_buf_len = (*iod).iod_size;
+        char        dst_buf[dst_buf_len];
+        d_sg_list_t dst_sgl;
+        d_iov_t     dst_iov;
+
+        dst_sgl.sg_nr       = 1;
+        dst_sgl.sg_nr_out   = 0;
+        dst_sgl.sg_iovs     = &dst_iov;
+
+        d_iov_set(&dst_iov, dst_buf, dst_buf_len);
+        rc = daos_obj_fetch(*dst_oh, DAOS_TX_NONE, 0, dkey, 1, iod, &dst_sgl, NULL, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS object fetch returned with errors "DF_RC, DP_RC(rc));
+            goto out_err;
+        }
+        /* Reset iod values after fetching the destination */
+        (*iod).iod_nr = 1;
+        (*iod).iod_size = src_buf_len;
+
+        /* Determine whether the dst is equal to the src */
+        if (dst_sgl.sg_nr_out > 0) {
+            stats->bytes_read += dst_buf_len;
+            dst_equal = (memcmp(src_buf, dst_buf, src_buf_len) == 0);
+        }
+    }
+
+    /* Conditionally write to the destination */
+    if (write_dst && !dst_equal) {
+        rc = daos_obj_update(*dst_oh, DAOS_TX_NONE, 0, dkey, 1, iod, &src_sgl, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS object update returned with errors "DF_RC, DP_RC(rc));
+            goto out_err;
+        }
+        stats->bytes_written += src_buf_len;
+    }
+
+    /* return 0 if equal, 1 if different */
+    if (dst_equal) {
+        return 0;
+    }
+    return 1;
+
+out_err:
+    /* return -1 on true error */
+    return -1;
+}
+
+/* Free a buffer created with alloc_iov_buf */
+static void free_iov_buf(
+    uint32_t    number, /* buffer length */
+    char**      buf)    /* pointer to static buffer */
+{
+    if (buf != NULL) {
+        for (uint32_t i = 0; i < number; i++) {
+            mfu_free(&buf[i]);
+        }
     }
-    return rc;
 }
 
-static int daos_copy_recx_array(daos_key_t *dkey,
-                           daos_key_t *akey,
-                           daos_handle_t *src_oh,
-                           daos_handle_t *dst_oh,
-                           daos_iod_t *iod)
+/* Create a buffer based on recxs and set iov for each index */
+static int alloc_iov_buf(
+    uint32_t        number, /* number of recxs, iovs, and buffer */
+    daos_size_t     size,   /* size of each record */
+    daos_recx_t*    recxs,  /* array of recxs */
+    d_iov_t*        iov,    /* array of iovs */
+    char**          buf,    /* pointer to static buffer */
+    uint64_t*       buf_len)/* pointer to static buffer lengths */
 {
-    daos_anchor_t recx_anchor = {0}; 
+    for (uint32_t i = 0; i < number; i++) {
+        buf_len[i] = recxs[i].rx_nr * size;
+        buf[i] = calloc(buf_len[i], sizeof(void*));
+        if (buf[i] == NULL) {
+            free_iov_buf(number, buf);
+            return -1;
+        }
+        d_iov_set(&iov[i], buf[i], buf_len[i]);
+    }
+
+    return 0;
+}
+
+/* Sum an array of uint64_t */
+static uint64_t sum_uint64_t(uint32_t number, uint64_t* buf)
+{
+    uint64_t sum = 0;
+    for (uint32_t i = 0; i < number; i++) {
+        sum += buf[i];
+    }
+    return sum;
+}
+
+/* Copy all array recx from a src obj to dst obj for a given dkey/akey.
+ * returns -1 on error, 0 if same, 1 if different */
+static int mfu_daos_obj_sync_recx_array(
+    daos_key_t *dkey,
+    daos_key_t *akey,
+    daos_handle_t *src_oh,
+    daos_handle_t *dst_oh,
+    daos_iod_t *iod,
+    bool compare_dst,
+    bool write_dst,
+    mfu_daos_stats_t* stats)
+{
+    bool        all_dst_equal = true;   /* equal until found otherwise */
+    uint32_t    max_number = 5;         /* max recxs per fetch */
+    char*       src_buf[max_number];    /* src buffer data */
+    uint64_t    src_buf_len[max_number];/* src buffer lengths */
+    d_sg_list_t src_sgl;
+    daos_recx_t recxs[max_number];
+    daos_size_t size;
+    daos_epoch_range_t eprs[max_number];
+
+    daos_anchor_t recx_anchor = {0};
     int rc;
-    int i;
     while (!daos_anchor_is_eof(&recx_anchor)) {
-        daos_epoch_range_t	eprs[5];
-        daos_recx_t		recxs[5];
-        daos_size_t		size;
-
         /* list all recx for this dkey/akey */
-        uint32_t number = 5;
+        uint32_t number = max_number;
         rc = daos_obj_list_recx(*src_oh, DAOS_TX_NONE, dkey, akey,
-	                        &size, &number, recxs, eprs,
-				&recx_anchor, true, NULL);
+                                &size, &number, recxs, eprs,
+                                &recx_anchor, true, NULL);
         if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_list_recx returned with errors: ", MFU_ERRF,
-	            MFU_ERRP(-MFU_ERR_DAOS));
-            return 1;
+            MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_list_recx returned with errors "DF_RC, DP_RC(rc));
+            goto out_err;
         }
 
         /* if no recx is returned for this dkey/akey move on */
         if (number == 0) 
             continue;
 
-        for (i = 0; i < number; i++) {
-            uint64_t    buf_len = recxs[i].rx_nr;
-            char        buf[buf_len];
-            d_sg_list_t sgl;
-            d_iov_t     iov;
+        d_iov_t src_iov[number];
 
-            /* set iod values */
-            (*iod).iod_type  = DAOS_IOD_ARRAY;
-            (*iod).iod_size  = 1;
-            (*iod).iod_nr    = 1;
-            (*iod).iod_recxs = &recxs[i];
+        /* set iod values */
+        (*iod).iod_type  = DAOS_IOD_ARRAY;
+        (*iod).iod_nr    = number;
+        (*iod).iod_recxs = recxs;
+        (*iod).iod_size  = size;
 
-            /* set sgl values */
-            sgl.sg_nr     = 1;
-            sgl.sg_nr_out = 0;
-            sgl.sg_iovs   = &iov;
+        /* set src_sgl values */
+        src_sgl.sg_nr_out = 0;
+        src_sgl.sg_iovs   = src_iov;
+        src_sgl.sg_nr     = number;
 
-            /* fetch recx values from source */
-            d_iov_set(&iov, buf, buf_len);	
-            rc = daos_obj_fetch(*src_oh, DAOS_TX_NONE, 0, dkey, 1, iod,
-                                &sgl, NULL, NULL);
+        /* allocate and setup src_buf */
+        if (alloc_iov_buf(number, size, recxs, src_iov, src_buf, src_buf_len) != 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS failed to allocate source buffer.");
+            goto out_err;
+        }
+
+        bool recx_equal = false;
+        uint64_t total_bytes = sum_uint64_t(number, src_buf_len);
+
+        /* fetch recx values from source */
+        rc = daos_obj_fetch(*src_oh, DAOS_TX_NONE, 0, dkey, 1, iod,
+                            &src_sgl, NULL, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS object fetch returned with errors "DF_RC, DP_RC(rc));
+            goto out_err;
+        }
+
+        /* Sanity check */
+        if (src_sgl.sg_nr_out != number) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to fetch array recxs.");
+            goto out_err;
+        }
+
+        stats->bytes_read += total_bytes;
+
+        /* Conditionally compare the destination before writing */
+        if (compare_dst) {
+            char*       dst_buf[number];
+            uint64_t    dst_buf_len[number];
+            d_sg_list_t dst_sgl;
+            d_iov_t     dst_iov[number];
+
+            dst_sgl.sg_nr_out = 0;
+            dst_sgl.sg_iovs   = dst_iov;
+            dst_sgl.sg_nr     = number;
+
+            /* allocate and setup dst_buf */
+            if (alloc_iov_buf(number, size, recxs, dst_iov, dst_buf, dst_buf_len) != 0) {
+                MFU_LOG(MFU_LOG_ERR, "DAOS failed to allocate destination buffer.");
+                goto out_err;
+            }
+
+            rc = daos_obj_fetch(*dst_oh, DAOS_TX_NONE, 0, dkey, 1, iod,
+                                &dst_sgl, NULL, NULL);
             if (rc != 0) {
-                MFU_LOG(MFU_LOG_ERR, "DAOS object fetch returned with errors: ", MFU_ERRF,
-	                MFU_ERRP(-MFU_ERR_DAOS));
-                return 1;
+                MFU_LOG(MFU_LOG_ERR, "DAOS object fetch returned with errors "DF_RC, DP_RC(rc));
+                free_iov_buf(number, dst_buf);
+                goto out_err;
+            }
+
+            /* Reset iod values after fetching the destination */
+            (*iod).iod_nr    = number;
+            (*iod).iod_size  = size;
+
+            /* Determine whether all recxs in the dst are equal to the src.
+             * If any recx is different, update all recxs in dst and flag
+             * this akey as different. */
+            if (dst_sgl.sg_nr_out > 0) {
+                stats->bytes_read += total_bytes;
+                recx_equal = true;
+                for (uint32_t i = 0; i < number; i++) {
+                    if (memcmp(src_buf[i], dst_buf[i], src_buf_len[i]) != 0) {
+                        recx_equal = false;
+                        all_dst_equal = false;
+                        break;
+                    }
+                }
             }
+            free_iov_buf(number, dst_buf);
+        }
 
-            /* update fetched recx values and place in destination object */
+        /* Conditionally write to the destination */
+        if (write_dst && !recx_equal) {
             rc = daos_obj_update(*dst_oh, DAOS_TX_NONE, 0, dkey, 1, iod,
-                                 &sgl, NULL);
+                                 &src_sgl, NULL);
             if (rc != 0) {
-                MFU_LOG(MFU_LOG_ERR, "DAOS object update returned with errors: ", MFU_ERRF,
-	                MFU_ERRP(-MFU_ERR_DAOS));
-                return 1;
+                MFU_LOG(MFU_LOG_ERR, "DAOS object update returned with errors "DF_RC, DP_RC(rc));
+                goto out_err;
             }
-	
+            stats->bytes_written += total_bytes;
         }
+        free_iov_buf(number, src_buf);
     }
-    return rc;
-}
 
-static int daos_copy_list_keys(daos_handle_t *src_oh,
-                          daos_handle_t *dst_oh)
-{
-    /* loop to enumerate dkeys */
+    /* return 0 if equal, 1 if different */
+    if (all_dst_equal) {
+        return 0;
+    }
+    return 1;
+
+out_err:
+    /* return -1 on true errors */
+    return -1;
+}
+
+/* Copy all dkeys and akeys from a src obj to dst obj.
+ * returns -1 on error, 0 if same, 1 if different */
+static int mfu_daos_obj_sync_keys(
+  daos_handle_t* src_oh,
+  daos_handle_t* dst_oh,
+  bool compare_dst,
+  bool write_dst,
+  mfu_daos_stats_t* stats)
+{
+    /* Assume src and dst are equal until found otherwise */
+    bool all_dst_equal = true;
+
+    /* loop to enumerate dkeys */
     daos_anchor_t dkey_anchor = {0}; 
     int rc;
     while (!daos_anchor_is_eof(&dkey_anchor)) {
-        d_sg_list_t     dkey_sgl;
-        d_iov_t         dkey_iov;
+        d_sg_list_t     dkey_sgl = {0};
+        d_iov_t         dkey_iov = {0};
         daos_key_desc_t dkey_kds[ENUM_DESC_NR]       = {0};
         uint32_t        dkey_number                  = ENUM_DESC_NR;
         char            dkey_enum_buf[ENUM_DESC_BUF] = {0};
@@ -1046,9 +2278,8 @@ static int daos_copy_list_keys(daos_handle_t *src_oh,
         rc = daos_obj_list_dkey(*src_oh, DAOS_TX_NONE, &dkey_number, dkey_kds,
                                 &dkey_sgl, &dkey_anchor, NULL);
         if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_list_dkey returned with errors: ", MFU_ERRF,
-	            MFU_ERRP(-MFU_ERR_DAOS));
-            return 1;
+            MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_list_dkey returned with errors "DF_RC, DP_RC(rc));
+            goto out_err;
         }
 
         /* if no dkeys were returned move on */
@@ -1058,20 +2289,20 @@ static int daos_copy_list_keys(daos_handle_t *src_oh,
         char* dkey_ptr;
         int   i;
 
-        /* parse out individual dkeys based on key length and numver of dkeys returned */
+        /* parse out individual dkeys based on key length and number of dkeys returned */
         for (dkey_ptr = dkey_enum_buf, i = 0; i < dkey_number; i++) {
 
             /* Print enumerated dkeys */
             daos_key_t diov;
-            snprintf(dkey, dkey_kds[i].kd_key_len + 1, "%s", dkey_ptr);
+            memcpy(dkey, dkey_ptr, dkey_kds[i].kd_key_len);
             d_iov_set(&diov, (void*)dkey, dkey_kds[i].kd_key_len);
             dkey_ptr += dkey_kds[i].kd_key_len;
 
             /* loop to enumerate akeys */
             daos_anchor_t akey_anchor = {0}; 
             while (!daos_anchor_is_eof(&akey_anchor)) {
-                d_sg_list_t     akey_sgl;
-                d_iov_t         akey_iov;
+                d_sg_list_t     akey_sgl = {0};
+                d_iov_t         akey_iov = {0};
                 daos_key_desc_t akey_kds[ENUM_DESC_NR]       = {0};
                 uint32_t        akey_number                  = ENUM_DESC_NR;
                 char            akey_enum_buf[ENUM_DESC_BUF] = {0};
@@ -1087,9 +2318,8 @@ static int daos_copy_list_keys(daos_handle_t *src_oh,
                 rc = daos_obj_list_akey(*src_oh, DAOS_TX_NONE, &diov, &akey_number, akey_kds,
                                         &akey_sgl, &akey_anchor, NULL);
                 if (rc != 0) {
-                    MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_list_akey returned with errors: ", MFU_ERRF,
-	                    MFU_ERRP(-MFU_ERR_DAOS));
-                    return 1;
+                    MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_list_akey returned with errors "DF_RC, DP_RC(rc));
+                    goto out_err;
                 }
 
                 /* if no akeys returned move on */
@@ -1099,12 +2329,11 @@ static int daos_copy_list_keys(daos_handle_t *src_oh,
                 int j;
                 char* akey_ptr;
 
-                /* parse out individual akeys based on key length and numver of dkeys returned */
+                /* parse out individual akeys based on key length and number of dkeys returned */
                 for (akey_ptr = akey_enum_buf, j = 0; j < akey_number; j++) {
-                    daos_key_t aiov;
-                    daos_iod_t iod;
-                    daos_recx_t recx;
-                    snprintf(akey, akey_kds[j].kd_key_len + 1, "%s", akey_ptr);
+                    daos_key_t aiov = {0};
+                    daos_iod_t iod = {0};
+                    memcpy(akey, akey_ptr, akey_kds[j].kd_key_len);
                     d_iov_set(&aiov, (void*)akey, akey_kds[j].kd_key_len);
 
                     /* set iod values */
@@ -1112,47 +2341,125 @@ static int daos_copy_list_keys(daos_handle_t *src_oh,
                     iod.iod_type  = DAOS_IOD_SINGLE;
                     iod.iod_size  = DAOS_REC_ANY;
                     iod.iod_recxs = NULL;
-
-                    d_iov_set(&iod.iod_name, (void*)akey, strlen(akey));
+                    iod.iod_name  = aiov;
 
                     /* Do a fetch (with NULL sgl) of single value type, and if that
                      * returns iod_size == 0, then a single value does not exist. */
                     rc = daos_obj_fetch(*src_oh, DAOS_TX_NONE, 0, &diov, 1, &iod, NULL, NULL, NULL);
                     if (rc != 0) {
-                        MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_fetch returned with errors: ", MFU_ERRF,
-	                        MFU_ERRP(-MFU_ERR_DAOS));
-                        return 1;
+                        MFU_LOG(MFU_LOG_ERR, "DAOS daos_obj_fetch returned with errors "DF_RC, DP_RC(rc));
+                        goto out_err;
                     }
 
                     /* if iod_size == 0 then this is a DAOS_IOD_ARRAY type */
                     if ((int)iod.iod_size == 0) {
-                        rc = daos_copy_recx_array(&diov, &aiov, src_oh, dst_oh, &iod);
-                        if (rc != 0) {
-                            MFU_LOG(MFU_LOG_ERR, "DAOS daos_copy_recx_array returned with errors: ", MFU_ERRF,
-	                            MFU_ERRP(-MFU_ERR_DAOS));
-                            return 1;
+                        rc = mfu_daos_obj_sync_recx_array(&diov, &aiov, src_oh, dst_oh,
+                                                          &iod, compare_dst, write_dst, stats);
+                        if (rc == -1) {
+                            MFU_LOG(MFU_LOG_ERR, "DAOS mfu_daos_obj_sync_recx_array returned with errors: "
+                                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+                            goto out_err;
+                        } else if (rc == 1) {
+                            all_dst_equal = false;
                         }
                     } else {
-                        rc = daos_copy_recx_single(&diov, src_oh, dst_oh, &iod);
-                        if (rc != 0) {
-                            MFU_LOG(MFU_LOG_ERR, "DAOS daos_copy_recx_single returned with errors: ", MFU_ERRF,
-	                            MFU_ERRP(-MFU_ERR_DAOS));
-                            return 1;
+                        rc = mfu_daos_obj_sync_recx_single(&diov, src_oh, dst_oh,
+                                                           &iod, compare_dst, write_dst, stats);
+                        if (rc == -1) {
+                            MFU_LOG(MFU_LOG_ERR, "DAOS mfu_daos_obj_sync_recx_single returned with errors: "
+                                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+                            goto out_err;
+                        } else if (rc == 1) {
+                            all_dst_equal = false;
                         }
                     }
 
-                    /* advance to next akey returned */	
+                    /* Increment akeys traversed */
+                    stats->total_akeys++;
+
+                    /* advance to next akey returned */
                     akey_ptr += akey_kds[j].kd_key_len;
                 }
             }
+
+            /* Increment dkeys traversed */
+            stats->total_dkeys++;
         }
     }
-    return rc;
+
+    /* return 0 if equal, 1 if different */
+    if (all_dst_equal) {
+        return 0;
+    }
+    return 1;
+
+out_err:
+    /* return -1 on true errors */
+    return -1;
 }
 
-static int daos_obj_copy(
+static int mfu_daos_obj_sync(
     daos_args_t* da,
-    mfu_flist bflist)
+    daos_handle_t src_coh,
+    daos_handle_t dst_coh,
+    daos_obj_id_t oid,
+    bool compare_dst,             /* Whether to compare the src and dst before writing */
+    bool write_dst,               /* Whether to write to the dst */
+    mfu_daos_stats_t* stats)
+{
+    int rc = 0;
+
+    /* open handle of object in src container */
+    daos_handle_t src_oh;
+    rc = daos_obj_open(src_coh, oid, DAOS_OO_RO, &src_oh, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "DAOS object open returned with errors: " MFU_ERRF,
+                MFU_ERRP(-MFU_ERR_DAOS));
+        goto out_err;
+    }
+
+    /* open handle of object in dst container */
+    daos_handle_t dst_oh;
+    rc = daos_obj_open(dst_coh, oid, DAOS_OO_EXCL, &dst_oh, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "DAOS object open returned with errors: " MFU_ERRF,
+                MFU_ERRP(-MFU_ERR_DAOS));
+        /* make sure to close the source if opening dst fails */
+        daos_obj_close(src_oh, NULL);
+        goto out_err;
+    }
+    int copy_rc = mfu_daos_obj_sync_keys(&src_oh, &dst_oh, compare_dst, write_dst, stats);
+    if (copy_rc == -1) {
+        MFU_LOG(MFU_LOG_ERR, "DAOS copy list keys returned with errors: " MFU_ERRF,
+                MFU_ERRP(-MFU_ERR_DAOS));
+        /* cleanup object handles on failure */
+        daos_obj_close(src_oh, NULL);
+        daos_obj_close(dst_oh, NULL);
+        goto out_err;
+    }
+
+    stats->total_oids++;
+
+    /* close source and destination object */
+    daos_obj_close(src_oh, NULL);
+    daos_obj_close(dst_oh, NULL);
+
+    return copy_rc;
+
+out_err:
+    /* return -1 on true error */
+    return -1;
+}
+
+/* returns -1 on error, 0 if same, 1 if different */
+static int mfu_daos_flist_obj_sync(
+  daos_args_t* da,
+  mfu_flist bflist,
+  daos_handle_t src_coh,
+  daos_handle_t dst_coh,
+  bool compare_dst,
+  bool write_dst,
+  mfu_daos_stats_t* stats)
 {
     int rc = 0;
 
@@ -1161,111 +2468,98 @@ static int daos_obj_copy(
     uint64_t i;
     const elem_t* p = flist->list_head;
     for (i = 0; i < flist->list_count; i++) {
-        /* open DAOS object based on oid[i] to get obj handle */
-        daos_handle_t oh;
         daos_obj_id_t oid;
-        oid.lo = p->obj_id_lo;	
-        oid.hi = p->obj_id_hi;	
-        rc = daos_obj_open(da->src_coh, oid, 0, &oh, NULL);
-        if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS object open returned with errors: ", MFU_ERRF,
-	            MFU_ERRP(-MFU_ERR_DAOS));
-	    return 1;
-        }
-
-        /* open handle of object in dst container */
-        daos_handle_t dst_oh;
-        rc = daos_obj_open(da->dst_coh, oid, 0, &dst_oh, NULL);
-        if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS object open returned with errors: ", MFU_ERRF,
-	            MFU_ERRP(-MFU_ERR_DAOS));
-            /* make sure to close the source if opening dst fails */
-            daos_obj_close(oh, NULL);
-	    return 1;
-        }
-        rc = daos_copy_list_keys(&oh, &dst_oh);
-        if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS copy list keys returned with errors: ", MFU_ERRF,
-                    MFU_ERRP(-MFU_ERR_DAOS));
-            /* cleanup object handles on failure */
-            daos_obj_close(oh, NULL);
-            daos_obj_close(dst_oh, NULL);
-	    return 1;
+        oid.lo = p->obj_id_lo;
+        oid.hi = p->obj_id_hi;
+
+        /* Copy this object */
+        rc = mfu_daos_obj_sync(da, src_coh, dst_coh, oid,
+                               compare_dst, write_dst, stats);
+        if (rc == -1) {
+            MFU_LOG(MFU_LOG_ERR, "mfu_daos_obj_sync return with error");
+            return rc;
         }
 
-        /* close source and destination object */
-        daos_obj_close(oh, NULL);
-        daos_obj_close(dst_oh, NULL);
         p = p->next;
     }
 
     return rc;
 }
 
-static int daos_obj_list_oids(daos_args_t* da, mfu_flist bflist) {
-    /* List objects in src container to be copied to 
-     * destination container */
-    static const int     OID_ARR_SIZE = 50;
+/*
+ * Create a snapshot and oit at the supplied epc.
+ * Add an flist entry for each oid in the oit.
+ */
+static int mfu_daos_obj_list_oids(
+    daos_args_t* da, mfu_flist bflist,
+    daos_handle_t coh, daos_epoch_t epoch)
+{
     daos_obj_id_t        oids[OID_ARR_SIZE];
     daos_anchor_t        anchor;
     uint32_t             oids_nr;
-    daos_handle_t        toh;
+    daos_handle_t        toh = DAOS_HDL_INVAL;
     uint32_t             oids_total = 0;
     int                  rc = 0;
 
-    /* create snapshot to pass to object iterator table */
-    rc = daos_cont_create_snap_opt(da->src_coh, &da->epc, NULL,
-    				   DAOS_SNAP_OPT_CR | DAOS_SNAP_OPT_OIT,
-				   NULL);
-    if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS failed to create snapshot: ", MFU_ERRF,
-                MFU_ERRP(-MFU_ERR_DAOS));
-        return 1;
-    }
-
     /* open object iterator table */
-    rc = daos_oit_open(da->src_coh, da->epc, &toh, NULL);
+    rc = daos_oit_open(coh, epoch, &toh, NULL);
     if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS failed to open oit: ", MFU_ERRF,
-                MFU_ERRP(-MFU_ERR_DAOS));
+        MFU_LOG(MFU_LOG_ERR, "DAOS failed to open oit "DF_RC, DP_RC(rc));
         return 1;
     }
+    memset(&oids, 0, OID_ARR_SIZE*sizeof(daos_obj_id_t));
     memset(&anchor, 0, sizeof(anchor));
-    flist_t* flist = (flist_t*) bflist;
 
     /* list and store all object ids in flist for this epoch */
     while (1) {
         oids_nr = OID_ARR_SIZE;
         rc = daos_oit_list(toh, oids, &oids_nr, &anchor, NULL);
         if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS daos_oit_list returned with errors: ", MFU_ERRF,
-	            MFU_ERRP(-MFU_ERR_DAOS));
+            MFU_LOG(MFU_LOG_ERR, "DAOS daos_oit_list returned with errors "DF_RC, DP_RC(rc));
+            daos_oit_close(toh, NULL);
+            return 1;
+        }
+
+        if (oids_nr < 1) {
+            MFU_LOG(MFU_LOG_ERR, "No Objects Currently in Container, Exiting.");
             daos_oit_close(toh, NULL);
             return 1;
         }
-	int i;
-	/* create element in flist for each obj id retrived */
- 	for (i = 0; i < oids_nr; i++) {
+
+        /* create element in flist for each obj id retrived */
+        for (int i = 0; i < oids_nr; i++) {
             uint64_t idx = mfu_flist_file_create(bflist);
             mfu_flist_file_set_oid(bflist, idx, oids[i]);
-	}
-	oids_total = oids_nr + oids_total;
- 	if (daos_anchor_is_eof(&anchor)) {
- 		break;
- 	}
+            char* name = NULL;
+            if (asprintf(&name, "%llu.%llu", oids[i].hi, oids[i].lo) == -1) {
+                MFU_LOG(MFU_LOG_ERR, "Unable to allocate space for object.");
+                daos_oit_close(toh, NULL);
+                return 1;
+            }
+            mfu_flist_file_set_name(bflist, idx, name);
+            mfu_free(&name);
+        }
+        oids_total = oids_nr + oids_total;
+        if (daos_anchor_is_eof(&anchor)) {
+            break;
+        }
     }
 
     /* close object iterator */
     rc = daos_oit_close(toh, NULL);
     if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS failed to close oit: ", MFU_ERRF,
-                MFU_ERRP(-MFU_ERR_DAOS));
+        MFU_LOG(MFU_LOG_ERR, "DAOS failed to close oit "DF_RC, DP_RC(rc));
+        rc = 1;
     }
     return rc;
 }
 
-int mfu_flist_walk_daos(
+/* Walk objects in daos and insert to given flist.
+ * Returns -1 on failure, 0 on success. */
+int mfu_daos_flist_walk(
     daos_args_t* da,
+    daos_handle_t coh,
+    daos_epoch_t* epoch,
     mfu_flist flist)
 {
     /* assume we'll succeed */
@@ -1277,39 +2571,3378 @@ int mfu_flist_walk_daos(
 
     /* have rank 0 do the work of listing the objects */
     if (rank == 0) {
-        rc = daos_obj_list_oids(da, flist);
+        /* TODO give this some meaningful name? (arg 3) */
+        rc = daos_cont_create_snap_opt(coh, epoch, NULL,
+                                       DAOS_SNAP_OPT_CR | DAOS_SNAP_OPT_OIT,
+                                       NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS failed to create snapshot "DF_RC, DP_RC(rc));
+            rc = -1;
+            goto out_broadcast;
+        }
+
+        rc = mfu_daos_obj_list_oids(da, flist, coh, *epoch);
         if (rc != 0) {
-            MFU_LOG(MFU_LOG_ERR, "DAOS failed to list oids: ",
-                MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+            rc = -1;
+            goto out_broadcast;
         }
     }
 
     /* summarize list since we added items to it */
     mfu_flist_summarize(flist);
 
+out_broadcast:
     /* broadcast return code from rank 0 so everyone knows whether walk succeeded */
     MPI_Bcast(&rc, 1, MPI_INT, 0, MPI_COMM_WORLD);
 
     return rc;
 }
 
-int mfu_flist_copy_daos(
-    daos_args_t* da,
-    mfu_flist flist)
+/* Collectively copy/sync objects in flist to destination listed in daos args.
+ * Copies DAOS data at object level (non-posix).
+ * Returns 0 on success, 1 on failure. */
+int mfu_daos_flist_sync(
+    daos_args_t* da,    /* DAOS args */
+    mfu_flist flist,    /* flist containing oids */
+    bool compare_dst,   /* whether to compare the dst before writing */
+    bool write_dst)     /* whether to actually write to the dst */
 {
-    /* copy object ids listed in flist to destination in daos args */
-    int rc = daos_obj_copy(da, flist);
-    if (rc != 0) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS object copy failed: ",
-            MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
-    }
+    /* get our rank */
+    int rank;
+    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
+
+    /* Initialize some stats */
+    mfu_daos_stats_t stats;
+    mfu_daos_stats_init(&stats);
+    mfu_daos_stats_start(&stats);
+
+    /* evenly spread the objects across all ranks */
+    mfu_flist newflist = mfu_flist_spread(flist);
+
+    /* copy object ids listed in newflist to destination in daos args */
+    int rc = mfu_daos_flist_obj_sync(da, newflist, da->src_coh, da->dst_coh,
+                                     compare_dst, write_dst, &stats);
 
     /* wait until all procs are done copying,
-     * and determine whether everyone succeeded */
-    if (! mfu_alltrue(rc == 0, MPI_COMM_WORLD)) {
+     * and determine whether everyone succeeded. */
+    if (! mfu_alltrue(rc != -1, MPI_COMM_WORLD)) {
         /* someone failed, so return failure on all ranks */
         rc = 1;
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS object copy failed: "
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+        }
+    } else {
+        /* either rc == 0 or rc == 1.
+         * both are success. */
+        rc = 0;
+    }
+
+    /* Record end time */
+    mfu_daos_stats_end(&stats);
+
+    /* Sum and print the stats */
+    mfu_daos_stats_print_sum(rank, &stats, true, true, true, true);
+
+    mfu_flist_free(&newflist);
+
+    return rc;
+}
+
+#ifdef HDF5_SUPPORT
+static inline void init_hdf5_args(struct hdf5_args *hdf5)
+{
+    hdf5->status = 0;
+    hdf5->file = -1;
+    /* User attribute data */
+    hdf5->usr_attr_memtype = 0;
+    hdf5->usr_attr_name_vtype = 0;
+    hdf5->usr_attr_val_vtype = 0;
+    /* OID Data */
+    hdf5->oid_memtype = 0;
+    hdf5->oid_dspace = 0;
+    hdf5->oid_dset = 0;
+    /* DKEY Data */
+    hdf5->dkey_memtype = 0;
+    hdf5->dkey_vtype = 0;
+    hdf5->dkey_dspace = 0;
+    hdf5->dkey_dset = 0;
+    /* AKEY Data */
+    hdf5->akey_memtype = 0;
+    hdf5->akey_vtype = 0;
+    hdf5->akey_dspace = 0;
+    hdf5->akey_dset = 0;
+    /* dims for dsets */
+    hdf5->oid_dims[0] = 0;
+    hdf5->dkey_dims[0] = 0;     
+    hdf5->akey_dims[0] = 0;     
+    /* data for keys */
+    hdf5->oid_data = NULL;
+    hdf5->dkey_data = NULL;
+    hdf5->akey_data = NULL;
+    hdf5->oid = NULL;
+    hdf5->dk = NULL;
+    hdf5->ak = NULL;
+}
+
+static int serialize_kv_rec(struct hdf5_args *hdf5, 
+                            daos_key_t dkey,
+                            daos_handle_t *oh,
+                            uint64_t *dk_index,
+                            char *dkey_val,
+                            mfu_daos_stats_t* stats)
+{
+    void        *buf;
+    int         rc;
+    hvl_t       *kv_val;
+    daos_size_t size = 0;
+
+    /* get the size of the value */
+    rc = daos_kv_get(*oh, DAOS_TX_NONE, 0, dkey_val, &size, buf, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to fetch object "DF_RC, DP_RC(rc));
+        goto out;
+    }
+    buf = MFU_CALLOC(1, size);
+    if (buf == NULL) {
+        rc = -DER_NOMEM;
+        goto out;
+    }
+
+    rc = daos_kv_get(*oh, DAOS_TX_NONE, 0, dkey_val, &size, buf, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to fetch object "DF_RC, DP_RC(rc));
+        goto out;
+    }
+
+    stats->bytes_read += size;
+    kv_val = &(*hdf5->dk)[*dk_index].rec_kv_val;
+    kv_val->p = MFU_CALLOC(1, (uint64_t)size);
+    if (kv_val->p == NULL) {
+        rc = -DER_NOMEM;
+        goto out;
+    }
+    memcpy(kv_val->p, buf, (uint64_t)size);
+    kv_val->len = (uint64_t)size; 
+out:
+    mfu_free(&buf);
+    return rc;
+}
+
+static int serialize_recx_single(struct hdf5_args *hdf5, 
+                                 daos_key_t *dkey,
+                                 daos_handle_t *oh,
+                                 daos_iod_t *iod,
+                                 uint64_t *ak_index,
+                                 mfu_daos_stats_t* stats)
+{
+    /* if iod_type is single value just fetch iod size from source
+     * and update in destination object */
+    int         buf_len = (int)(*iod).iod_size;
+    void        *buf;
+    d_sg_list_t sgl;
+    d_iov_t     iov;
+    int         rc;
+    hvl_t       *single_val;
+
+    buf = MFU_CALLOC(1, buf_len);
+
+    /* set sgl values */
+    sgl.sg_nr     = 1;
+    sgl.sg_nr_out = 0;
+    sgl.sg_iovs   = &iov;
+    d_iov_set(&iov, buf, buf_len);
+    rc = daos_obj_fetch(*oh, DAOS_TX_NONE, 0, dkey, 1, iod, &sgl,
+                        NULL, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to fetch object");
+        goto out;
+    }
+
+    /* Sanity check */
+    if (sgl.sg_nr_out != 1) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to fetch single recx.");
+        rc = 1;
+        goto out;
+    }
+
+    stats->bytes_read += buf_len;
+
+    /* store the single values inside of the akey dataset */
+    single_val = &(*hdf5->ak)[*ak_index].rec_single_val;
+    single_val->p = MFU_CALLOC(1, (uint64_t)(*iod).iod_size);
+    if (single_val->p == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    memcpy(single_val->p, sgl.sg_iovs[0].iov_buf, (uint64_t)(*iod).iod_size);
+    single_val->len = (uint64_t)(*iod).iod_size; 
+out:
+    mfu_free(&buf);
+    return rc;
+}
+
+static int serialize_recx_array(struct hdf5_args *hdf5,
+                                daos_key_t *dkey,
+                                daos_key_t *akey,
+                                char *rec_name,
+                                uint64_t *ak_index,
+                                daos_handle_t *oh,
+                                daos_iod_t *iod,
+                                mfu_daos_stats_t* stats)
+{
+    int                 rc = 0;
+    int                 i = 0;
+    int                 attr_num = 0;
+    int                 buf_len = 0;
+    int                 path_len = 0;
+    uint32_t            number = 5;
+    size_t              nalloc = 0;
+    daos_anchor_t       recx_anchor = {0}; 
+    daos_anchor_t       fetch_anchor = {0}; 
+    daos_epoch_range_t  eprs[5] = {0};
+    daos_recx_t         recxs[5] = {0};
+    daos_size_t         size = 0;
+    char                attr_name[ATTR_NAME_LEN] = {0};
+    char                number_str[ATTR_NAME_LEN] = {0};
+    char                attr_num_str[ATTR_NAME_LEN] = {0};
+    unsigned char       *encode_buf = NULL;
+    d_sg_list_t         sgl = {0};
+    d_iov_t             iov = {0};
+    hid_t               status = 0;
+    char                *buf = NULL;
+
+    hdf5->rx_dset = 0;
+    hdf5->selection_attr = 0;
+    hdf5->rx_memspace = 0;
+    hdf5->rx_dtype = 0;
+
+    /* need to do a fetch for size, so that we can
+     * create the dataset with the correct datatype size */
+    number = 1;
+    rc = daos_obj_list_recx(*oh, DAOS_TX_NONE, dkey,
+                            akey, &size, &number, NULL, eprs, &fetch_anchor,
+                            true, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to list recx "DF_RC, DP_RC(rc));
+        goto out;
+    }
+    if (number == 0) {
+        rc = 0;
+        goto out;
+    }
+
+    if (size > 2000) {
+        MFU_LOG(MFU_LOG_ERR, "recx size is too large: %llu", size);
+        rc = 1;
+        goto out;
+    }
+
+    /* create the dataset with the correct type size */
+    hdf5->rx_dtype = H5Tcreate(H5T_OPAQUE, size);
+    if (hdf5->rx_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create rx_dtype");
+        rc = 1;
+        goto out;
+    }
+
+    hdf5->rx_dset = H5Dcreate(hdf5->file,
+                              rec_name,
+                              hdf5->rx_dtype,
+                              hdf5->rx_dspace,
+                              H5P_DEFAULT,
+                              hdf5->plist,
+                              H5P_DEFAULT);
+    if (hdf5->rx_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create rx_dset");
+        rc = 1;
+        goto out;
+    }
+    size = 0;
+
+    hdf5->rx_dims[0] = 0;
+    while (!daos_anchor_is_eof(&recx_anchor)) {
+        memset(recxs, 0, sizeof(recxs));
+        memset(eprs, 0, sizeof(eprs));
+
+        /* list all recx for this dkey/akey */
+        number = 5;
+        rc = daos_obj_list_recx(*oh, DAOS_TX_NONE, dkey,
+                                akey, &size, &number, recxs, eprs, &recx_anchor,
+                                true, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to list recx "DF_RC, DP_RC(rc));
+            goto out;
+        }
+
+        /* if no recx is returned for this dkey/akey move on */
+        if (number == 0) 
+            continue;
+        for (i = 0; i < number; i++) {
+            buf_len = recxs[i].rx_nr * size;
+            buf = MFU_CALLOC(buf_len, 1);
+
+            memset(&sgl, 0, sizeof(sgl));
+            memset(&iov, 0, sizeof(iov));
+
+            /* set iod values */
+            (*iod).iod_type  = DAOS_IOD_ARRAY;
+            (*iod).iod_size  = size;
+            (*iod).iod_nr    = 1;
+            (*iod).iod_recxs = &recxs[i];
+
+            /* set sgl values */
+            sgl.sg_nr     = 1;
+            sgl.sg_nr_out = 0;
+            sgl.sg_iovs   = &iov;
+
+            d_iov_set(&iov, buf, buf_len);  
+            /* fetch recx values from source */
+            rc = daos_obj_fetch(*oh, DAOS_TX_NONE, 0, dkey, 1, iod,
+                                &sgl, NULL, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to fetch object "DF_RC, DP_RC(rc));
+                goto out;
+            }
+
+            /* Sanity check */
+            if (sgl.sg_nr_out != 1) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to fetch array recxs.");
+                rc = 1;
+                goto out;
+            }
+
+            stats->bytes_read += buf_len;
+
+            /* write data to record dset */
+            hdf5->mem_dims[0] = recxs[i].rx_nr;
+            hdf5->rx_memspace = H5Screate_simple(1, hdf5->mem_dims,
+                                                 hdf5->mem_dims);
+            if (hdf5->rx_memspace < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to create rx_memspace");
+                rc = 1;
+                goto out;
+            }
+            /* extend dataset */
+            hdf5->rx_dims[0] += recxs[i].rx_nr;
+            status = H5Dset_extent(hdf5->rx_dset, hdf5->rx_dims);
+            if (status < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to extend rx_dset");
+                rc = 1;
+                goto out;
+            }
+            /* retrieve extended dataspace */
+            hdf5->rx_dspace = H5Dget_space(hdf5->rx_dset);
+            if (hdf5->rx_dspace < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to get rx_dspace");
+                rc = 1;
+                goto out;
+            }
+            hsize_t start = (hsize_t)recxs[i].rx_idx;
+            hsize_t count = (hsize_t)recxs[i].rx_nr;
+            status = H5Sselect_hyperslab(hdf5->rx_dspace,
+                                         H5S_SELECT_AND, &start,
+                                         NULL, &count, NULL);
+            if (status < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to select hyperslab");
+                rc = 1;
+                goto out;
+            }
+
+            status = H5Dwrite(hdf5->rx_dset, hdf5->rx_dtype,
+                              hdf5->rx_memspace, hdf5->rx_dspace,
+                              H5P_DEFAULT, sgl.sg_iovs[0].iov_buf);
+            if (status < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to write rx_dset");
+                rc = 1;
+                goto out;
+            }
+            /* get size of buffer needed
+             * from nalloc
+             */
+            status = H5Sencode1(hdf5->rx_dspace, NULL, &nalloc);
+            if (status < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to get size of buffer needed");
+                rc = 1;
+                goto out;
+            }
+            /* encode dataspace description
+             * in buffer then store in
+             * attribute on dataset
+             */
+            encode_buf = MFU_CALLOC(nalloc, sizeof(unsigned char));
+            if (encode_buf == NULL) {
+                rc = ENOMEM;
+                goto out;
+            }
+            status = H5Sencode1(hdf5->rx_dspace, encode_buf,
+                                &nalloc);
+            if (status < 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to encode dataspace");
+                rc = 1;
+                goto out;
+            }
+            /* created attribute in HDF5 file with encoded
+             * dataspace for this record extent */
+            path_len = snprintf(number_str, ATTR_NAME_LEN, "%lu",
+                                (*ak_index));
+            if (path_len >= ATTR_NAME_LEN) {
+                MFU_LOG(MFU_LOG_ERR, "number_str is too long");
+                rc = 1;
+                goto out;
+            }
+            path_len = snprintf(attr_num_str, ATTR_NAME_LEN, "-%d", attr_num);
+            if (path_len >= ATTR_NAME_LEN) {
+                MFU_LOG(MFU_LOG_ERR, "attr number str is too long");
+                rc = 1;
+                goto out;
+            }
+            path_len = snprintf(attr_name, ATTR_NAME_LEN, "%s%lu%d", "A-",
+                                *ak_index, attr_num);
+            if (path_len >= ATTR_NAME_LEN) {
+                MFU_LOG(MFU_LOG_ERR, "attr name is too long");
+                rc = 1;
+                goto out;
+            }
+            hdf5->attr_dims[0] = 1;
+            hdf5->attr_dspace = H5Screate_simple(1, hdf5->attr_dims, NULL);
+            if (hdf5->attr_dspace < 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to create attr");
+                rc = 1;
+                goto out;
+            }
+            hdf5->attr_dtype = H5Tcreate(H5T_OPAQUE, nalloc);
+            if (hdf5->attr_dtype < 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to create attr dtype");
+                rc = 1;
+                goto out;
+            }
+            hdf5->selection_attr = H5Acreate2(hdf5->rx_dset,
+                                              attr_name,
+                                              hdf5->attr_dtype,
+                                              hdf5->attr_dspace,
+                                              H5P_DEFAULT,
+                                              H5P_DEFAULT);
+            if (hdf5->selection_attr < 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to create selection attr");
+                rc = 1;
+                goto out;
+            }
+            status = H5Awrite(hdf5->selection_attr, hdf5->attr_dtype,
+                              encode_buf);
+            if (status < 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to write attr");
+                rc = 1;
+                goto out;
+            }
+            if (hdf5->selection_attr > 0) {
+                H5Aclose(hdf5->selection_attr);
+            }
+            if (hdf5->rx_memspace > 0) {
+                H5Sclose(hdf5->rx_memspace);
+            }
+            if (hdf5->attr_dtype > 0) {
+                H5Tclose(hdf5->attr_dtype);
+            }
+            mfu_free(&encode_buf);
+            mfu_free(&buf);
+            attr_num++;
+        }
+    }
+out:
+    if (hdf5->rx_dset > 0) {
+        H5Dclose(hdf5->rx_dset);
+    }
+    if (hdf5->rx_dtype > 0) {
+        H5Tclose(hdf5->rx_dtype);
+    }
+    if (rc != 0) {
+        if (hdf5->selection_attr > 0) {
+            H5Aclose(hdf5->selection_attr);
+        }
+        if (hdf5->rx_memspace > 0) {
+            H5Sclose(hdf5->rx_memspace);
+        }
+        mfu_free(&encode_buf);
+    }
+    return rc;
+}
+
+static int init_recx_data(struct hdf5_args *hdf5)
+{
+    int     rc = 0;
+    herr_t  err = 0;
+
+    hdf5->rx_dims[0] = 0;
+    hdf5->rx_max_dims[0] = H5S_UNLIMITED;
+
+    /* TODO consider other chunk sizes or possibly use different
+     * chunk sizes for different dkeys/akeys. */
+    hdf5->rx_chunk_dims[0] = 1024;
+
+    hdf5->plist = H5Pcreate(H5P_DATASET_CREATE);
+    if (hdf5->plist < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create prop list");
+        rc = 1;
+        goto out;
+    }
+    hdf5->rx_dspace = H5Screate_simple(1, hdf5->rx_dims, hdf5->rx_max_dims);
+    if (hdf5->rx_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create rx_dspace");
+        rc = 1;
+        goto out;
+    }
+    err = H5Pset_layout(hdf5->plist, H5D_CHUNKED);
+    if (err < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set property layout");
+        rc = 1;
+        goto out;
     }
+    err = H5Pset_chunk(hdf5->plist, 1, hdf5->rx_chunk_dims);
+    if (err < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set chunk size");
+        rc = 1;
+        goto out;
+    }
+out:
+    if (rc != 0) {
+        H5Tclose(hdf5->usr_attr_memtype);
+        H5Tclose(hdf5->usr_attr_name_vtype);
+        H5Tclose(hdf5->usr_attr_val_vtype);
+        H5Tclose(hdf5->dkey_memtype);
+        H5Tclose(hdf5->dkey_vtype);
+        H5Tclose(hdf5->akey_memtype);
+        H5Tclose(hdf5->akey_vtype);
+    }
+    return rc;
+}
+
+static int serialize_akeys(struct hdf5_args *hdf5,
+                           daos_key_t diov,
+                           uint64_t *dk_index,
+                           uint64_t *ak_index,
+                           daos_handle_t *oh,
+                           mfu_daos_stats_t *stats)
+{
+    int             rc = 0;
+    int             j = 0;
+    daos_anchor_t   akey_anchor = {0}; 
+    d_sg_list_t     akey_sgl;
+    d_iov_t         akey_iov;
+    daos_key_desc_t akey_kds[ENUM_DESC_NR] = {0};
+    uint32_t        akey_number = ENUM_DESC_NR;
+    char            akey_enum_buf[ENUM_DESC_BUF] = {0};
+    char            akey[ENUM_KEY_BUF] = {0};
+    char            *akey_ptr = NULL;
+    daos_key_t      aiov = {0};
+    daos_iod_t      iod = {0};
+    size_t          rec_name_len = 32;
+    char            rec_name[rec_name_len];
+    int             path_len = 0;
+    int             size = 0;
+    hvl_t           *akey_val;
+
+    (*hdf5->dk)[*dk_index].akey_offset = *ak_index;
+    while (!daos_anchor_is_eof(&akey_anchor)) {
+        memset(akey_kds, 0, sizeof(akey_kds));
+        memset(akey, 0, sizeof(akey));
+        memset(akey_enum_buf, 0, sizeof(akey_enum_buf));
+        akey_number = ENUM_DESC_NR;
+
+        akey_sgl.sg_nr     = 1;
+        akey_sgl.sg_nr_out = 0;
+        akey_sgl.sg_iovs   = &akey_iov;
+
+        d_iov_set(&akey_iov, akey_enum_buf, ENUM_DESC_BUF);
+
+        /* get akeys */
+        rc = daos_obj_list_akey(*oh, DAOS_TX_NONE, &diov,
+                                &akey_number, akey_kds,
+                                &akey_sgl, &akey_anchor, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to list akeys: %d", rc);
+            goto out;
+        }
+
+        /* if no akeys returned move on */
+        if (akey_number == 0)
+            continue;
+
+        size = (akey_number + stats->total_akeys) * sizeof(akey_t);
+        *hdf5->ak = realloc(*hdf5->ak, size);
+        if (*hdf5->ak == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+
+        /* parse out individual akeys based on key length and
+         * numver of dkeys returned
+         */
+        for (akey_ptr = akey_enum_buf, j = 0; j < akey_number; j++) {
+            memcpy(akey, akey_ptr, akey_kds[j].kd_key_len);
+            memset(&aiov, 0, sizeof(aiov));
+            d_iov_set(&aiov, (void*)akey,
+                      akey_kds[j].kd_key_len);
+            akey_val = &(*hdf5->ak)[*ak_index].akey_val;
+            akey_val->p = MFU_CALLOC((uint64_t)akey_kds[j].kd_key_len, sizeof(char));
+            if (akey_val->p == NULL) {
+                rc = ENOMEM;
+                goto out;
+            }
+            memcpy(akey_val->p, (void*)akey_ptr,
+                   (uint64_t)akey_kds[j].kd_key_len);
+            akey_val->len = (uint64_t)akey_kds[j].kd_key_len; 
+            (*hdf5->ak)[*ak_index].rec_dset_id = *ak_index;
+
+            /* set iod values */
+            iod.iod_nr   = 1;
+            iod.iod_type = DAOS_IOD_SINGLE;
+            iod.iod_size = DAOS_REC_ANY;
+            iod.iod_recxs = NULL;
+            iod.iod_name  = aiov;
+
+            /* do a fetch (with NULL sgl) of single value type,
+            * and if that returns iod_size == 0, then a single
+            * value does not exist.
+            */
+            rc = daos_obj_fetch(*oh, DAOS_TX_NONE, 0, &diov,
+                                1, &iod, NULL, NULL, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to fetch object");
+                rc = 1;
+                goto out;
+            }
+
+            /* if iod_size == 0 then this is a
+             * DAOS_IOD_ARRAY type
+             */
+            if ((int)iod.iod_size == 0) {
+                /* set single value field to NULL, 0 for array types */
+                (*hdf5->ak)[*ak_index].rec_single_val.p = NULL;
+                (*hdf5->ak)[*ak_index].rec_single_val.len = 0;
+
+                /* create a record dset only for array types */
+                memset(&rec_name, 0, rec_name_len);
+                path_len = snprintf(rec_name, rec_name_len, "%lu", *ak_index);
+                if (path_len > FILENAME_LEN) {
+                    MFU_LOG(MFU_LOG_ERR, "record name too long");
+                    rc = 1;
+                    goto out;
+                }
 
+                rc = serialize_recx_array(hdf5, &diov, &aiov, rec_name,
+                                          ak_index, oh, &iod, stats);
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "failed to serialize recx array: %d",
+                            rc);
+                    goto out;
+                }
+            } else {
+                rc = serialize_recx_single(hdf5, &diov, oh,
+                                           &iod, ak_index, stats);
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "failed to serialize recx single: %d",
+                            rc);
+                    goto out;
+                }
+            }
+            /* advance to next akey returned */ 
+            akey_ptr += akey_kds[j].kd_key_len;
+            (*ak_index)++;
+        }
+        stats->total_akeys += akey_number;
+    }
+out:
     return rc;
 }
+
+static int serialize_dkeys(struct hdf5_args *hdf5,
+                           uint64_t *dk_index,
+                           uint64_t *ak_index,
+                           daos_handle_t *oh,
+                           int *oid_index,
+                           daos_args_t *da,
+                           daos_obj_id_t oid,
+                           bool is_kv,
+                           mfu_daos_stats_t* stats)
+{
+    int             rc = 0;
+    herr_t          err = 0;
+    int             i = 0;
+    daos_anchor_t   dkey_anchor = {0}; 
+    d_sg_list_t     dkey_sgl;
+    d_iov_t         dkey_iov;
+    daos_key_desc_t dkey_kds[ENUM_DESC_NR] = {0};
+    uint32_t        dkey_number = ENUM_DESC_NR;
+    char            dkey_enum_buf[ENUM_DESC_BUF] = {0};
+    char            dkey[ENUM_KEY_BUF] = {0};
+    char            *dkey_ptr;
+    daos_key_t      diov;
+    int             path_len = 0;
+    hvl_t           *dkey_val;
+
+    rc = init_recx_data(hdf5);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to initialize recx data: %d", rc);
+        rc = 1;
+        goto out;
+    }
+    (*hdf5->oid)[*oid_index].dkey_offset = *dk_index;
+    while (!daos_anchor_is_eof(&dkey_anchor)) {
+        memset(dkey_kds, 0, sizeof(dkey_kds));
+        memset(dkey, 0, sizeof(dkey));
+        memset(dkey_enum_buf, 0, sizeof(dkey_enum_buf));
+        dkey_number = ENUM_DESC_NR;
+
+        dkey_sgl.sg_nr     = 1;
+        dkey_sgl.sg_nr_out = 0;
+        dkey_sgl.sg_iovs   = &dkey_iov;
+
+        d_iov_set(&dkey_iov, dkey_enum_buf, ENUM_DESC_BUF);
+
+        if (is_kv) {
+            rc = daos_kv_list(*oh, DAOS_TX_NONE, &dkey_number,
+                              dkey_kds, &dkey_sgl, &dkey_anchor, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to list dkeys: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        } else {
+            rc = daos_obj_list_dkey(*oh, DAOS_TX_NONE, &dkey_number,
+                                    dkey_kds, &dkey_sgl, &dkey_anchor, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to list dkeys: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        }
+
+        /* if no dkeys were returned move on */
+        if (dkey_number == 0)
+            continue;
+        *hdf5->dk = realloc(*hdf5->dk,
+                            (dkey_number + stats->total_dkeys) * sizeof(dkey_t));
+        if (*hdf5->dk == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+
+        /* parse out individual dkeys based on key length and
+         * number of dkeys returned
+         */
+        for (dkey_ptr = dkey_enum_buf, i = 0; i < dkey_number; i++) {
+            /* Print enumerated dkeys */
+            memcpy(dkey, dkey_ptr, dkey_kds[i].kd_key_len);
+            memset(&diov, 0, sizeof(diov));
+            d_iov_set(&diov, (void*)dkey, dkey_kds[i].kd_key_len);
+            dkey_val = &(*hdf5->dk)[*dk_index].dkey_val;
+            dkey_val->p = MFU_CALLOC((uint64_t)dkey_kds[i].kd_key_len, sizeof(char));
+            if (dkey_val->p == NULL) {
+                rc = ENOMEM;
+                goto out;
+            }
+            memcpy(dkey_val->p, (void*)dkey_ptr,
+                   (uint64_t)dkey_kds[i].kd_key_len);
+            dkey_val->len = (uint64_t)dkey_kds[i].kd_key_len; 
+            (*hdf5->dk)[*dk_index].rec_kv_val.p = NULL;
+            (*hdf5->dk)[*dk_index].rec_kv_val.len = 0;
+            if (is_kv) {
+                /* akey offset will not be used in this case */
+                (*hdf5->dk)[*dk_index].akey_offset = 0;
+
+                /** create the KV store */
+                rc = daos_kv_open(da->src_coh, oid, DAOS_OO_RW, oh, NULL);
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Failed to open kv object: "DF_RC, DP_RC(rc));
+                    rc = 1;
+                    goto out;
+                }
+
+                /* daos_kv_get takes a char *key */
+                char key_val[ENUM_LARGE_KEY_BUF];
+                path_len = snprintf(key_val, ENUM_LARGE_KEY_BUF, "%s", dkey_val->p);
+                if (path_len > ENUM_LARGE_KEY_BUF) {
+                    rc = 1;
+                    goto out;
+                }
+
+                /* TODO: serialize the array that was read */
+                rc = serialize_kv_rec(hdf5, diov, oh, dk_index, key_val, stats);
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Failed to serialize kv record: "DF_RC, DP_RC(rc));
+                    rc = 1;
+                    goto out;
+                }
+            } else {
+                rc = serialize_akeys(hdf5, diov, dk_index, ak_index,
+                                     oh, stats); 
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "failed to list akeys: %d", rc);
+                    rc = 1;
+                    goto out;
+                }
+            }
+            dkey_ptr += dkey_kds[i].kd_key_len;
+            (*dk_index)++;
+        }
+        stats->total_dkeys += dkey_number;
+    }
+    err = H5Sclose(hdf5->rx_dspace);
+    if (err < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close rx_dspace");
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+static int init_hdf5_file(struct hdf5_args *hdf5, char *filename) {
+    int rc = 0;
+    hdf5->file = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT,
+                           H5P_DEFAULT);
+    if (hdf5->file < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create hdf5 file");
+        rc = 1;
+        goto out;
+    }
+
+    /* Set user attribute dataset types */
+    hdf5->usr_attr_memtype = H5Tcreate(H5T_COMPOUND, sizeof(usr_attr_t));
+    if (hdf5->usr_attr_memtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr memtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->usr_attr_name_vtype = H5Tcopy(H5T_C_S1);
+    if (hdf5->usr_attr_name_vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr name vtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tset_size(hdf5->usr_attr_name_vtype, H5T_VARIABLE);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr name vtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->usr_attr_val_vtype = H5Tvlen_create(H5T_NATIVE_OPAQUE);
+    if (hdf5->usr_attr_val_vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr val vtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->usr_attr_memtype, "Attribute Name",
+                             HOFFSET(usr_attr_t, attr_name), hdf5->usr_attr_name_vtype);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert user attr name");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->usr_attr_memtype, "Attribute Value",
+                             HOFFSET(usr_attr_t, attr_val), hdf5->usr_attr_val_vtype);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert user attr val");
+        rc = 1;
+        goto out;
+    }
+
+    /* Set oid dataset types */
+    hdf5->oid_memtype = H5Tcreate(H5T_COMPOUND, sizeof(oid_t));
+    if (hdf5->oid_memtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create oid memtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->oid_memtype, "OID Hi",
+                             HOFFSET(oid_t, oid_hi), H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert oid hi");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->oid_memtype, "OID Low",
+                             HOFFSET(oid_t, oid_low), H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert oid low");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->oid_memtype, "Dkey Offset",
+                             HOFFSET(oid_t, dkey_offset), H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert dkey offset");
+        rc = 1;
+        goto out;
+    }
+
+    /* Set dkey dataset types */
+    hdf5->dkey_memtype = H5Tcreate(H5T_COMPOUND, sizeof(dkey_t));
+    if (hdf5->dkey_memtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create dkey memtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->dkey_vtype = H5Tvlen_create(H5T_NATIVE_OPAQUE);
+    if (hdf5->dkey_vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create dkey vtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->dkey_memtype, "Akey Offset",
+                             HOFFSET(dkey_t, akey_offset),
+                             H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert akey offset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->dkey_memtype, "Dkey Value",
+                             HOFFSET(dkey_t, dkey_val), hdf5->dkey_vtype);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert dkey value");
+        rc = 1;
+        goto out;
+    }
+
+    hdf5->status = H5Tinsert(hdf5->dkey_memtype, "Record KV Value",
+                             HOFFSET(dkey_t, rec_kv_val),
+                             hdf5->dkey_vtype);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert record KV value");
+        rc = 1;
+        goto out;
+    }
+
+    /* Set akey dataset types */
+    hdf5->akey_memtype = H5Tcreate(H5T_COMPOUND, sizeof(akey_t));
+    if (hdf5->akey_memtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create akey memtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->akey_vtype = H5Tvlen_create(H5T_NATIVE_OPAQUE);
+    if (hdf5->akey_vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create akey vtype");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->akey_memtype, "Dataset ID",
+                             HOFFSET(akey_t, rec_dset_id),
+                             H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert record dset id");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->akey_memtype, "Record Single Value",
+                             HOFFSET(akey_t, rec_single_val),
+                             hdf5->akey_vtype);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert record single value");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Tinsert(hdf5->akey_memtype, "Akey Value",
+                             HOFFSET(akey_t, akey_val), hdf5->akey_vtype);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert akey value");
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+static int cont_serialize_version(struct hdf5_args *hdf5, float version)
+{
+    int     rc = 0;
+    hid_t   status = 0;
+    char    *version_name = "Version";
+
+    hdf5->version_attr_dims[0] = 1;
+    hdf5->version_attr_type = H5Tcopy(H5T_NATIVE_FLOAT);
+    status = H5Tset_size(hdf5->version_attr_type, 4);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version dtype");
+        rc = 1;
+        goto out;
+    }
+    if (hdf5->version_attr_type < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attr type");
+        rc = 1;
+        goto out;
+    }
+    hdf5->version_attr_dspace = H5Screate_simple(1, hdf5->version_attr_dims,
+                                                 NULL);
+    if (hdf5->version_attr_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attr dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->version_attr = H5Acreate2(hdf5->file,
+                                    version_name,
+                                    hdf5->version_attr_type,
+                                    hdf5->version_attr_dspace,
+                                    H5P_DEFAULT,
+                                    H5P_DEFAULT);
+    if (hdf5->version_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attr");
+        rc = 1;
+        goto out;
+    }   
+    status = H5Awrite(hdf5->version_attr, hdf5->version_attr_type,
+                      &version);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write attribute");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(hdf5->version_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute");
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+int cont_serialize_usr_attrs(struct hdf5_args *hdf5, daos_handle_t cont)
+{
+    int         rc = 0;
+    hid_t       status = 0;
+    hid_t       dset = 0;
+    hid_t       dspace = 0;
+    hsize_t     dims[1];
+    usr_attr_t* attr_data = NULL;
+    int         num_attrs = 0;
+    char**      names = NULL;
+    void**      buffers = NULL;
+    size_t*     sizes = NULL;
+
+    /* Get all user attributes */
+    rc = cont_get_usr_attrs(cont, &num_attrs, &names, &buffers, &sizes);
+    if (rc != 0) {
+        rc = 1;
+        goto out;
+    }
+
+    if (num_attrs == 0) {
+        goto out_no_attrs;
+    }
+
+    /* Create the user attribute data space */
+    dims[0] = num_attrs;
+    dspace = H5Screate_simple(1, dims, NULL);
+    if (dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attr dspace");
+        rc = 1;
+        goto out;
+    }
+
+    /* Create the user attribute dataset */
+    dset = H5Dcreate(hdf5->file, "User Attributes",
+                     hdf5->usr_attr_memtype, dspace,
+                     H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
+    if (dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create user attribute dset");
+        rc = 1;
+        goto out;
+    }
+
+    /* Allocate space for all attributes */
+    attr_data = MFU_CALLOC(num_attrs, sizeof(usr_attr_t));
+    if (attr_data == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attributes");
+        rc = 1;
+        goto out;
+    }
+
+    /* Set the data for all attributes */
+    for (int i = 0; i < num_attrs; i++) {
+        attr_data[i].attr_name = names[i];
+        attr_data[i].attr_val.p = buffers[i];
+        attr_data[i].attr_val.len = sizes[i];
+    }
+
+    status = H5Dwrite(dset, hdf5->usr_attr_memtype, H5S_ALL,
+                      H5S_ALL, H5P_DEFAULT, attr_data);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write user attr dset");
+        rc = 1;
+        goto out;
+    }
+
+out:
+    cont_free_usr_attrs(num_attrs, &names, &buffers, &sizes);
+    mfu_free(&attr_data);
+    H5Dclose(dset);
+    H5Tclose(hdf5->usr_attr_memtype);
+    H5Sclose(dspace);
+out_no_attrs:
+    return rc;
+}
+
+static int cont_serialize_prop_roots(struct hdf5_args* hdf5,
+                                     struct daos_prop_entry* entry,
+                                     const char* prop_str)
+{
+    int                         rc = 0;
+    hid_t                       status = 0;
+    struct daos_prop_co_roots   *roots;
+    obj_id_t                    root_oids[4];
+    hsize_t                     attr_dims[1];
+    /* HDF5 returns non-negative identifiers if successfully opened/created */
+    hid_t                       attr_dtype = -1;
+    hid_t                       attr_dspace = -1;
+    hid_t                       usr_attr = -1;
+    int                         i = 0;
+
+    if (entry == NULL || entry->dpe_val_ptr == NULL) {
+        goto out;
+    }
+
+    roots = entry->dpe_val_ptr;
+    attr_dims[0] = 4;
+
+    for (i = 0; i < 4; i++) {
+        root_oids[i].hi = roots->cr_oids[i].hi;
+        root_oids[i].lo = roots->cr_oids[i].lo;
+    }
+
+    attr_dtype = H5Tcreate(H5T_COMPOUND, sizeof(obj_id_t));
+    if (attr_dtype < 0) {
+        rc = 1;
+        MFU_LOG(MFU_LOG_ERR, "failed to create attr dtype");
+        goto out;
+    }
+    status = H5Tinsert(attr_dtype, "hi", HOFFSET(obj_id_t, hi), H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert oid hi");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tinsert(attr_dtype, "lo", HOFFSET(obj_id_t, lo), H5T_NATIVE_UINT64);
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to insert oid low");
+        rc = 1;
+        goto out;
+    }
+
+    attr_dspace = H5Screate_simple(1, attr_dims, NULL);
+    if (attr_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attribute");
+        rc = 1;
+        goto out;
+    }
+    usr_attr = H5Acreate2(hdf5->file, prop_str, attr_dtype, attr_dspace,
+                          H5P_DEFAULT, H5P_DEFAULT);
+    if (usr_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create attribute");
+        rc = 1;
+        goto out;
+    }   
+    status = H5Awrite(usr_attr, attr_dtype, root_oids);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write attribute");
+        rc = 1;
+        goto out;
+    }
+out:
+    if (usr_attr >= 0) {
+        H5Aclose(usr_attr);
+    }
+    if (attr_dtype >= 0) {
+        H5Tclose(attr_dtype);
+    }
+    if (attr_dspace >= 0) {
+        H5Sclose(attr_dspace);
+    }
+    return rc;
+}
+
+static int cont_serialize_prop_acl(struct hdf5_args* hdf5,
+                                   struct daos_prop_entry* entry,
+                                   const char* prop_str)
+{
+    int                 rc = 0;
+    hid_t               status = 0;
+    struct daos_acl     *acl = NULL;
+    char                **acl_strs = NULL;
+    size_t              len_acl = 0;
+    hsize_t             attr_dims[1];
+    hid_t               attr_dtype;
+    hid_t               attr_dspace;
+    hid_t               usr_attr;
+    int                 i = 0;
+
+
+    if (entry == NULL || entry->dpe_val_ptr == NULL) {
+        goto out;
+    }
+
+    /* convert acl to list of strings */
+    acl = (struct daos_acl *)entry->dpe_val_ptr;                         
+    rc = daos_acl_to_strs(acl, &acl_strs, &len_acl);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to convert acl to strs");
+        goto out;
+    }
+    attr_dims[0] = len_acl;
+    attr_dtype = H5Tcopy(H5T_C_S1);
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create acl type");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tset_size(attr_dtype, H5T_VARIABLE);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set acl dtype size");
+        rc = 1;
+        goto out;
+    }
+    attr_dspace = H5Screate_simple(1, attr_dims, NULL);
+    if (attr_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attribute");
+        rc = 1;
+        goto out;
+    }
+    usr_attr = H5Acreate2(hdf5->file, prop_str, attr_dtype, attr_dspace,
+                                H5P_DEFAULT, H5P_DEFAULT);
+    if (usr_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create attribute");
+        rc = 1;
+        goto out;
+    }   
+    status = H5Awrite(usr_attr, attr_dtype, acl_strs);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write attribute");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(usr_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tclose(attr_dtype);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close dtype");
+        rc = 1;
+        goto out;
+    }
+    status = H5Sclose(attr_dspace);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close dspace");
+        rc = 1;
+        goto out;
+    }
+out:
+    for (i = 0; i < len_acl; i++) {
+        mfu_free(&acl_strs[i]);
+    }
+    mfu_free(&acl_strs);
+    return rc;
+}
+
+static int cont_serialize_prop_str(struct hdf5_args* hdf5,
+                                   struct daos_prop_entry* entry,
+                                   const char* prop_str)
+{
+    int     rc = 0;
+    hid_t   status = 0;
+    hsize_t attr_dims[1];
+    hid_t   attr_dtype;
+    hid_t   attr_dspace;
+    hid_t   usr_attr;
+
+    if (entry == NULL || entry->dpe_str == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "Property %s not found", prop_str);
+        rc = 1;
+        goto out;
+    }
+
+    attr_dims[0] = 1;
+    attr_dtype = H5Tcopy(H5T_C_S1);
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create usr attr type");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tset_size(attr_dtype, strlen(entry->dpe_str) + 1);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set dtype size");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tset_strpad(attr_dtype, H5T_STR_NULLTERM);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set null terminator");
+        rc = 1;
+        goto out;
+    }
+    attr_dspace = H5Screate_simple(1, attr_dims, NULL);
+    if (attr_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attribute dataspace");
+        rc = 1;
+        goto out;
+    }
+    usr_attr = H5Acreate2(hdf5->file, prop_str, attr_dtype, attr_dspace,
+                          H5P_DEFAULT, H5P_DEFAULT);
+    if (usr_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create attribute");
+        rc = 1;
+        goto out;
+    }   
+    status = H5Awrite(usr_attr, attr_dtype, entry->dpe_str);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write attribute");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(usr_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tclose(attr_dtype);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close dtype");
+        rc = 1;
+        goto out;
+    }
+    status = H5Sclose(attr_dspace);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close dspace");
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+int daos_cont_serialize_files_generated(struct hdf5_args *hdf5,
+                                        uint64_t *files_generated)
+{
+    int     rc = 0;
+    hid_t   status = 0;
+
+    hdf5->attr_dims[0] = 1;
+    hdf5->attr_dtype = H5Tcopy(H5T_NATIVE_UINT64);
+    status = H5Tset_size(hdf5->attr_dtype, 8);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version dtype");
+        rc = 1;
+        goto out;
+    }
+    if (hdf5->attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create usr attr type");
+        rc = 1;
+        goto out;
+    }
+    hdf5->attr_dspace = H5Screate_simple(1, hdf5->attr_dims,
+                                         NULL);
+    if (hdf5->attr_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attr dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->usr_attr = H5Acreate2(hdf5->file,
+                                "Files Generated",
+                                hdf5->attr_dtype,
+                                hdf5->attr_dspace,
+                                H5P_DEFAULT,
+                                H5P_DEFAULT);
+    if (hdf5->usr_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create attr");
+        rc = 1;
+        goto out;
+    }   
+    status = H5Awrite(hdf5->usr_attr, hdf5->attr_dtype,
+                      files_generated);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write attr");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(hdf5->usr_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attr");
+        rc = 1;
+        goto out;
+    }
+out:
+    if (hdf5->file > 0) {
+        H5Fclose(hdf5->file);
+    }
+    return rc;
+}
+
+static int cont_serialize_prop_uint(struct hdf5_args *hdf5,
+                                    struct daos_prop_entry* entry,
+                                    const char *prop_str)
+{
+    int     rc = 0;
+    hid_t   status = 0;
+    hsize_t attr_dims[1];
+    hid_t   attr_dtype;
+    hid_t   attr_dspace;
+    hid_t   usr_attr;
+
+
+    if (entry == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "Property %s not found", prop_str);
+        rc = 1;
+        goto out;
+    }
+
+    attr_dims[0] = 1;
+    attr_dtype = H5Tcopy(H5T_NATIVE_UINT64);
+    status = H5Tset_size(attr_dtype, 8);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version dtype");
+        rc = 1;
+        goto out;
+    }
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create usr attr type");
+        rc = 1;
+        goto out;
+    }
+    attr_dspace = H5Screate_simple(1, attr_dims, NULL);
+    if (attr_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create version attr dspace");
+        rc = 1;
+        goto out;
+    }
+    usr_attr = H5Acreate2(hdf5->file, prop_str, attr_dtype,
+                          attr_dspace, H5P_DEFAULT, H5P_DEFAULT);
+    if (usr_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create attr");
+        rc = 1;
+        goto out;
+    }   
+    status = H5Awrite(usr_attr, attr_dtype, &entry->dpe_val);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write attr");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(usr_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attr");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tclose(attr_dtype);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close dtype");
+        rc = 1;
+        goto out;
+    }
+    status = H5Sclose(attr_dspace);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close dspace");
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+int cont_serialize_props(struct hdf5_args *hdf5,
+                         daos_handle_t cont)
+{
+    int                     rc = 0;
+    daos_prop_t*            prop_query = NULL;
+    struct daos_prop_entry* entry;
+
+    rc = cont_get_props(cont, &prop_query, true, true, true);
+    if (rc != 0) {
+        rc = 1;
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[0];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_LAYOUT_TYPE");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[1];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_LAYOUT_VER");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[2];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_CSUM");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[3];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_CSUM_CHUNK_SIZE");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[4];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_CSUM_SERVER_VERIFY");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[5];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_REDUN_FAC");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[6];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_REDUN_LVL");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[7];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_SNAPSHOT_MAX");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[8];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_COMPRESS");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[9];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_ENCRYPT");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[10];
+    rc = cont_serialize_prop_str(hdf5, entry, "DAOS_PROP_CO_OWNER");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[11];
+    rc = cont_serialize_prop_str(hdf5, entry, "DAOS_PROP_CO_OWNER_GROUP");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[12];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_DEDUP");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[13];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_DEDUP_THRESHOLD");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[14];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_EC_CELL_SZ");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[15];
+    rc = cont_serialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_ALLOCED_OID");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[16];
+    rc = cont_serialize_prop_str(hdf5, entry, "DAOS_PROP_CO_LABEL");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop_query->dpp_entries[17];
+    rc = cont_serialize_prop_roots(hdf5, entry, "DAOS_PROP_CO_ROOTS");
+    if (rc != 0) {
+        goto out;
+    }
+
+    /* serialize ACL */
+    if (prop_query->dpp_nr > 18) {
+        entry = &prop_query->dpp_entries[18];
+        rc = cont_serialize_prop_acl(hdf5, entry, "DAOS_PROP_CO_ACL");
+        if (rc != 0) {
+            goto out;
+        }
+    }
+
+out:
+    daos_prop_free(prop_query);
+    return rc;
+}
+
+static bool obj_is_kv(daos_obj_id_t oid)
+{
+
+#if CHECK_DAOS_API_VERSION(2, 0)
+	return daos_obj_id2type(oid) == DAOS_OT_KV_HASHED;
+#else
+	daos_ofeat_t ofeat;
+
+        ofeat = (oid.hi & OID_FMT_FEAT_MASK) >> OID_FMT_FEAT_SHIFT;
+        if ((ofeat & DAOS_OF_KV_FLAT) &&
+            !(ofeat & DAOS_OF_ARRAY_BYTE) && !(ofeat & DAOS_OF_ARRAY)) {
+		return true;
+        }
+	return false;
+#endif
+}
+
+int daos_cont_serialize_hdlr(int rank, struct hdf5_args *hdf5, char *output_dir,
+                             uint64_t *files_written, daos_args_t *da,
+                             mfu_flist flist, uint64_t num_oids,
+                             mfu_daos_stats_t* stats)
+{
+    int             rc = 0;
+    int             i = 0;
+    uint64_t        dk_index = 0;
+    uint64_t        ak_index = 0;
+    daos_handle_t   oh;
+    float           version = 0.0;
+    char            *filename = NULL;
+    char            cont_str[FILENAME_LEN];
+    bool            is_kv = false;
+    int             size = 0;
+
+    /* init HDF5 args */
+    init_hdf5_args(hdf5);
+
+    uuid_unparse(da->src_cont, cont_str);
+
+    size = asprintf(&filename, "%s/%s%s%s%d%s", output_dir, cont_str, "_", "rank", rank, ".h5");
+    if (size == -1) {
+        rc = 1;
+        goto out;
+    }
+
+    printf("Serializing Container to %s\n", filename);
+
+    /* keep track of number of files written */
+    (*files_written)++;
+
+    /* init HDF5 datatypes in HDF5 file */
+    rc = init_hdf5_file(hdf5, filename);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to init hdf5 file");
+        rc = 1;
+        goto out;
+    }
+    hdf5->oid_data = MFU_CALLOC(num_oids, sizeof(oid_t));
+    if (hdf5->oid_data == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    hdf5->dkey_data = MFU_CALLOC(1, sizeof(dkey_t));
+    if (hdf5->dkey_data == NULL) {
+        rc = ENOMEM;
+        mfu_free(&hdf5->oid_data);
+        goto out;
+    }
+    hdf5->akey_data = MFU_CALLOC(1, sizeof(akey_t));
+    if (hdf5->akey_data == NULL) {
+        rc = ENOMEM;
+        mfu_free(&hdf5->oid_data);
+        mfu_free(&hdf5->dkey_data);
+        goto out;
+    }
+    hdf5->dk = &(hdf5->dkey_data);
+    hdf5->ak = &(hdf5->akey_data);
+    hdf5->oid = &(hdf5->oid_data);
+
+    /* size is total oids for this rank, loop over each oid and serialize */
+    for (i = 0; i < num_oids; i++) {
+        /* open DAOS object based on oid to get obj
+         * handle
+         */
+        daos_obj_id_t oid;
+        oid.hi = mfu_flist_file_get_oid_high(flist, i);
+        oid.lo = mfu_flist_file_get_oid_low(flist, i);
+        (*hdf5->oid)[i].oid_hi = oid.hi;
+        (*hdf5->oid)[i].oid_low = oid.lo;
+
+	is_kv = obj_is_kv(oid);
+        /* TODO: DAOS_OF_KV_FLAT uses daos_kv_* functions, and
+         * other object types use daos_obj_* functions. Maybe there is
+         * a better way to organize this with swtich statements, or
+         * creating "daos_obj" wrappers, etc. */
+
+        if (is_kv) {
+            rc = daos_kv_open(da->src_coh, oid, DAOS_OO_RW, &oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to open kv object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+            rc = serialize_dkeys(hdf5, &dk_index, &ak_index,
+                                 &oh, &i, da, oid, is_kv, stats);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to serialize keys: %d", rc);
+                goto out;
+            }
+            /* close source and destination object */
+            rc = daos_kv_close(oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to close kv object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        } else {
+            rc = daos_obj_open(da->src_coh, oid, 0, &oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to open object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+            rc = serialize_dkeys(hdf5, &dk_index, &ak_index,
+                                 &oh, &i, da, oid, is_kv, stats);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to serialize keys: %d", rc);
+                goto out;
+            }
+            /* close source and destination object */
+            rc = daos_obj_close(oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to close object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        }
+        /* Increment as we go */
+        stats->total_oids++;
+    }
+
+    /* write container version as attribute */
+    rc = cont_serialize_version(hdf5, version);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to serialize version");
+        goto out;
+    }
+
+    rc = cont_serialize_usr_attrs(hdf5, da->src_coh);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to serialize user attributes");
+        goto out;
+    }
+
+    rc = cont_serialize_props(hdf5, da->src_coh);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to serialize cont layout");
+        goto out;
+    }
+
+    hdf5->oid_dims[0] = num_oids;
+    hdf5->oid_dspace = H5Screate_simple(1, hdf5->oid_dims, NULL);
+    if (hdf5->oid_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create oid dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->oid_dset = H5Dcreate(hdf5->file, "Oid Data",
+                              hdf5->oid_memtype, hdf5->oid_dspace,
+                              H5P_DEFAULT, H5P_DEFAULT,
+                              H5P_DEFAULT);
+    if (hdf5->oid_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create oid dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->dkey_dims[0] = stats->total_dkeys;     
+    hdf5->dkey_dspace = H5Screate_simple(1, hdf5->dkey_dims, NULL);
+    if (hdf5->dkey_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create dkey dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->dkey_dset = H5Dcreate(hdf5->file, "Dkey Data",
+                                hdf5->dkey_memtype, hdf5->dkey_dspace,
+                                H5P_DEFAULT, H5P_DEFAULT,
+                                H5P_DEFAULT);
+    if (hdf5->dkey_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create dkey dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->akey_dims[0] = stats->total_akeys;     
+    hdf5->akey_dspace = H5Screate_simple(1, hdf5->akey_dims, NULL);
+    if (hdf5->akey_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create akey dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->akey_dset = H5Dcreate(hdf5->file, "Akey Data",
+                                hdf5->akey_memtype, hdf5->akey_dspace,
+                                H5P_DEFAULT, H5P_DEFAULT,
+                                H5P_DEFAULT);
+    if (hdf5->akey_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create akey dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Dwrite(hdf5->oid_dset, hdf5->oid_memtype, H5S_ALL,
+                            H5S_ALL, H5P_DEFAULT, *(hdf5->oid));
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write oid dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Dwrite(hdf5->dkey_dset, hdf5->dkey_memtype,
+                           H5S_ALL, H5S_ALL, H5P_DEFAULT,
+                           *(hdf5->dk));
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write dkey dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->status = H5Dwrite(hdf5->akey_dset, hdf5->akey_memtype,
+                           H5S_ALL, H5S_ALL, H5P_DEFAULT,
+                           *(hdf5->ak));
+    if (hdf5->status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to write akey dset");
+        rc = 1;
+        goto out;
+    }
+
+out:
+    /* free dkey, akey values and single record values */
+    for (i = 0; i < stats->total_dkeys; i++) {
+        mfu_free(&((*hdf5->dk)[i].dkey_val.p));
+    }
+    for (i = 0; i < stats->total_akeys; i++) {
+        mfu_free(&((*hdf5->ak)[i].akey_val.p));
+        mfu_free(&((*hdf5->ak)[i].rec_single_val.p));
+    }
+    if (hdf5->oid_dset > 0)
+        H5Dclose(hdf5->oid_dset);
+    if (hdf5->dkey_dset > 0)
+        H5Dclose(hdf5->dkey_dset);
+    if (hdf5->akey_dset > 0)
+        H5Dclose(hdf5->akey_dset);
+    if (hdf5->oid_dspace > 0)
+        H5Sclose(hdf5->oid_dspace);
+    if (hdf5->dkey_dspace > 0)
+        H5Sclose(hdf5->dkey_dspace);
+    if (hdf5->akey_dspace > 0)
+        H5Sclose(hdf5->akey_dspace);
+    if (hdf5->oid_memtype > 0)
+        H5Tclose(hdf5->oid_memtype);
+    if (hdf5->dkey_memtype > 0)
+        H5Tclose(hdf5->dkey_memtype);
+    if (hdf5->akey_memtype > 0)
+        H5Tclose(hdf5->akey_memtype);
+    mfu_free(&hdf5->oid_data);
+    mfu_free(&hdf5->dkey_data);
+    mfu_free(&hdf5->akey_data);
+    mfu_free(&filename);
+    /* dont close file until the files generated is serialized */
+    return rc;
+}
+
+static int hdf5_read_key_data(struct hdf5_args *hdf5)
+{
+    int     rc = 0;
+    hid_t   status = 0;
+    int     oid_ndims = 0;
+    int     dkey_ndims = 0;
+    int     akey_ndims = 0;
+
+    /* read oid data */
+    hdf5->oid_dset = H5Dopen(hdf5->file, "Oid Data", H5P_DEFAULT);
+    if (hdf5->oid_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open OID dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->oid_dspace = H5Dget_space(hdf5->oid_dset);
+    if (hdf5->oid_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get oid dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->oid_dtype = H5Dget_type(hdf5->oid_dset);
+    if (hdf5->oid_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get oid dtype");
+        rc = 1;
+        goto out;
+    }
+    oid_ndims = H5Sget_simple_extent_dims(hdf5->oid_dspace, hdf5->oid_dims,
+                                          NULL);
+    if (oid_ndims < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get oid dimensions");
+        rc = 1;
+        goto out;
+    }
+    hdf5->oid_data = MFU_CALLOC(hdf5->oid_dims[0], sizeof(oid_t));
+    if (hdf5->oid_data == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    status = H5Dread(hdf5->oid_dset, hdf5->oid_dtype, H5S_ALL, H5S_ALL,
+                     H5P_DEFAULT, hdf5->oid_data);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read oid data");
+        rc = 1;
+        goto out;
+    }
+
+    /* read dkey data */
+    hdf5->dkey_dset = H5Dopen(hdf5->file, "Dkey Data", H5P_DEFAULT);
+    if (hdf5->dkey_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open dkey dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->dkey_dspace = H5Dget_space(hdf5->dkey_dset);
+    if (hdf5->dkey_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get dkey dspace");
+        rc = 1;
+        goto out;
+    }
+    hdf5->dkey_vtype = H5Dget_type(hdf5->dkey_dset);
+    if (hdf5->dkey_vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get dkey vtype");
+        rc = 1;
+        goto out;
+    }
+    dkey_ndims = H5Sget_simple_extent_dims(hdf5->dkey_dspace,
+                                           hdf5->dkey_dims, NULL);
+    if (dkey_ndims < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get dkey dimensions");
+        rc = 1;
+        goto out;
+    }
+    hdf5->dkey_data = MFU_CALLOC(hdf5->dkey_dims[0], sizeof(dkey_t));
+    if (hdf5->dkey_data == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    status = H5Dread(hdf5->dkey_dset, hdf5->dkey_vtype, H5S_ALL, H5S_ALL,
+                     H5P_DEFAULT, hdf5->dkey_data);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read dkey data");
+        rc = 1;
+        goto out;
+    }
+
+    /* read akey data */
+    hdf5->akey_dset = H5Dopen(hdf5->file, "Akey Data", H5P_DEFAULT);
+    if (hdf5->akey_dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open akey dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->akey_dspace = H5Dget_space(hdf5->akey_dset);
+    if (hdf5->akey_dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get akey dset");
+        rc = 1;
+        goto out;
+    }
+    hdf5->akey_vtype = H5Dget_type(hdf5->akey_dset);
+    if (hdf5->akey_vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get akey vtype");
+        rc = 1;
+        goto out;
+    }
+    akey_ndims = H5Sget_simple_extent_dims(hdf5->akey_dspace,
+                                           hdf5->akey_dims, NULL);
+    if (akey_ndims < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get akey dimensions");
+        rc = 1;
+        goto out;
+    }
+    if (hdf5->akey_dims[0] > 0) {
+        hdf5->akey_data = MFU_CALLOC(hdf5->akey_dims[0], sizeof(akey_t));
+        if (hdf5->akey_data == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+        status = H5Dread(hdf5->akey_dset, hdf5->akey_vtype, H5S_ALL, H5S_ALL,
+                         H5P_DEFAULT, hdf5->akey_data);
+        if (status < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read akey data");
+            rc = 1;
+            goto out;
+        }
+    }
+out:
+    return rc;
+}
+
+static int cont_deserialize_recx(struct hdf5_args *hdf5,
+                                 daos_handle_t *oh,
+                                 daos_key_t diov,
+                                 int num_attrs,
+                                 uint64_t ak_off,
+                                 int k,
+                                 mfu_daos_stats_t* stats)
+{
+    int             rc = 0;
+    hid_t           status = 0;
+    int             i = 0;
+    ssize_t         attr_len = 0;
+    char            attr_name_buf[124]; 
+    hsize_t         attr_space;
+    hid_t           attr_type;
+    size_t          type_size;
+    size_t          rx_dtype_size;
+    unsigned char   *decode_buf;
+    hid_t           rx_range_id;
+    hsize_t         *rx_range = NULL;
+    uint64_t        recx_len = 0;
+    void            *recx_data = NULL;
+    hssize_t        nblocks = 0;
+    d_sg_list_t     sgl;
+    d_iov_t         iov;
+    daos_iod_t      iod;
+    daos_recx_t     recxs;
+    hid_t           aid;
+
+    for (i = 0; i < num_attrs; i++) {
+        memset(attr_name_buf, 0, sizeof(attr_name_buf));
+        aid = H5Aopen_idx(hdf5->rx_dset, (unsigned int)i);
+        if (aid < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get open attr");
+            rc = 1;
+            goto out;
+        }
+        attr_len = H5Aget_name(aid, 124, attr_name_buf);
+        if (attr_len < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get attr name");
+            rc = 1;
+            goto out;
+        }
+        attr_space = H5Aget_storage_size(aid);
+        if (attr_len < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get attr space");
+            rc = 1;
+            goto out;
+        }
+        attr_type = H5Aget_type(aid);
+        if (attr_type < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get attr type");
+            rc = 1;
+            goto out;
+        }
+        type_size = H5Tget_size(attr_type);
+        if (type_size < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get type size");
+            rc = 1;
+            goto out;
+        }
+        rx_dtype_size = H5Tget_size(hdf5->rx_dtype);
+        if (rx_dtype_size < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get rx type size");
+            rc = 1;
+            goto out;
+        }
+        decode_buf = MFU_CALLOC(1, type_size * attr_space);
+        if (decode_buf == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+        rx_range = MFU_CALLOC(1, type_size * attr_space);
+        if (rx_range == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+        status = H5Aread(aid, attr_type, decode_buf);
+        if (status < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read attribute");
+            rc = 1;
+            goto out;
+        }
+        rx_range_id = H5Sdecode(decode_buf);
+        if (rx_range_id < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to decode attribute buffer");
+            rc = 1;
+            goto out;
+        }
+        nblocks = H5Sget_select_hyper_nblocks(rx_range_id);
+        if (nblocks < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get hyperslab blocks");
+            rc = 1;
+            goto out;
+        }
+        status = H5Sget_select_hyper_blocklist(rx_range_id, 0,
+                                               nblocks, rx_range);
+        if (status < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get blocklist");
+            rc = 1;
+            goto out;
+        }
+
+        /* read recx data then update */
+        hdf5->rx_dspace = H5Dget_space(hdf5->rx_dset);
+        if (hdf5->rx_dspace < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get rx_dspace");
+            rc = 1;
+            goto out;
+        }
+
+        hsize_t start = rx_range[0];
+        hsize_t count = (rx_range[1] - rx_range[0]) + 1;
+        status = H5Sselect_hyperslab(hdf5->rx_dspace,
+                                     H5S_SELECT_AND,
+                                     &start, NULL,
+                                     &count, NULL);
+        if (status < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to select hyperslab");
+            rc = 1;
+            goto out;
+        }
+        recx_len = count;
+        recx_data = MFU_CALLOC(recx_len, rx_dtype_size);
+        if (recx_data == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+        hdf5->mem_dims[0] = count;
+        hdf5->rx_memspace = H5Screate_simple(1, hdf5->mem_dims,
+                                             hdf5->mem_dims);
+        status = H5Dread(hdf5->rx_dset,
+                         hdf5->rx_dtype,
+                         hdf5->rx_memspace,
+                         hdf5->rx_dspace,
+                         H5P_DEFAULT,
+                         recx_data);
+        if (status < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read record extent");
+            rc = 1;
+            goto out;
+        }
+        memset(&sgl, 0, sizeof(sgl));
+        memset(&iov, 0, sizeof(iov));
+        memset(&iod, 0, sizeof(iod));
+        memset(&recxs, 0, sizeof(recxs));
+        d_iov_set(&iod.iod_name,
+                  (void*)hdf5->akey_data[ak_off + k].akey_val.p,
+                  hdf5->akey_data[ak_off + k].akey_val.len);
+        /* set iod values */
+        iod.iod_type  = DAOS_IOD_ARRAY;
+        iod.iod_size  = rx_dtype_size;
+        iod.iod_nr    = 1;
+
+        recxs.rx_nr = recx_len;
+        recxs.rx_idx = start;
+        iod.iod_recxs = &recxs;
+
+        /* set sgl values */
+        sgl.sg_nr     = 1;
+        sgl.sg_iovs   = &iov;
+
+        uint64_t buf_size = recx_len * rx_dtype_size;
+        d_iov_set(&iov, recx_data, buf_size); 
+
+        /* update fetched recx values and place in destination object */
+        rc = daos_obj_update(*oh, DAOS_TX_NONE, 0, &diov, 1, &iod,
+                             &sgl, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to update object: "DF_RC, DP_RC(rc));
+            goto out;
+        }
+
+        stats->bytes_written += buf_size;
+
+        H5Aclose(aid);
+        mfu_free(&rx_range);
+        mfu_free(&recx_data);
+        mfu_free(&decode_buf);
+    }
+out:
+    return rc;
+}
+
+static int cont_deserialize_akeys(struct hdf5_args *hdf5,
+                                  daos_key_t diov,
+                                  uint64_t *ak_off,
+                                  int k,
+                                  daos_handle_t *oh,
+                                  mfu_daos_stats_t* stats)
+{
+    int             rc = 0;
+    daos_key_t      aiov;
+    char            akey[ENUM_KEY_BUF] = {0};
+    int             rx_ndims;
+    uint64_t        index = 0;
+    int             len = 0;
+    int             num_attrs;
+    size_t          single_tsize;
+    void            *single_data = NULL;
+    d_sg_list_t     sgl;
+    d_iov_t         iov;
+    daos_iod_t      iod;
+    hvl_t           *akey_val;
+    hvl_t           *rec_single_val;
+    
+    memset(&aiov, 0, sizeof(aiov));
+    akey_val = &(hdf5->akey_data)[*ak_off + k].akey_val;
+    rec_single_val = &(hdf5->akey_data)[*ak_off + k].rec_single_val;
+    memcpy(akey, akey_val->p, akey_val->len);
+    d_iov_set(&aiov, (void*)akey_val->p, akey_val->len);
+
+    /* if the len of the single value is set to zero,
+     * then this akey points to an array record dataset */
+    if (rec_single_val->len == 0) {
+        index = *ak_off + k;
+        len = snprintf(NULL, 0, "%lu", index);
+        char *dset_name = NULL;
+        dset_name = MFU_CALLOC(1, len + 1);
+        snprintf(dset_name, len + 1, "%lu", index);
+        hdf5->rx_dset = H5Dopen(hdf5->file, dset_name,
+                                H5P_DEFAULT);
+        if (hdf5->rx_dset < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read rx_dset");
+            rc = 1;
+            goto out;
+        }
+        hdf5->rx_dspace = H5Dget_space(hdf5->rx_dset);
+        if (hdf5->rx_dspace < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read rx_dspace");
+            rc = 1;
+            goto out;
+        }
+        hdf5->rx_dtype = H5Dget_type(hdf5->rx_dset);
+        if (hdf5->rx_dtype < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read rx_dtype");
+            rc = 1;
+            goto out;
+        }
+        hdf5->plist = H5Dget_create_plist(hdf5->rx_dset);
+        if (hdf5->plist < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get plist");
+            rc = 1;
+            goto out;
+        }
+        rx_ndims = H5Sget_simple_extent_dims(hdf5->rx_dspace,
+                                             hdf5->rx_dims,
+                                             NULL);
+        if (rx_ndims < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get rx_ndims");
+            rc = 1;
+            goto out;
+        }
+        num_attrs = H5Aget_num_attrs(hdf5->rx_dset);
+        if (num_attrs < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get num attrs");
+            rc = 1;
+            goto out;
+        }
+        rc = cont_deserialize_recx(hdf5, oh, diov, num_attrs, *ak_off, k, stats);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to deserialize recx");
+            rc = 1;
+            goto out;
+        }
+        H5Pclose(hdf5->plist);
+        H5Tclose(hdf5->rx_dtype);
+        H5Sclose(hdf5->rx_dspace);
+        H5Dclose(hdf5->rx_dset);
+        mfu_free(&dset_name);
+    } else {
+        memset(&sgl, 0, sizeof(sgl));
+        memset(&iov, 0, sizeof(iov));
+        memset(&iod, 0, sizeof(iod));
+        single_tsize = rec_single_val->len;
+        if (single_tsize == 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get size of type in single "
+                    "record datatype");
+            rc = 1;
+            goto out;
+        }
+        single_data = MFU_CALLOC(1, single_tsize);
+        if (single_data == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+        memcpy(single_data, rec_single_val->p, rec_single_val->len);
+
+        /* set iod values */
+        iod.iod_type  = DAOS_IOD_SINGLE;
+        iod.iod_size  = single_tsize;
+        iod.iod_nr    = 1;
+        iod.iod_recxs = NULL;
+        iod.iod_name  = aiov;
+
+        /* set sgl values */
+        sgl.sg_nr     = 1;
+        sgl.sg_nr_out = 0;
+        sgl.sg_iovs   = &iov;
+        d_iov_set(&iov, single_data, single_tsize);
+
+        /* update fetched recx values and place in destination object */
+        rc = daos_obj_update(*oh, DAOS_TX_NONE, 0,
+                             &diov, 1, &iod, &sgl, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to update object: "DF_RC, DP_RC(rc));
+            goto out;
+        }
+        stats->bytes_written += single_tsize;
+        mfu_free(&single_data);
+    }
+    stats->total_akeys++;
+out:
+    mfu_free(&single_data);
+    return rc;
+}
+
+static int cont_deserialize_keys(struct hdf5_args *hdf5,
+                                 uint64_t *total_dkeys_this_oid,
+                                 uint64_t *dk_off,
+                                 daos_handle_t *oh,
+                                 mfu_daos_stats_t* stats)
+{
+    int             rc = 0;
+    int             j = 0;
+    daos_key_t      diov;
+    char            dkey[ENUM_KEY_BUF] = {0};
+    uint64_t        ak_off = 0;
+    uint64_t        ak_next = 0;
+    uint64_t        total_akeys_this_dkey = 0;
+    int             k = 0;
+    hvl_t           *dkey_val;
+    hvl_t           *rec_kv_val;
+    
+    for(j = 0; j < *total_dkeys_this_oid; j++) {
+        memset(&diov, 0, sizeof(diov));
+        memset(dkey, 0, sizeof(dkey));
+        dkey_val = &(hdf5->dkey_data)[*dk_off + j].dkey_val;
+        rec_kv_val = &(hdf5->dkey_data)[*dk_off + j].rec_kv_val;
+        memcpy(dkey, dkey_val->p, dkey_val->len);
+        d_iov_set(&diov, (void*)dkey_val->p, dkey_val->len);
+        ak_off = hdf5->dkey_data[*dk_off + j].akey_offset;
+        ak_next = 0;
+        total_akeys_this_dkey = 0;
+        if (*dk_off + j + 1 < (int)hdf5->dkey_dims[0]) {
+            ak_next = hdf5->dkey_data[(*dk_off + j) + 1].akey_offset;
+            total_akeys_this_dkey = ak_next - ak_off;
+        } else if (*dk_off + j == ((int)hdf5->dkey_dims[0] - 1)) {
+            total_akeys_this_dkey = ((int)hdf5->akey_dims[0]) - ak_off;
+        }
+
+        /* if rec_kv_val.len != 0 then skip akey iteration, we can
+         * write data back into DAOS using the daos_kv.h API using just
+         * oid, dkey, and key value (stored in dkey dataset) */
+
+        /* run daos_kv_put on rec_kv_val (dkey val) and key (dkey) */
+        /* skip akey iteration for DAOS_OF_KV_FLAT objects */
+        if (rec_kv_val->len > 0) {
+            daos_size_t kv_single_size = 0;
+            kv_single_size = rec_kv_val->len;
+            rc = daos_kv_put(*oh, DAOS_TX_NONE, 0, dkey, kv_single_size,
+                             rec_kv_val->p, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to put kv object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+            stats->bytes_written += kv_single_size;
+        } else {
+            for (k = 0; k < total_akeys_this_dkey; k++) {
+                rc = cont_deserialize_akeys(hdf5, diov, &ak_off, k, oh, stats);
+                if (rc != 0) {
+                    MFU_LOG(MFU_LOG_ERR, "failed to deserialize akeys: "DF_RC,
+                            DP_RC(rc));
+                    goto out;
+                }
+            }
+        }
+        stats->total_dkeys++;
+    }
+out:
+    return rc;
+}
+
+static int cont_deserialize_prop_str(struct hdf5_args* hdf5,
+                                     struct daos_prop_entry* entry,
+                                     const char* prop_str)
+{
+    hid_t   status = 0;
+    int     rc = 0;
+    hid_t   attr_dtype;
+    hid_t   cont_attr;
+
+    cont_attr = H5Aopen(hdf5->file, prop_str, H5P_DEFAULT);
+    if (cont_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    attr_dtype = H5Aget_type(cont_attr);
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    size_t buf_size = H5Tget_size(attr_dtype);
+    if (buf_size <= 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get size for property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    entry->dpe_str = MFU_CALLOC(1, buf_size);
+    if (entry->dpe_str == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Aread(cont_attr, attr_dtype, entry->dpe_str);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(cont_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Tclose(attr_dtype);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute datatype");
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+static int cont_deserialize_prop_uint(struct hdf5_args* hdf5,
+                                      struct daos_prop_entry* entry,
+                                      const char* prop_str)
+{
+    hid_t   status = 0;
+    int     rc = 0;
+    hid_t   cont_attr;
+    hid_t   attr_dtype;
+
+    cont_attr = H5Aopen(hdf5->file, prop_str, H5P_DEFAULT);
+    if (cont_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    attr_dtype = H5Aget_type(cont_attr);
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Aread(cont_attr, attr_dtype, &entry->dpe_val);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Aclose(cont_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Tclose(attr_dtype);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute datatype", prop_str);
+        rc = 1;
+        goto out;
+    }
+out:
+    return rc;
+}
+
+static int cont_deserialize_prop_roots(struct hdf5_args* hdf5,
+                                       struct daos_prop_entry* entry,
+                                       const char* prop_str,
+                                       struct daos_prop_co_roots *roots)
+{
+    hid_t                       status = 0;
+    int                         rc = 0;
+    int                         i = 0;
+    int                         ndims = 0;
+    obj_id_t                    *root_oids;
+    htri_t                      roots_exist;
+    hid_t                       cont_attr = -1;
+    hid_t                       attr_dtype = -1;
+    hid_t                       attr_dspace = -1;
+    hsize_t                     attr_dims[1];
+    size_t                      attr_dtype_size;
+
+    /* First check if the ACL attribute exists. */
+    roots_exist = H5Aexists(hdf5->file, prop_str);
+    if (roots_exist < 0) {
+        /* Actual error */
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    } else if (roots_exist == 0) {
+        /* Does not exist, but that's okay. */
+        rc = 0;
+        goto out;
+    }
+    cont_attr = H5Aopen(hdf5->file, prop_str, H5P_DEFAULT);
+    if (cont_attr < 0) {
+        /* Could not open, but that's okay. */
+        rc = 0;
+        goto out;
+    }
+    attr_dtype = H5Aget_type(cont_attr);
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    attr_dtype_size = H5Tget_size(attr_dtype);
+    if (attr_dtype_size < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    attr_dspace = H5Aget_space(cont_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read acl dspace");
+        rc = 1;
+        goto out;
+    }
+    ndims = H5Sget_simple_extent_dims(attr_dspace, attr_dims, NULL);
+    if (ndims < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get dimensions of dspace");
+        rc = 1;
+        goto out;
+    }
+    root_oids = MFU_CALLOC(attr_dims[0], sizeof(obj_id_t));
+    if (root_oids == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    /* freed ahead of daos_prop_free in daos_cont_deserialize_connect */
+    roots = MFU_CALLOC(1, sizeof(struct daos_prop_co_roots));
+    if (roots == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    status = H5Aread(cont_attr, attr_dtype, root_oids);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    entry->dpe_val_ptr = (void*)roots;
+    for (i = 0; i < 4; i++) {
+        roots->cr_oids[i].hi = root_oids[i].hi;
+        roots->cr_oids[i].lo = root_oids[i].lo;
+    }
+out:
+    if (cont_attr >= 0) {
+        status = H5Aclose(cont_attr);
+    }
+    if (attr_dtype >= 0) {
+        status = H5Tclose(attr_dtype);
+    }
+    if (attr_dspace >= 0) {
+        status = H5Sclose(attr_dspace);
+    }
+    mfu_free(&root_oids);
+    return rc;
+}
+
+static int cont_deserialize_prop_acl(struct hdf5_args* hdf5,
+                                     struct daos_prop_entry* entry,
+                                     const char* prop_str)
+{
+    hid_t           status = 0;
+    int             rc = 0;
+    int             ndims = 0;
+    const char      **rdata = NULL;
+    struct daos_acl *acl;
+    htri_t          acl_exist;
+    hid_t           cont_attr;
+    hid_t           attr_dtype;
+    hid_t           attr_dspace;
+    hsize_t         attr_dims[1];
+
+    /* First check if the ACL attribute exists. */
+    acl_exist = H5Aexists(hdf5->file, prop_str);
+    if (acl_exist < 0) {
+        /* Actual error */
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    } else if (acl_exist == 0) {
+        /* Does not exist, but that's okay. */
+        rc = 0;
+        goto out;
+    }
+
+    cont_attr = H5Aopen(hdf5->file, prop_str, H5P_DEFAULT);
+    if (cont_attr < 0) {
+        /* Could not open, but that's okay. */
+        rc = 0;
+        goto out;
+    }
+    attr_dtype = H5Aget_type(cont_attr);
+    if (attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open property attribute type %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    attr_dspace = H5Aget_space(cont_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read acl dspace");
+        rc = 1;
+        goto out;
+    }
+    ndims = H5Sget_simple_extent_dims(attr_dspace, attr_dims, NULL);
+    if (ndims < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get dimensions of dspace");
+        rc = 1;
+        goto out;
+    }
+    rdata = MFU_CALLOC(attr_dims[0], sizeof(char*));
+    if (rdata == NULL) {
+        rc = ENOMEM;
+        goto out;
+    }
+    attr_dtype = H5Tcopy(H5T_C_S1);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create dtype");
+        rc = 1;
+        goto out;
+    }
+    status = H5Tset_size(attr_dtype, H5T_VARIABLE);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set acl dtype size");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aread(cont_attr, attr_dtype, rdata);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    /* convert acl strings back to struct acl, then store in entry */
+    rc = daos_acl_from_strs(rdata, (size_t)attr_dims[0], &acl);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to convert acl strs");
+        goto out;
+    }
+    entry->dpe_val_ptr = (void*)acl;
+    status = H5Aclose(cont_attr);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close property attribute %s", prop_str);
+        rc = 1;
+        goto out;
+    }
+    status = H5Tclose(attr_dtype);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute datatype");
+        rc = 1;
+        goto out;
+    }
+    status = H5Sclose(attr_dspace);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to close attribute dataspace");
+        rc = 1;
+        goto out;
+    }
+out:
+    mfu_free(&rdata);
+    return rc;
+}
+
+int cont_deserialize_usr_attrs(struct hdf5_args* hdf5,
+                               daos_handle_t coh)
+{
+    hid_t       status = 0;
+    int         rc = 0;
+    int         num_attrs = 0;
+    int         num_dims = 0;
+    char**      names = NULL;
+    void**      buffers = NULL;
+    size_t*     sizes = NULL;
+    hid_t       dset = 0;
+    hid_t       dspace = 0;
+    hid_t       vtype = 0;
+    hsize_t     dims[1];
+    usr_attr_t* attr_data = NULL;
+
+    /* Read the user attributes */
+    dset= H5Dopen(hdf5->file, "User Attributes", H5P_DEFAULT);
+    if (dset < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open user attributes dset");
+        rc = 1;
+        goto out;
+    }
+    dspace = H5Dget_space(dset);
+    if (dspace < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get user attributes dspace");
+        rc = 1;
+        goto out;
+    }
+    vtype = H5Dget_type(dset);
+    if (vtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get user attributes vtype");
+        rc = 1;
+        goto out;
+    }
+    num_dims = H5Sget_simple_extent_dims(dspace, dims, NULL);
+    if (num_dims < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get user attributes dimensions");
+        rc = 1;
+        goto out;
+    }
+    num_attrs = dims[0];
+    attr_data = MFU_CALLOC(dims[0], sizeof(usr_attr_t));
+    if (attr_data == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attributes");
+        rc = 1;
+        goto out;
+    }
+    status = H5Dread(dset, vtype, H5S_ALL, H5S_ALL,
+                     H5P_DEFAULT, attr_data);
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read user attributes data");
+        rc = 1;
+        goto out;
+    }
+
+    names = MFU_CALLOC(num_attrs, sizeof(char*));
+    if (names == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attributes");
+        rc = 1;
+        goto out;
+    }
+    buffers = MFU_CALLOC(num_attrs, sizeof(void*));
+    if (buffers == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attributes");
+        rc = 1;
+        goto out;
+    }
+    sizes = MFU_CALLOC(num_attrs, sizeof(size_t));
+    if (sizes == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "failed to allocate user attributes");
+        rc = 1;
+        goto out;
+    }
+
+    /* Set the user attribute buffers */
+    for (int i = 0; i < num_attrs; i++) {
+        names[i] = attr_data[i].attr_name;
+        buffers[i] = attr_data[i].attr_val.p;
+        sizes[i] = attr_data[i].attr_val.len;
+    }
+
+    rc = daos_cont_set_attr(coh, num_attrs,
+                            (const char * const*) names,
+                            (const void * const*) buffers,
+                            sizes, NULL);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to set user attributes "DF_RC, DP_RC(rc));
+        rc = 1;
+        goto out;
+    }
+
+out:
+    H5Dclose(dset);
+    H5Sclose(dspace);
+    H5Tclose(vtype);
+    cont_free_usr_attrs(num_attrs, &names, &buffers, &sizes);
+    mfu_free(&attr_data);
+    return rc;
+}
+
+int cont_deserialize_all_props(struct hdf5_args *hdf5, 
+                               daos_prop_t **_prop,
+                               struct daos_prop_co_roots *roots,
+                               daos_cont_layout_t *cont_type,
+                               daos_handle_t poh)
+{
+    int                     rc = 0;
+    bool                    deserialize_label = false;
+    uint32_t                num_props = 18;
+    daos_prop_t             *label = NULL;
+    daos_prop_t             *prop = NULL;
+    struct daos_prop_entry  *entry;
+    struct daos_prop_entry  *label_entry;
+    daos_handle_t           coh;
+    daos_cont_info_t        cont_info = {0};
+
+    label = daos_prop_alloc(1);
+    if (label == NULL) {
+        return ENOMEM;
+    }
+    label->dpp_entries[0].dpe_type = DAOS_PROP_CO_LABEL; 
+
+    /* read the container label entry to decide if it should be added
+     * to property list. The container label is required to be unique in
+     * DAOS, which is why it is handled differently than the other 
+     * container properties. If the label already  exists in the
+     * pool then this property will be skipped for deserialization */
+    label_entry = &label->dpp_entries[0];
+    rc = cont_deserialize_prop_str(hdf5, label_entry, "DAOS_PROP_CO_LABEL");
+    if (rc != 0) {
+        goto out;
+    }
+
+    rc = daos_cont_open(poh, label_entry->dpe_str, DAOS_COO_RW, &coh, &cont_info, NULL);
+    if (rc == -DER_NONEXIST) {
+        /* doesn't exist so ok to deserialize this container label */
+        deserialize_label = true;
+    } else if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "daos_cont_open failed: "DF_RC, DP_RC(rc));
+        goto out;
+    }  else {
+        /* if this succeeds then label already exists, close container after
+         * checking */
+        rc = daos_cont_close(coh, NULL);
+        if (rc != 0) {
+            goto out;
+        }
+    }
+
+    if (deserialize_label) {
+        num_props++;
+    }
+
+    prop = daos_prop_alloc(num_props);
+    if (prop == NULL) {
+        return ENOMEM;
+    }
+
+    prop->dpp_entries[0].dpe_type = DAOS_PROP_CO_LAYOUT_TYPE;
+    prop->dpp_entries[1].dpe_type = DAOS_PROP_CO_LAYOUT_VER;
+    prop->dpp_entries[2].dpe_type = DAOS_PROP_CO_CSUM;
+    prop->dpp_entries[3].dpe_type = DAOS_PROP_CO_CSUM_CHUNK_SIZE;
+    prop->dpp_entries[4].dpe_type = DAOS_PROP_CO_CSUM_SERVER_VERIFY;
+    prop->dpp_entries[5].dpe_type = DAOS_PROP_CO_REDUN_FAC;
+    prop->dpp_entries[6].dpe_type = DAOS_PROP_CO_REDUN_LVL;
+    prop->dpp_entries[7].dpe_type = DAOS_PROP_CO_SNAPSHOT_MAX;
+    prop->dpp_entries[8].dpe_type = DAOS_PROP_CO_COMPRESS;
+    prop->dpp_entries[9].dpe_type = DAOS_PROP_CO_ENCRYPT;
+    prop->dpp_entries[10].dpe_type = DAOS_PROP_CO_OWNER;
+    prop->dpp_entries[11].dpe_type = DAOS_PROP_CO_OWNER_GROUP;
+    prop->dpp_entries[12].dpe_type = DAOS_PROP_CO_DEDUP;
+    prop->dpp_entries[13].dpe_type = DAOS_PROP_CO_DEDUP_THRESHOLD;
+    prop->dpp_entries[14].dpe_type = DAOS_PROP_CO_EC_CELL_SZ;
+    prop->dpp_entries[15].dpe_type = DAOS_PROP_CO_ALLOCED_OID;
+    prop->dpp_entries[16].dpe_type = DAOS_PROP_CO_ACL;
+    prop->dpp_entries[17].dpe_type = DAOS_PROP_CO_ROOTS;
+    if (deserialize_label) {
+        prop->dpp_entries[18].dpe_type = DAOS_PROP_CO_LABEL; 
+    }
+
+    entry = &prop->dpp_entries[0];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_LAYOUT_TYPE");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[1];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_LAYOUT_VER");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[2];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_CSUM");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[3];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_CSUM_CHUNK_SIZE");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[4];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_CSUM_SERVER_VERIFY");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[5];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_REDUN_FAC");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[6];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_REDUN_LVL");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[7];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_SNAPSHOT_MAX");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[8];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_COMPRESS");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[9];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_ENCRYPT");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[10];
+    rc = cont_deserialize_prop_str(hdf5, entry, "DAOS_PROP_CO_OWNER");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[11];
+    rc = cont_deserialize_prop_str(hdf5, entry, "DAOS_PROP_CO_OWNER_GROUP");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[12];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_DEDUP");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[13];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_DEDUP_THRESHOLD");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[14];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_EC_CELL_SZ");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[15];
+    rc = cont_deserialize_prop_uint(hdf5, entry, "DAOS_PROP_CO_ALLOCED_OID");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[16];
+    /* read acl as a list of strings in deserialize, then convert
+     * back to acl for property entry
+     */
+    rc = cont_deserialize_prop_acl(hdf5, entry, "DAOS_PROP_CO_ACL");
+    if (rc != 0) {
+        goto out;
+    }
+
+    entry = &prop->dpp_entries[17];
+    rc = cont_deserialize_prop_roots(hdf5, entry, "DAOS_PROP_CO_ROOTS", roots);
+    if (rc != 0) {
+        goto out;
+    }
+
+    if (deserialize_label) {
+        prop->dpp_entries[18].dpe_str = strdup(label_entry->dpe_str);
+    }
+    *cont_type = prop->dpp_entries[0].dpe_val;
+    *_prop = prop;
+out:
+    daos_prop_free(label);
+    return rc;
+}
+
+/* on rank 0, connect to pool, read cont properties because
+ * MAX_OID can only works if set on container creation,
+ * and then broadcast handles to all ranks */
+int daos_cont_deserialize_connect(daos_args_t *daos_args,
+                                  struct hdf5_args *hdf5,
+                                  daos_cont_layout_t *cont_type,
+                                  char *label)
+{
+    int                         rc = 0;
+    daos_prop_t                 *prop = NULL;
+    struct daos_prop_co_roots   roots = {0};
+
+    /* generate container UUID */
+    uuid_generate(daos_args->dst_cont_uuid);
+
+    daos_pool_info_t pool_info = {0};
+    daos_cont_info_t co_info = {0};
+#if DAOS_API_VERSION_MAJOR < 1
+    rc = daos_pool_connect(daos_args->src_pool, NULL, NULL, DAOS_PC_RW,
+                           &daos_args->src_poh, &pool_info, NULL);
+#else
+    rc = daos_pool_connect(daos_args->src_pool, NULL, DAOS_PC_RW,
+                           &daos_args->src_poh, &pool_info, NULL);
+#endif
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to connect to pool: "DF_RC, DP_RC(rc));
+        goto out;
+    }
+
+    /* need to read cont props before creating container, then
+     * broadcast handles to the rest of the ranks */
+    rc = cont_deserialize_all_props(hdf5, &prop, &roots, cont_type,
+                                    daos_args->src_poh);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to deserialize container properties");
+        goto out;
+    }
+
+    if (label != NULL) {
+        rc = daos_cont_create_with_label(daos_args->src_poh, label, prop, &daos_args->dst_cont_uuid, NULL);
+    } else {
+        rc = daos_cont_create(daos_args->src_poh, daos_args->dst_cont_uuid, prop, NULL);
+    }
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to create container: "DF_RC, DP_RC(rc));
+        goto out;
+    }
+    char cont_str[130];
+    if (label != NULL) {
+        MFU_LOG(MFU_LOG_INFO, "Successfully created container %s", label);
+        rc = daos_cont_open(daos_args->src_poh, label, DAOS_COO_RW,
+                            &daos_args->src_coh, &co_info, NULL);
+    } else {
+        uuid_unparse(daos_args->dst_cont_uuid, cont_str);
+        MFU_LOG(MFU_LOG_INFO, "Successfully created container %s", cont_str);
+        rc = daos_cont_open(daos_args->src_poh, cont_str,
+                            DAOS_COO_RW, &daos_args->src_coh, &co_info, NULL);
+    }
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open container: "DF_RC, DP_RC(rc));
+        goto out;
+    }
+    mfu_free(&roots);
+    daos_prop_free(prop);
+out:
+    return rc;
+}
+
+int daos_cont_deserialize_hdlr(int rank, daos_args_t *da, const char *h5filename,
+                               mfu_daos_stats_t* stats)
+{
+    int                     rc = 0;
+    int                     i = 0;
+    struct                  hdf5_args hdf5;
+    daos_obj_id_t           oid;
+    daos_handle_t           oh;
+    uint64_t                dk_off = 0;
+    uint64_t                dk_next = 0;
+    uint64_t                total_dkeys_this_oid = 0;
+    hid_t                   status = 0;
+    float                   version;
+    bool                    is_kv = false;
+
+    /* init HDF5 args */
+    init_hdf5_args(&hdf5);
+
+    printf("\tDeserializing filename: %s\n", h5filename);
+
+    /* open passed in HDF5 file */
+    hdf5.file = H5Fopen(h5filename, H5F_ACC_RDONLY, H5P_DEFAULT);
+    if (hdf5.file < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open hdf5 file");
+        rc = 1;
+        goto out;
+    }
+
+    /* deserialize version -- serialization version/format should
+     * be compatible with deserialization version
+     */
+    hdf5.version_attr = H5Aopen(hdf5.file, "Version", H5P_DEFAULT);
+    if (hdf5.version_attr < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to open version attr");
+        rc = 1;
+        goto out;
+    }
+    hdf5.attr_dtype = H5Aget_type(hdf5.version_attr);
+    if (hdf5.attr_dtype < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to get attr type");
+        rc = 1;
+        goto out;
+    }
+    status = H5Aread(hdf5.version_attr, hdf5.attr_dtype, &version); 
+    if (status < 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read version");
+        rc = 1;
+        goto out;
+    }
+    if (version > 0.0) {
+        MFU_LOG(MFU_LOG_ERR, "serialization format is not compatible with "
+                "deserialization version\n");
+        rc = 1;
+        goto out;
+    }
+
+    /* deserialize and set the user attributes if they exist */
+    htri_t usr_attrs_exist = H5Lexists(hdf5.file, "User Attributes", H5P_DEFAULT);
+    if (usr_attrs_exist > 0) {
+        rc = cont_deserialize_usr_attrs(&hdf5, da->src_coh);
+        if (rc != 0) {
+            rc = 1;
+            goto out;
+        }
+    }
+
+    rc = hdf5_read_key_data(&hdf5);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "failed to read hdf5 key data");
+        rc = 1;
+        goto out;
+    }
+    for (i = 0; i < (int)hdf5.oid_dims[0]; i++) {
+        oid.lo = hdf5.oid_data[i].oid_low;
+        oid.hi = hdf5.oid_data[i].oid_hi;
+	is_kv = obj_is_kv(oid);
+        if (is_kv) {
+            rc = daos_kv_open(da->src_coh, oid, DAOS_OO_RW, &oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to open kv object: "DF_RC, DP_RC(rc));
+                rc = 1;
+                goto out;
+            }
+        } else {
+            rc = daos_obj_open(da->src_coh, oid, 0, &oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to open object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        }
+        dk_off = hdf5.oid_data[i].dkey_offset;
+        dk_next = 0;
+        total_dkeys_this_oid = 0;
+        if (i + 1 < (int)hdf5.oid_dims[0]) {
+            dk_next = hdf5.oid_data[i + 1].dkey_offset;
+            total_dkeys_this_oid = dk_next - dk_off;
+        } else if (i == ((int)hdf5.oid_dims[0] - 1)){
+            total_dkeys_this_oid = (int)hdf5.dkey_dims[0] - (dk_off);
+        } 
+        rc = cont_deserialize_keys(&hdf5, &total_dkeys_this_oid, &dk_off, &oh, stats);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to deserialize keys: %d", rc);
+            goto out;
+        }
+        if (is_kv) {
+            rc = daos_kv_close(oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to close kv object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        } else {
+            rc = daos_obj_close(oh, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to close object: "DF_RC, DP_RC(rc));
+                goto out;
+            }
+        }
+
+        /* Increment as we go */
+        stats->total_oids++;
+    }
+out:
+    if (hdf5.oid_dset > 0) {
+        H5Dclose(hdf5.oid_dset);
+    }
+    if (hdf5.dkey_dset > 0) {
+        H5Dclose(hdf5.dkey_dset);
+    }
+    if (hdf5.akey_dset > 0) {
+        H5Dclose(hdf5.akey_dset);
+    }
+    if (hdf5.oid_dspace > 0) {
+        H5Sclose(hdf5.oid_dspace);
+    }
+    if (hdf5.dkey_dspace > 0) {
+        H5Sclose(hdf5.dkey_dspace);
+    }
+    if (hdf5.akey_dspace > 0) {
+        H5Sclose(hdf5.akey_dspace);
+    }
+    if (hdf5.oid_dtype > 0) {
+        H5Tclose(hdf5.oid_dtype);
+    }
+    if (hdf5.dkey_vtype > 0) {
+        H5Tclose(hdf5.dkey_vtype);
+    }
+    if (hdf5.akey_vtype > 0) {
+        H5Tclose(hdf5.akey_vtype);
+    }
+    if (hdf5.file > 0) {
+        H5Fclose(hdf5.file);
+    }
+    return rc;
+}
+
+int mfu_daos_hdf5_copy(char **argpaths,
+                       daos_args_t *daos_args)
+{
+    int                 rc = 0;
+    int                 size;
+    char                src_path[FILENAME_LEN];
+    char                dst_path[FILENAME_LEN];
+    char                dst_cont_str[UUID_LEN];
+    struct duns_attr_t  src_dattr = {0};
+    struct duns_attr_t  dst_dattr = {0};
+    bool                src_daos = false;
+    bool                dst_daos = false;
+    int                 len = 0;
+
+    MPI_Comm_size(MPI_COMM_WORLD, &size);
+    if (size > 1) {
+        MFU_LOG(MFU_LOG_ERR, "dcp uses h5repack to copy DAOS HDF5 containers "
+                "to and from a POSIX filesystem. Only one process "
+                "is currently supported. Restart program using only "
+                "one MPI process "
+                MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+                rc = 1;
+                return rc;
+    }
+
+    /* check for DAOS path and set VOL Name accordingly */
+    rc = duns_resolve_path(argpaths[0], &src_dattr);
+    if (rc == 0) {
+        src_daos = true;
+        snprintf(daos_args->src_pool, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", src_dattr.da_pool);
+        snprintf(daos_args->src_cont, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", src_dattr.da_cont);
+
+        /* build src path for h5repack */
+        len = snprintf(src_path, FILENAME_LEN, "daos://%s/%s",
+                       daos_args->src_pool, daos_args->src_cont);
+        if (len > FILENAME_LEN) {
+            MFU_LOG(MFU_LOG_ERR, "source path exceeds max length "
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DCP));
+        }
+    } else {
+        /* source path is POSIX */
+        rc = 0;
+    }
+
+    rc = duns_resolve_path(argpaths[1], &dst_dattr);
+    if (rc == 0) {
+        dst_daos = true;
+        snprintf(daos_args->dst_pool, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", dst_dattr.da_pool);
+        snprintf(daos_args->dst_cont, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", dst_dattr.da_cont);
+        bool dst_cont_passed = strlen(daos_args->dst_cont) > 0 ? true : false;
+        if (!dst_cont_passed) {
+            uuid_generate(daos_args->dst_cont_uuid);
+            uuid_unparse(daos_args->dst_cont_uuid, dst_cont_str);
+        }
+
+        /* build dst path for h5repack */
+        if (dst_cont_passed) {
+            len = snprintf(dst_path, FILENAME_LEN, "daos://%s/%s",
+                           daos_args->dst_pool, daos_args->dst_cont);
+        } else {
+            len = snprintf(dst_path, FILENAME_LEN, "daos://%s/%s",
+                           daos_args->dst_pool, dst_cont_str);
+        }
+        if (len > FILENAME_LEN) {
+            MFU_LOG(MFU_LOG_ERR, "destination path exceeds max length "
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DCP));
+        }
+    } else {
+        /* destination path is POSIX */
+        rc = 0;
+    }
+
+    pid_t child;
+    int child_status;
+    pid_t child_pid;
+    char *args[4];
+    args[0] = "h5repack";
+    args[1] = NULL;
+    args[2] = NULL;
+    args[3] = NULL;
+    args[4] = NULL;
+
+    if (src_daos && dst_daos) {
+        /* this must be a DAOS->DAOS copy */
+        args[1] = strdup(src_path);
+        args[2] = strdup(dst_path);
+        args[3] = NULL;
+    } else if (src_daos && !dst_daos) {
+        args[1] = strdup("--dst-vol-name=native");
+        args[2] = strdup(src_path);
+        args[3] = strdup(argpaths[1]);
+        args[4] = NULL; 
+    } else if (!src_daos && dst_daos) {
+        args[1] = strdup("--src-vol-name=native");
+        args[2] = strdup(argpaths[0]);
+        args[3] = strdup(dst_path);
+        args[4] = NULL; 
+    } else {
+        MFU_LOG(MFU_LOG_ERR, "Invalid Format " MFU_ERRF,
+                MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        rc = 1;
+        return rc;
+    }
+
+    /* Child process */
+    if ((child = fork()) == 0) {
+
+        /* start h5repack */
+        execvp(args[0], args);
+
+        /* if child process reaches this point then execvp must have failed */
+        MFU_LOG(MFU_LOG_ERR, "execvp on h5repack failed: "
+                MFU_ERRF, MFU_ERRP(-MFU_ERR_DCP));
+        mfu_free(&args[1]);
+        mfu_free(&args[2]);
+        mfu_free(&args[3]);
+        mfu_free(&args[4]);
+        rc = 1;
+        return rc;
+    /* Parent process */
+    } else {
+        if (child == (pid_t)(-1)) {
+            MFU_LOG(MFU_LOG_ERR, "fork failed: "
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_DCP));
+            mfu_free(&args[1]);
+            mfu_free(&args[2]);
+            mfu_free(&args[3]);
+            mfu_free(&args[4]);
+            rc = 1;
+            return rc;
+        } else {
+            /* parent will wait for child to complete */
+            child_pid = wait(&child_status);
+        }
+    }
+
+    /* if child status is 0, then copy was successful */
+    if (child_status != 0) {
+        MFU_LOG(MFU_LOG_ERR, "errors occurred while copying to/from container: "
+                MFU_ERRF, MFU_ERRP(-MFU_ERR_DCP));
+        mfu_free(&args[1]);
+        mfu_free(&args[2]);
+        mfu_free(&args[3]);
+        mfu_free(&args[4]);
+        rc = 1;
+        return rc;
+    } else {
+        if (dst_daos) {
+            MFU_LOG(MFU_LOG_INFO, "Successfully copied to %s", dst_path);
+        } else {
+            MFU_LOG(MFU_LOG_INFO, "Successfully copied to %s", argpaths[1]);
+        }
+    }
+
+    mfu_free(&args[1]);
+    mfu_free(&args[2]);
+    mfu_free(&args[3]);
+    mfu_free(&args[4]);
+    return rc;
+}
+#endif
diff --git a/src/common/mfu_daos.h b/src/common/mfu_daos.h
index 454ee04..86624c0 100644
--- a/src/common/mfu_daos.h
+++ b/src/common/mfu_daos.h
@@ -1,12 +1,19 @@
 #include "mfu.h"
 
 #include <daos.h>
+#ifdef HDF5_SUPPORT
+#include <hdf5.h>
+#endif
 #include "mfu_flist_internal.h"
 
-#define ENUM_KEY_BUF		32 /* size of each dkey/akey */
+#define ENUM_KEY_BUF		128 /* size of each dkey/akey */
 #define ENUM_LARGE_KEY_BUF	(512 * 1024) /* 512k large key */
 #define ENUM_DESC_NR		5 /* number of keys/records returned by enum */
 #define ENUM_DESC_BUF		512 /* all keys/records returned by enum */
+#define OID_ARR_SIZE        50
+#define ATTR_NAME_LEN       64
+#define FILENAME_LEN        1024 
+#define UUID_LEN            128
 
 enum handleType {
     POOL_HANDLE,
@@ -17,26 +24,241 @@ enum handleType {
 typedef enum {
     DAOS_API_AUTO,
     DAOS_API_DFS,
-    DAOS_API_DAOS
+    DAOS_API_DAOS,
+    DAOS_API_HDF5
 } daos_api_t;
 
 /* struct for holding DAOS arguments */
 typedef struct {
-    daos_handle_t src_poh; /* source pool handle */
-    daos_handle_t dst_poh; /* destination pool handle */
-    daos_handle_t src_coh; /* source container handle */
-    daos_handle_t dst_coh; /* destination container handle */
-    uuid_t src_pool_uuid;  /* source pool UUID */
-    uuid_t dst_pool_uuid;  /* destination pool UUID */
-    uuid_t src_cont_uuid;  /* source container UUID */
-    uuid_t dst_cont_uuid;  /* destination container UUID */
-    char* dfs_prefix;      /* prefix for UNS */
-    char* src_path;        /* allocated src path */
-    char* dst_path;        /* allocated dst path */
-    daos_api_t api;        /* API to use */
-    daos_epoch_t epc;      /* src container epoch */
+    daos_handle_t src_poh;  /* source pool handle */
+    daos_handle_t dst_poh;  /* destination pool handle */
+    daos_handle_t src_coh;  /* source container handle */
+    daos_handle_t dst_coh;  /* destination container handle */
+    char src_pool[DAOS_PROP_LABEL_MAX_LEN + 1];
+    char src_cont[DAOS_PROP_LABEL_MAX_LEN + 1];
+    char dst_pool[DAOS_PROP_LABEL_MAX_LEN + 1];
+    char dst_cont[DAOS_PROP_LABEL_MAX_LEN + 1];
+    /* if destination container is not created new UUID is generated */
+    uuid_t dst_cont_uuid;   /* destination container UUID */
+    char* dfs_prefix;       /* prefix for UNS */
+    char* src_path;         /* allocated src path */
+    char* dst_path;         /* allocated dst path */
+    daos_api_t api;         /* API to use */
+    daos_epoch_t src_epc;   /* src container epoch */
+    daos_epoch_t dst_epc;   /* dst container epoch */
+    bool allow_exist_dst_cont;          /* whether to allow the dst container to exist for DAOS API */
+    enum daos_cont_props src_cont_type; /* type of the source container */
+    enum daos_cont_props dst_cont_type; /* type of the destination container */
+    bool daos_preserve;                 /* preserve daos cont props and user attrs */
+    char *daos_preserve_path;           /* set path to write daos props and user attrs */
 } daos_args_t;
 
+/* struct for holding statistics */
+typedef struct mfu_daos_stats {
+    uint64_t total_oids;    /* sum of object ids */
+    uint64_t total_dkeys;   /* sum of dkeys */
+    uint64_t total_akeys;   /* sum of akeys */
+    uint64_t bytes_read;    /* sum of bytes read (src and dst) */
+    uint64_t bytes_written; /* sum of bytes written */
+    time_t   time_started;  /* time when started */
+    time_t   time_ended;    /* time when ended */
+    double   wtime_started; /* relative time when started */
+    double   wtime_ended;   /* relative time when ended */
+} mfu_daos_stats_t;
+
+/* initialize the mfu_daos_stats_t */
+void mfu_daos_stats_init(mfu_daos_stats_t* stats);
+
+/* set the stats start time */
+void mfu_daos_stats_start(mfu_daos_stats_t* stats);
+
+/* set the stats end time */
+void mfu_daos_stats_end(mfu_daos_stats_t* stats);
+
+/* sum the stats into another stats struct */
+void mfu_daos_stats_sum(mfu_daos_stats_t* stats, mfu_daos_stats_t* stats_sum);
+
+/* print the stats */
+void mfu_daos_stats_print(
+    mfu_daos_stats_t* stats,
+    bool print_read,
+    bool print_write,
+    bool print_read_rate,
+    bool print_write_rate
+);
+
+/* sum and printthe stats */
+void mfu_daos_stats_print_sum(
+    int rank,
+    mfu_daos_stats_t* stats,
+    bool print_read,
+    bool print_write,
+    bool print_read_rate,
+    bool print_write_rate
+);
+
+#ifdef HDF5_SUPPORT
+/* daos_obj_id_t type defined for hdf5 attribute. The DAOS_PROP_CO_ROOTS
+ * points to an array of daos_obj_id_t, which is a struct that contains
+ * hi, lo uint64_t's. In order to write these to an hdf5 file a similar
+ * type has to be defined to describe it to hdf5 */
+typedef struct {
+    uint64_t hi;
+    uint64_t lo;
+} obj_id_t;
+
+/* for user attr dataset */
+typedef struct {
+    char* attr_name;
+    hvl_t attr_val;
+} usr_attr_t;
+
+/* for oid dataset */
+typedef struct {
+	uint64_t oid_hi;
+	uint64_t oid_low;
+	uint64_t dkey_offset;
+} oid_t;
+
+/* for dkey dataset */
+typedef struct {
+	/* array of vlen structure */
+	hvl_t dkey_val;
+	uint64_t akey_offset;
+    /* for for kv values that can be
+     * written with daos_kv.h */
+	hvl_t rec_kv_val;
+} dkey_t;
+
+/* for akey dataset */
+typedef struct {
+	/* array of vlen structure */
+	hvl_t akey_val;
+	uint64_t rec_dset_id;
+	hvl_t rec_single_val;
+} akey_t;
+
+struct hdf5_args {
+    hid_t status;
+    hid_t file;
+    /* User attribute data */
+    hid_t usr_attr_memtype;
+    hid_t usr_attr_name_vtype;
+    hid_t usr_attr_val_vtype;
+    /* OID Data */
+    hid_t oid_memtype;
+    hid_t oid_dspace;
+    hid_t oid_dset;
+    hid_t oid_dtype;
+    /* DKEY Data */
+    hid_t dkey_memtype;
+    hid_t dkey_vtype;
+    hid_t dkey_dspace;
+    hid_t dkey_dset;
+    /* AKEY Data */
+    hid_t akey_memtype;
+    hid_t akey_vtype;
+    hid_t akey_dspace;
+    hid_t akey_dset;
+    /* recx Data */
+    hid_t plist;
+    hid_t rx_dspace;
+    hid_t rx_memspace;
+    hid_t attr_dspace;
+    hid_t attr_dtype;
+    hid_t rx_dset;
+    hid_t rx_dtype;
+    hid_t usr_attr;
+    hid_t cont_attr;
+    hid_t selection_attr;
+    hid_t version_attr;
+    hid_t version_attr_dspace;
+    hid_t version_attr_type;
+    /* dims for dsets */
+    hsize_t oid_dims[1];
+    hsize_t dkey_dims[1];     
+    hsize_t akey_dims[1];     
+    hsize_t rx_dims[1];
+    hsize_t mem_dims[1];
+    hsize_t attr_dims[1];
+    hsize_t rx_chunk_dims[1];
+    hsize_t rx_max_dims[1];
+    hsize_t version_attr_dims[1];
+    /* data for keys */
+    oid_t *oid_data;
+    dkey_t *dkey_data;
+    akey_t *akey_data;
+    oid_t  **oid;
+    dkey_t **dk;
+    akey_t **ak;
+};
+
+/* serialize a container to
+ * an HDF5 file */
+int daos_cont_serialize_hdlr(
+    int rank,
+    struct hdf5_args *hdf5,
+    char *output_dir,
+    uint64_t *files_written,
+    daos_args_t *da,
+    mfu_flist flist,
+    uint64_t num_oids,
+    mfu_daos_stats_t* stats
+);
+
+/* serialize a container to
+ * an HDF5 file */
+int daos_cont_deserialize_hdlr(
+    int rank,
+    daos_args_t *da,
+    const char *h5filename,
+    mfu_daos_stats_t* stats
+);
+
+/* connect to pool, read cont properties because
+ * MAX_OID can only works if set on container creation,
+ * and then broadcast handles to all ranks */
+int daos_cont_deserialize_connect(
+    daos_args_t *daos_args,
+    struct hdf5_args *hdf5,
+    daos_cont_layout_t *cont_type,
+    char *label
+);
+
+/* serialize the number of hdf5 files generated by
+ * daos_cont_serialize_hdlr, this allows deserialization
+ * to check if all data is present */
+int daos_cont_serialize_files_generated(
+    struct hdf5_args *hdf5,
+    uint64_t *files_generated
+);
+
+/* check if source or destination is HDF5 container, and
+ * if so, then launch/run h5repack */
+int mfu_daos_hdf5_copy(char **argpaths,
+                       daos_args_t *daos_args);
+
+/* serialize DAOS container properties */
+int cont_serialize_props(struct hdf5_args *hdf5,
+                         daos_handle_t cont);
+
+/* serialize DAOS user attributes*/
+int cont_serialize_usr_attrs(struct hdf5_args *hdf5,
+                             daos_handle_t cont);
+
+int cont_deserialize_all_props(struct hdf5_args *hdf5,
+                               daos_prop_t **prop, 
+                               struct daos_prop_co_roots *roots,
+                               daos_cont_layout_t *cont_type,
+                               daos_handle_t poh);
+
+int cont_deserialize_usr_attrs(struct hdf5_args* hdf5,
+                               daos_handle_t coh);
+#endif
+
+/* Check whether a uuid is valid */
+bool daos_uuid_valid(const uuid_t uuid);
+
 /* Return a newly allocated daos_args_t structure.
  * Set default values on its fields. */
 daos_args_t* daos_args_new(void);
@@ -62,11 +284,49 @@ int daos_parse_epc_str(
 int daos_setup(
   int rank,
   char** argpaths,
+  int numpaths,
   daos_args_t* da,
   mfu_file_t* mfu_src_file,
   mfu_file_t* mfu_dst_file
 );
 
+/* parse a daos path of the form
+ * daos://pool/cont, /pool/cont/,
+ * or UNS path */
+int daos_parse_path(
+    char *path,
+    size_t path_len,
+    char (*pool_str)[],
+    char (*cont_str)[]
+);
+
+/* connect to DAOS pool,
+ * and then open container */
+int daos_connect(
+  int rank,
+  daos_args_t* da,
+  char (*pool)[],
+  char (*cont)[],
+  daos_handle_t* poh,
+  daos_handle_t* coh,
+  bool force_serialize,
+  bool connect_pool,
+  bool create_cont,
+  bool require_new_cont,
+  bool preserve,
+  mfu_file_t* mfu_src_file,
+  bool dst_cont_passed
+);
+
+/* broadcast a pool or cont handle
+ * from rank 0 */
+void daos_bcast_handle(
+    int rank,
+    daos_handle_t* handle,
+    daos_handle_t* poh,
+    enum handleType type
+);
+
 /* Unmount DFS.
  * Disconnect from pool/cont.
  * Cleanup DAOS-related vars, handles. 
@@ -77,15 +337,20 @@ int daos_cleanup(
   mfu_file_t* mfu_dst_file
 );
 
-/* walk objects in daos and insert to given flist */
-int mfu_flist_walk_daos(
+/* Walk objects in daos and insert to given flist.
+ * Returns -1 on failure, 0 on success. */
+int mfu_daos_flist_walk(
     daos_args_t* da,
+    daos_handle_t coh,
+    daos_epoch_t* epoch,
     mfu_flist flist
 );
 
-/* copy objects in flist to destination listed in daos args,
+/* copy/sync objects in flist to destination listed in daos args,
  * copies DAOS data at object level (non-posix) */
-int mfu_flist_copy_daos(
-    daos_args_t* da,
-    mfu_flist flist
+int mfu_daos_flist_sync(
+    daos_args_t* da,    /* DAOS args */
+    mfu_flist flist,    /* flist containing oids */
+    bool compare_dst,   /* whether to compare the dst before writing */
+    bool write_dst      /* whether to actually write to the dst */
 );
diff --git a/src/common/mfu_decompress_bz2_libcircle.c b/src/common/mfu_decompress_bz2_libcircle.c
index 47c8ce3..f714369 100644
--- a/src/common/mfu_decompress_bz2_libcircle.c
+++ b/src/common/mfu_decompress_bz2_libcircle.c
@@ -258,13 +258,13 @@ int mfu_decompress_bz2_libcircle(const char* src, const char* dst)
     }
 
     /* extract values from footer into local variables */
-    block_meta  = (int64_t)footer[0]; /* offset to start of block metadata */
-    block_total = (int64_t)footer[1]; /* number of blocks */
+    block_meta  = (int64_t)footer[0];         /* offset to start of block metadata */
+    block_total = (int64_t)footer[1];         /* number of blocks */
     block_size          = (int64_t)footer[2]; /* max uncompressed size of a block */
-    int64_t data_size   = (int64_t)footer[3]; /* uncompressed size of all blocks */
+                                              /* (int64_t)footer[3] - unused - uncompressed size of all blocks */
     uint64_t version    = footer[4];          /* dbz2 file format footer version */
     uint64_t magic      = footer[5];          /* dbz2 file format magic value */
-    int64_t filesize    = (int64_t)footer[6]; /* file size of compressed file */
+                                              /* (int64_t)footer[6] - unused - file size of compressed file */
 
     /* check that we got correct magic value */
     if (magic != 0x3141314131413141) {
diff --git a/src/common/mfu_errors.h b/src/common/mfu_errors.h
index ae80453..ce4ff17 100644
--- a/src/common/mfu_errors.h
+++ b/src/common/mfu_errors.h
@@ -8,6 +8,18 @@ extern "C" {
 #ifndef MFU_ERRORS_H
 #define MFU_ERRORS_H
 
+#include <errno.h>
+
+/* Given a system error code, set errno and return -1 on error */
+static inline int mfu_errno2rc(int err)
+{
+    if (err == 0) {
+        return 0;
+    }
+    errno = err;
+    return -1;
+}
+
 /* Generic error codes */
 #define MFU_ERR           1000
 #define MFU_ERR_INVAL_ARG 1001
diff --git a/src/common/mfu_flist.c b/src/common/mfu_flist.c
index 772fdc2..a0071f1 100644
--- a/src/common/mfu_flist.c
+++ b/src/common/mfu_flist.c
@@ -73,7 +73,6 @@ mfu_walk_opts_t* mfu_walk_opts_new(void)
 void mfu_walk_opts_delete(mfu_walk_opts_t** popts)
 {
   if (popts != NULL) {
-    mfu_walk_opts_t* opts = *popts;
     mfu_free(popts);
   }
 }
@@ -771,6 +770,28 @@ void mfu_flist_set_detail (mfu_flist bflist, int detail)
     return;
 }
 
+uint64_t mfu_flist_file_get_oid_low(mfu_flist bflist, uint64_t idx)
+{
+    uint64_t oid_low;
+    flist_t* flist = (flist_t*) bflist;
+    elem_t* elem = list_get_elem(flist, idx);
+    if (elem != NULL) {
+        oid_low = elem->obj_id_lo;
+    }
+    return oid_low;
+}
+
+uint64_t mfu_flist_file_get_oid_high(mfu_flist bflist, uint64_t idx)
+{
+    uint64_t oid_high;
+    flist_t* flist = (flist_t*) bflist;
+    elem_t* elem = list_get_elem(flist, idx);
+    if (elem != NULL) {
+        oid_high = elem->obj_id_hi;
+    }
+    return oid_high;
+}
+
 const char* mfu_flist_file_get_name(mfu_flist bflist, uint64_t idx)
 {
     const char* name = NULL;
@@ -1011,6 +1032,20 @@ void mfu_flist_file_set_oid(mfu_flist bflist, uint64_t idx, daos_obj_id_t oid)
     }
     return;
 }
+
+void mfu_flist_file_set_cont(mfu_flist bflist, uint64_t idx, const char* name)
+{
+    flist_t* flist = (flist_t*) bflist;
+    elem_t* elem = list_get_elem(flist, idx);
+    if (elem != NULL) {
+        /* free existing name if there is one */
+        mfu_free(&elem->file);
+
+        /* set new name */
+        elem->file = MFU_STRDUP(name);
+    }
+    return;
+}
 #endif 
 
 void mfu_flist_file_set_type(mfu_flist bflist, uint64_t idx, mfu_filetype type)
diff --git a/src/common/mfu_flist.h b/src/common/mfu_flist.h
index 0cebf3f..dd02888 100644
--- a/src/common/mfu_flist.h
+++ b/src/common/mfu_flist.h
@@ -378,6 +378,8 @@ mfu_filetype mfu_flist_mode_to_filetype(mode_t mode);
 
 /* read properties on specified item in local flist */
 /* always set */
+uint64_t mfu_flist_file_get_oid_low(mfu_flist flist, uint64_t index);
+uint64_t mfu_flist_file_get_oid_high(mfu_flist flist, uint64_t index);
 const char* mfu_flist_file_get_name(mfu_flist flist, uint64_t index);
 int mfu_flist_file_get_depth(mfu_flist flist, uint64_t index);
 mfu_filetype mfu_flist_file_get_type(mfu_flist flist, uint64_t index);
@@ -403,6 +405,7 @@ const char* mfu_flist_file_get_groupname(mfu_flist flist, uint64_t index);
 /* set properties on specified item in local flist */
 #ifdef DAOS_SUPPORT
 void mfu_flist_file_set_oid(mfu_flist flist, uint64_t index, daos_obj_id_t oid);
+void mfu_flist_file_set_cont(mfu_flist flist, uint64_t index, const char* name);
 #endif
 void mfu_flist_file_set_name(mfu_flist flist, uint64_t index, const char* name);
 void mfu_flist_file_set_type(mfu_flist flist, uint64_t index, mfu_filetype type);
diff --git a/src/common/mfu_flist_chmod.c b/src/common/mfu_flist_chmod.c
index 7f269ef..d1bd985 100644
--- a/src/common/mfu_flist_chmod.c
+++ b/src/common/mfu_flist_chmod.c
@@ -1311,8 +1311,6 @@ mfu_chmod_opts_t* mfu_chmod_opts_new(void)
 void mfu_chmod_opts_delete(mfu_chmod_opts_t** popts)
 {
   if (popts != NULL) {
-    mfu_chmod_opts_t* opts = *popts;
-
     mfu_free(popts);
   }
 }
diff --git a/src/common/mfu_flist_copy.c b/src/common/mfu_flist_copy.c
index 8a36bc9..f5acaab 100644
--- a/src/common/mfu_flist_copy.c
+++ b/src/common/mfu_flist_copy.c
@@ -18,6 +18,10 @@
 #include <time.h> /* asctime / localtime */
 #include <regex.h>
 
+#ifdef HAVE_LIBATTR
+#include <attr/libattr.h>
+#endif /* HAVE_LIBATTR */
+
 /* These headers are needed to query the Lustre MDS for stat
  * information.  This information may be incomplete, but it
  * is faster than a normal stat, which requires communication
@@ -155,10 +159,6 @@ static int mfu_copy_open_file(
     char* name = cache->name;
     if (name != NULL) {
         /* we have a cached file descriptor */
-        int fd = cache->fd;
-#ifdef DAOS_SUPPORT
-        dfs_obj_t* obj = cache->obj;
-#endif
         if (strcmp(name, file) == 0 && cache->read == read_flag) {
             /* the file we're trying to open matches name and read/write mode,
              * so just return the cached descriptor */
@@ -240,9 +240,6 @@ static int mfu_copy_close_file(
     char* name = cache->name;
     if (name != NULL) {
         int fd = cache->fd;
-#ifdef DAOS_SUPPORT
-        dfs_obj_t* obj = cache->obj;
-#endif
         /* if open for write, fsync */
         int read_flag = cache->read;
         if (! read_flag && mfu_file->type == POSIX) {
@@ -330,18 +327,40 @@ static int mfu_copy_xattrs(
     /* iterate over list and copy values to new object lgetxattr/lsetxattr */
     if(got_list) {
         char* name = list;
-
         while(name < list + list_size) {
             /* start with a reasonable buffer,
              * allocate something bigger as needed */
             size_t val_bufsize = 1024;
             void* val = (void*) MFU_MALLOC(val_bufsize);
+            int copy_xattr;
 
             /* lookup value for name */
             ssize_t val_size;
             int got_val = 0;
 
-            while(! got_val) {
+            copy_xattr = 1; /* copy unless indicated below not to */
+            if (copy_opts->copy_xattrs == XATTR_USE_LIBATTR) {
+#ifdef HAVE_LIBATTR
+                if (attr_copy_action(name, NULL) == ATTR_ACTION_SKIP) {
+                    copy_xattr = 0;
+                }
+#endif /* HAVE_LIBATTR */
+            } else if (copy_opts->copy_xattrs == XATTR_SKIP_LUSTRE) {
+                /* ignore xattrs lustre treats specially */
+                /* list from lustre source file lustre_idl.h */
+                if (    strncmp(name,"lustre.",strlen("lustre.")) == 0 ||
+                        strcmp(name,"trusted.som") == 0 || strcmp(name,"trusted.lov") == 0 ||
+                        strcmp(name,"trusted.lma") == 0 || strcmp(name,"trusted.lmv") == 0 ||
+                        strcmp(name,"trusted.dmv") == 0 || strcmp(name,"trusted.link") == 0 ||
+                        strcmp(name,"trusted.fid") == 0 || strcmp(name,"trusted.version") == 0 ||
+                        strcmp(name,"trusted.hsm") == 0 || strcmp(name,"trusted.lfsck_bitmap") == 0 ||
+                        strcmp(name,"trusted.dummy") == 0)
+                {
+                    copy_xattr = 0;
+                }
+            }
+
+            while(! got_val && copy_xattr) {
                 errno = 0;
                 if (copy_opts->dereference) {
                     /* getxattr of dereferenced symbolic links */
@@ -390,7 +409,7 @@ static int mfu_copy_xattrs(
             }
 
             /* set attribute on destination object */
-            if(got_val) {
+            if(got_val && copy_xattr) {
                 errno = 0;
                 /* lsetxattr of symbolic link itself. No need to dereference here */
                 int setrc = mfu_file_lsetxattr(dest_path, name, val, (size_t) val_size, 0, mfu_dst_file);
@@ -923,9 +942,9 @@ static int mfu_copy_set_metadata_dirs(
 
 /* creates dir in destpath for specified item, identifies source path
  * that contains source dir, computes relative path to dir under source path,
- * and creates dir at same relative path under destpath, copies xattrs
- * when preserving permissions, which contains file striping info on Lustre,
- * returns 0 on success and -1 on error */
+ * and creates dir at same relative path under destpath, optionally copies
+ * xattrs (which contain striping information under Lustre), optionally
+ * preserves permissions, returns 0 on success and -1 on error */
 static int mfu_create_directory(
     mfu_flist list,                 /* flist holding target directory */
     uint64_t idx,                   /* index of target directory within its list */
@@ -993,7 +1012,7 @@ static int mfu_create_directory(
      * creating / striping files in the directory */
 
     /* copy extended attributes on directory */
-    if (copy_opts->preserve) {
+    if (copy_opts->copy_xattrs != XATTR_COPY_NONE) {
         int tmp_rc = mfu_copy_xattrs(list, idx, dest_path, copy_opts, mfu_src_file, mfu_dst_file);
         if (tmp_rc < 0) {
             rc = -1;
@@ -1027,16 +1046,13 @@ static int mfu_create_directories(
     /* assume we'll succeed */
     int rc = 0;
 
-    /* determine whether we should print status messages */
-    int verbose = (mfu_debug_level >= MFU_LOG_VERBOSE);
-
     /* get current rank */
     int rank;
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
 
     /* count total number of directories to be created */
     int level;
-    uint64_t count = 0;
+    uint64_t mkdir_local_count = 0;
     for (level = 0; level < levels; level++) {
         /* get list of items for this level */
         mfu_flist list = lists[level];
@@ -1047,14 +1063,14 @@ static int mfu_create_directories(
            /* check whether we have a directory */
            mfu_filetype type = mfu_flist_file_get_type(list, idx);
            if (type == MFU_TYPE_DIR) {
-               count++;
+               mkdir_local_count++;
            }
         }
     }
 
     /* get total for print percent progress while creating */
     mkdir_total_count = 0;
-    MPI_Allreduce(&count, &mkdir_total_count, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);
+    MPI_Allreduce(&mkdir_local_count, &mkdir_total_count, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);
 
     /* bail early if there is no work to do */
     if (mkdir_total_count == 0) {
@@ -1078,7 +1094,6 @@ static int mfu_create_directories(
         /* create each directory we have at this level */
         uint64_t idx;
         uint64_t size = mfu_flist_size(list);
-        uint64_t count = 0;
         for (idx = 0; idx < size; idx++) {
             /* check whether we have a directory */
             mfu_filetype type = mfu_flist_file_get_type(list, idx);
@@ -1166,8 +1181,8 @@ static int mfu_create_link(
         }
     }
 
-    /* set permissions on link */
-    if (copy_opts->preserve) {
+    /* set xattrs on link */
+    if (copy_opts->copy_xattrs != XATTR_COPY_NONE) {
         int xattr_rc = mfu_copy_xattrs(list, idx, dest_path, copy_opts, mfu_src_file, mfu_dst_file);
         if (xattr_rc < 0) {
             rc = -1;
@@ -1185,9 +1200,9 @@ static int mfu_create_link(
 
 /* creates inode in destpath for specified file, identifies source path
  * that contains source file, computes relative path to file under source path,
- * and creates file at same relative path under destpath, copies xattrs
- * when preserving permissions, which contains file striping info on Lustre,
- * returns 0 on success and -1 on error */
+ * and creates file at same relative path under destpath, optionally copies
+ * xattrs (which contain striping information under Lustre), optionally
+ * preserves permissions, returns 0 on success and -1 on error */
 static int mfu_create_file(
     mfu_flist list,
     uint64_t idx,
@@ -1240,7 +1255,7 @@ static int mfu_create_file(
     /* copy extended attributes, important to do this first before
      * writing data because some attributes tell file system how to
      * stripe data, e.g., Lustre */
-    if (copy_opts->preserve) {
+    if (copy_opts->copy_xattrs != XATTR_COPY_NONE) {
         int tmp_rc = mfu_copy_xattrs(list, idx, dest_path, copy_opts, mfu_src_file, mfu_dst_file);
         if (tmp_rc < 0) {
             rc = -1;
@@ -1342,14 +1357,14 @@ static void create_progress_fn(const uint64_t* vals, int count, int complete, in
 
     /* compute percentage of items created */
     double percent = 0.0;
-    if (mkdir_total_count > 0) {
-        percent = (double)items * 100.0 / (double)mkdir_total_count;
+    if (mknod_total_count > 0) {
+        percent = (double)items * 100.0 / (double)mknod_total_count;
     }
 
     /* estimate seconds remaining */
     double secs_remaining = -1.0;
     if (item_rate > 0.0) {
-        secs_remaining = (double)(mkdir_total_count - items) / item_rate;
+        secs_remaining = (double)(mknod_total_count - items) / item_rate;
     }
 
     if (complete < ranks) {
@@ -1376,9 +1391,6 @@ static int mfu_create_files(
 {
     int rc = 0;
 
-    /* determine whether we should print status messages */
-    int verbose = (mfu_debug_level >= MFU_LOG_VERBOSE);
-
     /* get current rank */
     int rank;
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
@@ -1480,16 +1492,13 @@ static int mfu_create_hardlinks(
 {
     int rc = 0;
 
-    /* determine whether we should print status messages */
-    int verbose = (mfu_debug_level >= MFU_LOG_VERBOSE);
-
     /* get current rank */
     int rank;
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
 
     /* first, count number of items to create in the list of the current process */
     int level;
-    uint64_t count = 0;
+    uint64_t mknod_local_count = 0;
     for (level = 0; level < levels; level++) {
         /* get list of items for this level */
         mfu_flist list = lists[level];
@@ -1500,14 +1509,14 @@ static int mfu_create_hardlinks(
             /* count regular files */
             mfu_filetype type = mfu_flist_file_get_type(list, idx);
             if (type == MFU_TYPE_FILE) {
-                count++;
+                mknod_local_count++;
             }
         }
     }
 
     /* get total for print percent progress while creating */
     mknod_total_count = 0;
-    MPI_Allreduce(&count, &mknod_total_count, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);
+    MPI_Allreduce(&mknod_local_count, &mknod_total_count, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);
 
     /* bail early if there is no work to do */
     if (mknod_total_count == 0) {
@@ -1530,7 +1539,6 @@ static int mfu_create_hardlinks(
         /* iterate over items and create hardlink for each */
         uint64_t idx;
         uint64_t size = mfu_flist_size(list);
-        uint64_t count = 0;
         for (idx = 0; idx < size; idx++) {
             /* get type of item */
             mfu_filetype type = mfu_flist_file_get_type(list, idx);
@@ -3100,23 +3108,19 @@ int mfu_flist_hardlink(
                       mfu_copy_stats.wtime_started;
 
     /* prep our values into buffer */
-    int64_t values[5];
+    int64_t values[3];
     values[0] = mfu_copy_stats.total_dirs;
     values[1] = mfu_copy_stats.total_files;
     values[2] = mfu_copy_stats.total_links;
-    values[3] = mfu_copy_stats.total_size;
-    values[4] = mfu_copy_stats.total_bytes_copied;
 
     /* sum values across processes */
-    int64_t sums[5];
-    MPI_Allreduce(values, sums, 5, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);
+    int64_t sums[3];
+    MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);
 
     /* extract results from allreduce */
     int64_t agg_dirs   = sums[0];
     int64_t agg_files  = sums[1];
     int64_t agg_links  = sums[2];
-    int64_t agg_size   = sums[3];
-    int64_t agg_copied = sums[4];
 
     if(rank == 0) {
         /* format start time */
@@ -3220,8 +3224,8 @@ mfu_file_t* mfu_file_new(void)
     mfile->fd         = -1;
 #ifdef DAOS_SUPPORT
     mfile->obj        = NULL;
+    mfile->dfs_sys    = NULL;
     mfile->dfs        = NULL;
-    mfile->dfs_hash   = NULL;
 #endif
     return mfile;
 }
@@ -3229,7 +3233,6 @@ mfu_file_t* mfu_file_new(void)
 void mfu_file_delete(mfu_file_t** pfile)
 {
   if (pfile != NULL) {
-    mfu_file_t* mfile = *pfile;
     mfu_free(pfile);
   }
 }
@@ -3254,6 +3257,9 @@ mfu_copy_opts_t* mfu_copy_opts_new(void)
     /* By default, don't bother to preserve all attributes. */
     opts->preserve = false;
 
+    /* By default, do not copy special to Lustre (which set striping) */
+    opts->copy_xattrs = XATTR_SKIP_LUSTRE;
+
     /* By default, don't dereference source symbolic links. 
      * This is not a perfect opposite of no_dereference */
     opts->dereference = 0;
diff --git a/src/common/mfu_flist_create.c b/src/common/mfu_flist_create.c
index 36290ce..3455e1a 100644
--- a/src/common/mfu_flist_create.c
+++ b/src/common/mfu_flist_create.c
@@ -83,9 +83,6 @@ void mfu_flist_mkdir(mfu_flist flist, mfu_create_opts_t* opts)
 {
     int rc = 0;
 
-    /* determine whether we should print status messages */
-    int verbose = (mfu_debug_level <= MFU_LOG_INFO);
-
     /* get current rank */
     int rank;
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
@@ -293,11 +290,6 @@ static int create_file(mfu_flist list, uint64_t idx, mfu_create_opts_t* opts)
 /* create inodes for all regular files in flist, assumes directories exist */
 void mfu_flist_mknod(mfu_flist flist, mfu_create_opts_t* opts)
 {
-    int rc = 0;
-
-    /* determine whether we should print status messages */
-    int verbose = (mfu_debug_level <= MFU_LOG_INFO);
-
     /* get current rank */
     int rank;
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
diff --git a/src/common/mfu_flist_walk.c b/src/common/mfu_flist_walk.c
index fd30da2..120976e 100644
--- a/src/common/mfu_flist_walk.c
+++ b/src/common/mfu_flist_walk.c
@@ -93,6 +93,35 @@ static void reduce_fini(const void* buf, size_t size)
     MFU_LOG(MFU_LOG_INFO, "Walked %llu items in %.3lf secs (%.3lf items/sec) ...", val, secs, rate);
 }
 
+/****************************************
+ * Global helper functions
+ ***************************************/
+
+/** Build a full path from a dirname and basename in the form:
+ * <dir> + '/' + <name> + '/0'
+ * up to path_len long.
+ * Returns 0 on success and -1 if the new path is too long. */
+static int build_path(char* path, size_t path_len, const char* dir, const char* name)
+{
+    size_t dir_len = strlen(dir);
+
+    /* Only separate with a '/' if the dir does not have a trailing slash.
+     * Builds a path to at most path_len long. */
+    int new_len;
+    if ((dir_len > 0) && (dir[dir_len - 1] == '/')) {
+        new_len = snprintf(path, path_len, "%s%s", dir, name);
+    } else {
+        new_len = snprintf(path, path_len, "%s/%s", dir, name);
+    }
+    if (new_len > path_len) {
+        MFU_LOG(MFU_LOG_ERR, "Path name is too long, %lu chars exceeds limit %lu: '%s/%s'",
+                new_len, path_len, dir, name);
+        return -1;
+    }
+
+    return 0;
+}
+
 #ifdef LUSTRE_SUPPORT
 /****************************************
  * Walk directory tree using Lustre's MDS stat
@@ -191,13 +220,8 @@ static void walk_getdents_process_dir(const char* dir, CIRCLE_handle* handle)
                 /* check whether we can define path to item:
                  * <dir> + '/' + <name> + '/0' */
                 char newpath[CIRCLE_MAX_STRING_LEN];
-                size_t len = strlen(dir) + 1 + strlen(name) + 1;
-                if (len < sizeof(newpath)) {
-                    /* build full path to item */
-                    strcpy(newpath, dir);
-                    strcat(newpath, "/");
-                    strcat(newpath, name);
-
+                int rc = build_path(newpath, CIRCLE_MAX_STRING_LEN, dir, name);
+                if (rc == 0) {
                     /* get type of item */
                     char d_type = *(buf + bpos + d->d_reclen - 1);
 
@@ -239,9 +263,6 @@ static void walk_getdents_process_dir(const char* dir, CIRCLE_handle* handle)
                         reduce_items++;
                     }
                 }
-                else {
-                    MFU_LOG(MFU_LOG_ERR, "Path name is too long, %lu chars exceeds limit %lu: '%s/%s'", len, sizeof(newpath), dir, name);
-                }
             }
 
             /* advance to next record */
@@ -314,6 +335,7 @@ static void walk_readdir_process_dir(const char* dir, CIRCLE_handle* handle)
             struct stat st;
             mfu_file_t* mfu_file = *CURRENT_PFILE;
             int status = mfu_file_lstat(dir, &st, mfu_file);
+            // TODO handle status != 0
             // turn on the usr read & execute bits
             st.st_mode |= S_IRUSR;
             st.st_mode |= S_IXUSR;
@@ -340,13 +362,8 @@ static void walk_readdir_process_dir(const char* dir, CIRCLE_handle* handle)
             if ((strncmp(name, ".", 2)) && (strncmp(name, "..", 3))) {
                 /* <dir> + '/' + <name> + '/0' */
                 char newpath[CIRCLE_MAX_STRING_LEN];
-                size_t len = strlen(dir) + 1 + strlen(name) + 1;
-                if (len < sizeof(newpath)) {
-                    /* build full path to item */
-                    strcpy(newpath, dir);
-                    strcat(newpath, "/");
-                    strcat(newpath, name);
-
+                int rc = build_path(newpath, CIRCLE_MAX_STRING_LEN, dir, name);
+                if (rc == 0) {
 #ifdef _DIRENT_HAVE_D_TYPE
                     /* record info for item */
                     mode_t mode;
@@ -393,11 +410,6 @@ static void walk_readdir_process_dir(const char* dir, CIRCLE_handle* handle)
                     }
 #endif
                 }
-                else {
-                    /* TODO: print error in correct format */
-                    /* name is too long */
-                    MFU_LOG(MFU_LOG_ERR, "Path name is too long, %lu chars exceeds limit %lu: '%s/%s'", len, sizeof(newpath), dir, name);
-                }
             }
         }
     }
@@ -477,20 +489,11 @@ static void walk_stat_process_dir(char* dir, CIRCLE_handle* handle)
             if ((strncmp(name, ".", 2)) && (strncmp(name, "..", 3))) {
                 /* <dir> + '/' + <name> + '/0' */
                 char newpath[CIRCLE_MAX_STRING_LEN];
-                size_t len = strlen(dir) + 1 + strlen(name) + 1;
-                if (len < sizeof(newpath)) {
-                    /* build full path to item */
-                    strcpy(newpath, dir);
-                    strcat(newpath, "/");
-                    strcat(newpath, name);
-
+                int rc = build_path(newpath, CIRCLE_MAX_STRING_LEN, dir, name);
+                if (rc == 0) {
                     /* add item to queue */
                     handle->enqueue(newpath);
                 }
-                else {
-                    /* name is too long */
-                    MFU_LOG(MFU_LOG_ERR, "Path name is too long, %lu chars exceeds limit %lu: '%s/%s'", len, sizeof(newpath), dir, name);
-                }
             }
         }
     }
diff --git a/src/common/mfu_io.c b/src/common/mfu_io.c
index cd86d5d..f924ec4 100644
--- a/src/common/mfu_io.c
+++ b/src/common/mfu_io.c
@@ -14,248 +14,14 @@
 #include <assert.h>
 #include <libgen.h>
 
-#ifdef DAOS_SUPPORT
-#include <gurt/common.h>
-#include <gurt/hash.h>
-#endif
-
 #include "mfu.h"
+#include "mfu_errors.h"
 
 #define MFU_IO_TRIES  (5)
 #define MFU_IO_USLEEP (100)
 
 static int mpi_rank;
 
-#ifdef DAOS_SUPPORT
-/* Handle for a hash table entry */
-struct daos_dir_hdl {
-    d_list_t    entry;
-    dfs_obj_t*  oh;
-    char*       name;
-};
-
-/* Return a newly allocated daos_dir_hdl structure */
-static struct daos_dir_hdl* daos_dir_hdl_new(void)
-{
-    struct daos_dir_hdl* hdl = (struct daos_dir_hdl*) MFU_MALLOC(sizeof(struct daos_dir_hdl));
-    hdl->oh = NULL;
-    hdl->name = NULL;
-
-    return hdl;
-}
-
-/* free a daos_dir_hdl structure */
-static void daos_dir_hdl_delete(struct daos_dir_hdl** phdl)
-{
-    if (phdl != NULL) {
-        struct daos_dir_hdl* hdl = *phdl;
-        if (hdl->oh != NULL) {
-            dfs_release(hdl->oh);
-        }
-        mfu_free(&hdl->name);
-        mfu_free(phdl);
-    }
-}
-
-/* Get the daos_dir_hdl from its entry */
-static inline struct daos_dir_hdl* hdl_obj(d_list_t* rlink)
-{
-    return container_of(rlink, struct daos_dir_hdl, entry);
-}
-
-/* Simple string comparison of hdl->name as the key */
-static bool key_cmp(struct d_hash_table* htable, d_list_t* rlink, 
-        const void* key, unsigned int ksize)
-{
-    struct daos_dir_hdl* hdl = hdl_obj(rlink);
-
-    return (strcmp(hdl->name, (const char *)key) == 0);
-}
-
-/* Since we only delete entries when we are finished with them,
- * this should always return true so rec_free is called */
-static bool rec_decref(struct d_hash_table* htable, d_list_t* rlink)
-{
-    return true;
-}
-
-/* Free a hash entry. Called when the table is destroyed */
-static void rec_free(struct d_hash_table* htable, d_list_t* rlink)
-{
-    struct daos_dir_hdl* hdl = hdl_obj(rlink);
-
-    assert(d_hash_rec_unlinked(&hdl->entry));
-    daos_dir_hdl_delete(&hdl);
-}
-
-/* Operations for the hash table */
-static d_hash_table_ops_t hdl_hash_ops = {
-    .hop_key_cmp    = key_cmp,
-    .hop_rec_decref = rec_decref,
-    .hop_rec_free   = rec_free
-};
-
-/* Caches calls to dfs_lookup and returns lookups from the cache.
- * On error, sets errno and returns NULL */
-static dfs_obj_t* daos_hash_lookup(const char* name, mfu_file_t* mfu_file)
-{
-    struct daos_dir_hdl* hdl;
-    d_list_t* rlink;
-    int rc;
-
-    /* Make sure the hash is initialized */
-    if (mfu_file->dfs_hash == NULL) {
-        rc = d_hash_table_create(D_HASH_FT_NOLOCK, 16, NULL, &hdl_hash_ops, &mfu_file->dfs_hash);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "Failed to initialize dir hashtable");
-            errno = ENOMEM;
-            return NULL;
-        }
-    }
-
-    /* If cached, return it */
-    rlink = d_hash_rec_find(mfu_file->dfs_hash, name, strlen(name));
-    if (rlink != NULL) {
-        hdl = hdl_obj(rlink);
-        return hdl->oh;
-    }
-
-    /* Create a new entry */
-    hdl = daos_dir_hdl_new();
-    if (hdl == NULL) {
-        MFU_LOG(MFU_LOG_ERR, "Failed to initialze hash entry");
-        daos_dir_hdl_delete(&hdl);
-        errno = ENOMEM;
-        return NULL;
-    }
-
-    /* Allocate space for name, up to PATH_MAX,
-     * leaving 1 extra for the null terminator */
-    size_t name_len = strnlen(name, PATH_MAX);
-    if (name_len > PATH_MAX-1) {
-        daos_dir_hdl_delete(&hdl);
-        errno = ENAMETOOLONG;
-        return NULL;
-    }
-    hdl->name = MFU_STRDUP(name);
-
-    /* Lookup the object handle */
-    rc = dfs_lookup(mfu_file->dfs, name, O_RDWR, &hdl->oh, NULL, NULL);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_lookup() of %s Failed", name);
-        daos_dir_hdl_delete(&hdl);
-        errno = rc;
-        return NULL;
-    }
-
-    /* Store this entry in the hash.
-     * Since we have already called d_hash_rec_find,
-     * pass exclusive=false to avoid another find being called */
-    rc = d_hash_rec_insert(mfu_file->dfs_hash, hdl->name, name_len,
-                            &hdl->entry, false);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "Failed to insert hash entry");
-        daos_dir_hdl_delete(&hdl);
-        errno = ENOMEM;
-        return NULL;
-    }
-
-    /* Return the object */
-    return hdl->oh;
-}
-
-static int parse_filename(const char* path, char** _obj_name, char** _cont_name)
-{
-	char *f1 = NULL;
-	char *f2 = NULL;
-	char *fname = NULL;
-	char *cont_name = NULL;
-	int rc = 0;
-
-	if (path == NULL || _obj_name == NULL || _cont_name == NULL)
-		return -EINVAL;
-
-	if (strcmp(path, "/") == 0) {
-		*_cont_name = strdup("/");
-		if (*_cont_name == NULL)
-			return -ENOMEM;
-		*_obj_name = NULL;
-		return 0;
-	}
-
-	f1 = strdup(path);
-	if (f1 == NULL) {
-                rc = -ENOMEM;
-                goto out;
-        }
-
-	f2 = strdup(path);
-	if (f2 == NULL) {
-                rc = -ENOMEM;
-                goto out;
-        }
-
-	fname = basename(f1);
-	cont_name = dirname(f2);
-
-	if (cont_name[0] == '.' || cont_name[0] != '/') {
-		char cwd[1024];
-
-		if (getcwd(cwd, 1024) == NULL) {
-                        rc = -ENOMEM;
-                        goto out;
-                }
-
-		if (strcmp(cont_name, ".") == 0) {
-			cont_name = strdup(cwd);
-			if (cont_name == NULL) {
-                                rc = -ENOMEM;
-                                goto out;
-                        }
-		} else {
-			char *new_dir = calloc(strlen(cwd) + strlen(cont_name)
-					       + 1, sizeof(char));
-			if (new_dir == NULL) {
-                                rc = -ENOMEM;
-                                goto out;
-                        }
-
-			strcpy(new_dir, cwd);
-			if (cont_name[0] == '.') {
-				strcat(new_dir, &cont_name[1]);
-			} else {
-				strcat(new_dir, "/");
-				strcat(new_dir, cont_name);
-			}
-			cont_name = new_dir;
-		}
-		*_cont_name = cont_name;
-	} else {
-		*_cont_name = strdup(cont_name);
-		if (*_cont_name == NULL) {
-                        rc = -ENOMEM;
-                        goto out;
-                }
-	}
-
-	*_obj_name = strdup(fname);
-	if (*_obj_name == NULL) {
-		free(*_cont_name);
-		*_cont_name = NULL;
-                rc = -ENOMEM;
-                goto out;
-	}
-
-out:
-	if (f1)
-		free(f1);
-	if (f2)
-		free(f2);
-	return rc;
-}
-
-#endif /* DAOS_SUPPORT */
-
 /* calls access, and retries a few times if we get EIO or EINTR */
 int mfu_file_access(const char* path, int amode, mfu_file_t* mfu_file)
 {
@@ -294,33 +60,10 @@ retry:
 int daos_access(const char* path, int amode, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else {
-        rc = dfs_access(mfu_file->dfs, parent, name, amode);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_access %s failed (%d %s)",
-                    name, rc, strerror(rc));
-            errno = rc;
-            rc = -1;
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-    
-    return rc;
+    int rc = dfs_sys_access(mfu_file->dfs_sys, path, amode, 0);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -365,69 +108,14 @@ int daos_faccessat(int dirfd, const char* path, int amode, int flags, mfu_file_t
 #ifdef DAOS_SUPPORT
     /* Only current working directory supported at this time */
     if (dirfd != AT_FDCWD) {
-        errno = ENOTSUP;
-        return -1;
+        return mfu_errno2rc(ENOTSUP);
     }
 
-    /* Only real user and group IDs supported at this time */
-    if (flags & AT_EACCESS) {
-        errno = ENOTSUP;
-        return -1;
-    }
-
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else {
-        /* Get the mode of the object */
-        mode_t mode;
-        dfs_obj_t* obj;
-        int lookup_flags = O_RDWR;
-        if (flags & AT_SYMLINK_NOFOLLOW) {
-            lookup_flags |= O_NOFOLLOW;
-        }
-        rc = dfs_lookup_rel(mfu_file->dfs, parent, name, lookup_flags, &obj, &mode, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s/%s failed", dir_name, name);
-            errno = rc;
-            rc = -1;
-        } else {
-            /* return success for links, since dfs_access does not have proper support */
-            if (!S_ISLNK(mode)) {
-                rc = dfs_access(mfu_file->dfs, parent, name, amode);
-                if (rc) {
-                    MFU_LOG(MFU_LOG_ERR, "dfs_access %s failed (%d %s)",
-                            name, rc, strerror(rc));
-                    errno = rc;
-                    rc = -1;
-                }
-            }
-
-            /* Release the obj */
-            int tmp_rc = dfs_release(obj);
-            if (tmp_rc && (rc != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                        name, tmp_rc, strerror(tmp_rc));
-                errno = tmp_rc;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int access_flags = (flags & AT_SYMLINK_NOFOLLOW) ? O_NOFOLLOW : 0;
+    int rc = dfs_sys_access(mfu_file->dfs_sys, path, amode, access_flags);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -468,44 +156,18 @@ retry:
 
 int daos_lchown(const char* path, uid_t owner, gid_t group, mfu_file_t* mfu_file)
 {
-#ifdef DAOS_SUPPORT
     /* At this time, DFS does not support updating the uid or gid.
      * These are set at the container level, not file level */
-    return 0;
-#else
-    return 0;
-#endif
+    return mfu_errno2rc(0);
 }
 
 int daos_chmod(const char *path, mode_t mode, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else {
-        rc = dfs_chmod(mfu_file->dfs, parent, name, mode);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_chmod %s failed (%d %s)",
-                    name, rc, strerror(rc));
-            errno = rc;
-            rc = -1;
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_chmod(mfu_file->dfs_sys, path, mode);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -587,108 +249,24 @@ int daos_utimensat(int dirfd, const char* pathname, const struct timespec times[
 #ifdef DAOS_SUPPORT
     /* Only current working directory supported at this time */
     if (dirfd != AT_FDCWD) {
-        errno = ENOTSUP;
-        return -1;
+        return mfu_errno2rc(ENOTSUP);
     }
 
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(pathname, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else if (name) {
-        /* TODO DAOS properly handle when name is NULL. I.e. root /
-         * For now, just skip if root, since a proper fix needs to be
-         * done in several functions, and these functions will soon be moved
-         * internal to DAOS anyway, in which case they will be refactored and fixed. */
-
-        /* Lookup the object */
-        dfs_obj_t* obj;
-        int lookup_flags = O_RDWR;
-        if (flags & AT_SYMLINK_NOFOLLOW) {
-            lookup_flags |= O_NOFOLLOW;
-        }
-        rc = dfs_lookup_rel(mfu_file->dfs, parent, name, lookup_flags, &obj, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed", pathname);
-            errno = rc;
-            rc = -1;
-        } else {
-            /* Set the times on the obj */
-            if (rc != -1) {
-                struct stat stbuf;
-                stbuf.st_atim = times[0];
-                stbuf.st_mtim = times[1];
-                rc = dfs_osetattr(mfu_file->dfs, obj, &stbuf, DFS_SET_ATTR_ATIME | DFS_SET_ATTR_MTIME);
-                if (rc) {
-                    MFU_LOG(MFU_LOG_ERR, "dfs_osetattr %s failed", pathname);
-                    errno = rc;
-                    rc = -1;
-                }
-            }
-
-            /* Release the obj */
-            int tmp_rc = dfs_release(obj);
-            if (tmp_rc && (rc != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                        pathname, tmp_rc, strerror(tmp_rc));
-                errno = tmp_rc;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int time_flags = (flags & AT_SYMLINK_NOFOLLOW) ? O_NOFOLLOW : 0;
+    int rc = dfs_sys_utimens(mfu_file->dfs_sys, pathname, times, time_flags);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
-/* Since dfs_stat() performs like lstat(), this is emulated. */
 int daos_stat(const char* path, struct stat* buf, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-        goto out_free_name;
-    }
-
-    /* Lookup name within the parent */
-    dfs_obj_t* obj;
-    rc = dfs_lookup_rel(mfu_file->dfs, parent, name, O_RDWR, &obj, NULL, buf);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s / %s failed", dir_name, name);
-        errno = rc;
-        rc = -1;
-        goto out_free_name;
-    }
-
-    dfs_release(obj);
-out_free_name:
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_stat(mfu_file->dfs_sys, path, 0, buf);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -730,35 +308,10 @@ int mfu_file_stat(const char* path, struct stat* buf, mfu_file_t* mfu_file)
 int daos_lstat(const char* path, struct stat* buf, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else {
-        /* Stat the path.
-         * dfs_stat interrogates the link itself */
-        rc = dfs_stat(mfu_file->dfs, parent, name, buf);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_stat %s failed (%d %s)",
-                    name, rc, strerror(rc));
-            errno = rc;
-            rc = -1;
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc =  dfs_sys_stat(mfu_file->dfs_sys, path, O_NOFOLLOW, buf);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -817,63 +370,13 @@ retry:
     return rc;
 }
 
-/* emulates mknod with dfs_open, dfs_release */
-int daos_mknod(const char* path, mode_t mode, dev_t dev, mfu_file_t* mfu_file)
+int daos_mknod(const char* path, mode_t mode, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    /* Only regular files are supported at this time */
-    mode_t dfs_mode = mode | S_IFREG;
-    mode_t filetype = dfs_mode & S_IFMT;
-    if (filetype != S_IFREG) {
-        MFU_LOG(MFU_LOG_ERR, "Invalid entry type (not a file)");
-        errno = EINVAL;
-        return -1;
-    }
-
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    }
-    else {
-        /* create regular file */
-        rc = dfs_open(mfu_file->dfs, parent, name,
-                      dfs_mode, O_CREAT | O_EXCL,
-                      0, 0, NULL, &(mfu_file->obj));
-        if (rc) {
-            /* Avoid excessive logging */
-            if (rc != EEXIST) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_open %s failed (%d %s)",
-                        name, rc, strerror(rc));
-            }
-            errno = rc;
-            rc = -1;
-        }
-        else {
-            /* close the file */
-            rc = dfs_release(mfu_file->obj);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_mknod(mfu_file->dfs_sys, path, mode, 0, 0);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -904,7 +407,7 @@ int mfu_file_mknod(const char* path, mode_t mode, dev_t dev, mfu_file_t* mfu_fil
         int rc = mfu_mknod(path, mode, dev);
         return rc;
     } else if (mfu_file->type == DFS) {
-        int rc = daos_mknod(path, mode, dev, mfu_file);
+        int rc = daos_mknod(path, mode, mfu_file);
         return rc;
     } else {
         MFU_ABORT(-1, "File type not known: %s type=%d",
@@ -950,44 +453,10 @@ retry:
 int daos_remove(const char* path, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Can't delete root */
-    if (name == NULL) {
-        errno = EINVAL;
-        rc = -1;
-        goto out;
-    }
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-        goto out;
-    }
-
-    /* Delete the obj */
-    rc = dfs_remove(mfu_file->dfs, parent, name, false, NULL);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_remove %s failed (%d %s)",
-                name, rc, strerror(rc));
-        errno = rc;
-        rc = -1;
-        goto out;
-    }
-
-out:
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_remove(mfu_file->dfs_sys, path, false, NULL);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1018,6 +487,7 @@ char* daos_realpath(const char* path, char* resolved_path, mfu_file_t* mfu_file)
     /* There is currently not a reasonable way to do this */
     return NULL;
 #else
+    errno = ENOSYS;
     return NULL;
 #endif
 }
@@ -1026,60 +496,17 @@ char* daos_realpath(const char* path, char* resolved_path, mfu_file_t* mfu_file)
  * Links
  ****************************/
 
-/* emulates readlink with dfs_lookup, dfs_get_symlink_value */
 ssize_t daos_readlink(const char* path, char* buf, size_t bufsize, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    daos_size_t got_size = (daos_size_t) bufsize;
-
-    /* Lookup the parent directory first, since it is likely cached */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        got_size = -1;
-    } else { 
-        /* Lookup the symlink within the parent */
-        dfs_obj_t* sym_obj;
-        int lookup_flags = O_RDWR | O_NOFOLLOW;
-        int rc = dfs_lookup_rel(mfu_file->dfs, parent, name, lookup_flags, &sym_obj, NULL, NULL);
-        if (sym_obj == NULL) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed", path);
-            errno = rc;
-            got_size = -1;
-        } else {
-            /* Read the symlink value. This also makes sure it is S_IFLNK */
-            rc = dfs_get_symlink_value(sym_obj, buf, &got_size);
-            if (rc) {
-                errno = rc;
-                got_size = -1;
-            } else {
-                /* got_size includes the NULL terminator, but mfu_file_readlink
-                * expects that it does not */
-                got_size--;
-            }
-
-            /* Release the symlink */
-            rc = dfs_release(sym_obj);
-            if (rc && (got_size != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-        }
+    daos_size_t got_size = bufsize;
+    int rc = dfs_sys_readlink(mfu_file->dfs_sys, path, buf, &got_size);
+    if (rc != 0) {
+        errno = rc;
     }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
     return (ssize_t) got_size;
-
 #else
-    return (ssize_t) 0;
+    return (ssize_t) mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1124,48 +551,10 @@ ssize_t mfu_file_readlink(const char* path, char* buf, size_t bufsize, mfu_file_
 int daos_symlink(const char* oldpath, const char* newpath, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(newpath, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else {
-        /* open/create the symlink */
-        rc = dfs_open(mfu_file->dfs, parent, name,
-                      S_IFLNK, O_CREAT | O_EXCL,
-                      0, 0, oldpath, &(mfu_file->obj));
-        if (rc) {
-            /* Avoid excessive logging */
-            if (rc != EEXIST) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_open %s failed (%d %s)",
-                        name, rc, strerror(rc));
-            }
-            errno = rc;
-            rc = -1;
-        } else {
-            /* close the symlink */
-            rc = dfs_release(mfu_file->obj);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                        newpath, rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_symlink(mfu_file->dfs_sys, oldpath, newpath);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1233,70 +622,10 @@ retry:
 int daos_open(const char* file, int flags, mode_t mode, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    /* Only regular files are supported at this time */
-    mode_t dfs_mode = mode | S_IFREG;
-    if (!S_ISREG(dfs_mode)) {
-        MFU_LOG(MFU_LOG_ERR, "Invalid entry type (not a file)");
-        errno = EINVAL;
-        return -1;
-    }
-
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(file, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } else {
-        mode_t obj_mode;
-        rc = dfs_lookup_rel(mfu_file->dfs, parent, name, O_RDWR, &(mfu_file->obj),
-                            &obj_mode, NULL);
-        if (!rc) {
-            /* lookup found an obj */
-            if (flags & O_CREAT && flags & O_EXCL) {
-                /* ... but it should not have */
-                dfs_release(mfu_file->obj);
-                errno = EEXIST;
-                rc = -1;
-            } else if (!S_ISREG(obj_mode)) {
-                /* ... but the obj isn't a file */
-                dfs_release(mfu_file->obj);
-                MFU_LOG(MFU_LOG_ERR, "Invalid entry type (not a file)");
-                errno = EINVAL;
-                rc = -1;
-            } else {
-                /* good */
-            }
-        } else if (rc == ENOENT && flags & O_CREAT) {
-            /* call dfs_open so it can be created */
-            rc = dfs_open(mfu_file->dfs, parent, name,
-                          dfs_mode, flags,
-                          0, 0, NULL, &(mfu_file->obj));
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_open %s failed (%d %s)",
-                        name, rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-        } else if (rc) {
-            /* this is actually an error */
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s/%s failed", dir_name, name);
-            errno = rc;
-            rc = -1;
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_open(mfu_file->dfs_sys, file, mode, flags, 0, 0, NULL, &(mfu_file->obj));
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1396,16 +725,13 @@ int mfu_file_open(const char* file, int flags, mfu_file_t* mfu_file, ...)
 int daos_close(const char* file, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    int rc = dfs_release(mfu_file->obj);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_close %s failed (%d %s)",
-                file, rc, strerror(rc));
-        errno = rc;
-        rc = -1;
+    int rc = dfs_sys_close(mfu_file->obj);
+    if (rc == 0) {
+        mfu_file->obj = NULL;
     }
-    return rc;
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1439,11 +765,6 @@ int mfu_file_close(const char* file, mfu_file_t* mfu_file)
         return rc;
     } else if (mfu_file->type == DFS) {
         int rc = daos_close(file, mfu_file);
-#ifdef DAOS_SUPPORT
-        if (rc == 0) {
-            mfu_file->obj = NULL;
-        }
-#endif
         return rc;
     } else {
         MFU_ABORT(-1, "File type not known: %s type=%d",
@@ -1461,7 +782,7 @@ int daos_lseek(const char* file, mfu_file_t* mfu_file, off_t pos, int whence)
     }
     return 0;
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1517,32 +838,18 @@ ssize_t mfu_file_read(const char* file, void* buf, size_t size, mfu_file_t* mfu_
 ssize_t daos_read(const char* file, void* buf, size_t size, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    /* record address and size of user buffer in io vector */
-    d_iov_t iov;
-    d_iov_set(&iov, buf, size);
-
-    /* define scatter-gather list for dfs_read */
-    d_sg_list_t sgl;
-    sgl.sg_nr = 1;
-    sgl.sg_iovs = &iov;
-    sgl.sg_nr_out = 1;
-
-    /* execute read operation */
-    daos_size_t got_size;
-    int rc = dfs_read(mfu_file->dfs, mfu_file->obj, &sgl, mfu_file->offset, &got_size, NULL); 
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_read %s failed (%d %s)",
-                file, rc, strerror(rc));
+    daos_size_t got_size = size;
+    int rc = dfs_sys_read(mfu_file->dfs_sys, mfu_file->obj, buf, mfu_file->offset, &got_size, NULL);
+    if (rc != 0) {
         errno = rc;
         return -1;
+    } else {
+        /* update file pointer with number of bytes read */
+        mfu_file->offset += (daos_off_t)got_size;
     }
-
-    /* update file pointer with number of bytes read */
-    mfu_file->offset += (daos_off_t)got_size;
-
     return (ssize_t)got_size;
 #else
-    return (ssize_t)0;
+    return (ssize_t)mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1635,32 +942,18 @@ ssize_t mfu_write(const char* file, int fd, const void* buf, size_t size)
 ssize_t daos_write(const char* file, const void* buf, size_t size, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    /* record address and size of user buffer in io vector */
-    d_iov_t iov;
-    d_iov_set(&iov, (void*) buf, size);
-
-    /* define scatter-gather list for dfs_write */
-    d_sg_list_t sgl;
-    sgl.sg_nr = 1;
-    sgl.sg_iovs = &iov;
-    sgl.sg_nr_out = 1;
-
-    /* execute write operation,
-     * dfs_write writes all bytes if there is no error */
-    int rc = dfs_write(mfu_file->dfs, mfu_file->obj, &sgl, mfu_file->offset, NULL); 
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_write %s failed (%d %s)",
-                file, rc, strerror(rc));
+    daos_size_t write_size = size;
+    int rc = dfs_sys_write(mfu_file->dfs_sys, mfu_file->obj, buf, mfu_file->offset, &write_size, NULL);
+    if (rc != 0) {
         errno = rc;
         return -1;
+    } else {
+        /* update file pointer with number of bytes written */
+        mfu_file->offset += write_size;
     }
-
-    /* update file pointer with number of bytes written */
-    mfu_file->offset += (daos_off_t)size;
-
-    return (ssize_t)size;
+    return (ssize_t)write_size;
 #else
-    return (ssize_t)0;
+    return (ssize_t)mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1682,29 +975,16 @@ ssize_t mfu_file_pread(const char* file, void* buf, size_t size, off_t offset, m
 ssize_t daos_pread(const char* file, void* buf, size_t size, off_t offset, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    /* record address and size of user buffer in io vector */
-    d_iov_t iov;
-    d_iov_set(&iov, buf, size);
-
-    /* define scatter-gather list for dfs_read */
-    d_sg_list_t sgl;
-    sgl.sg_nr = 1;
-    sgl.sg_iovs = &iov;
-    sgl.sg_nr_out = 1;
-
-    /* execute read operation */
-    daos_size_t got_size;
-    int rc = dfs_read(mfu_file->dfs, mfu_file->obj, &sgl, offset, &got_size, NULL); 
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_read %s failed (%d %s)",
-            file, rc, strerror(rc));
+    daos_size_t got_size = size;
+    int rc = dfs_sys_read(mfu_file->dfs_sys, mfu_file->obj, buf, offset, &got_size, NULL);
+    if (rc != 0) {
         errno = rc;
+        /* return -1 if dfs_sys_read encounters error */
         return -1;
     }
-
     return (ssize_t)got_size;
 #else
-    return (ssize_t)0;
+    return (ssize_t)mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1781,29 +1061,16 @@ ssize_t mfu_pwrite(const char* file, int fd, const void* buf, size_t size, off_t
 ssize_t daos_pwrite(const char* file, const void* buf, size_t size, off_t offset, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    /* record address and size of user buffer in io vector */
-    d_iov_t iov;
-    d_iov_set(&iov, (void*) buf, size);
-
-    /* define scatter-gather list for dfs_write */
-    d_sg_list_t sgl;
-    sgl.sg_nr = 1;
-    sgl.sg_iovs = &iov;
-    sgl.sg_nr_out = 1;
-
-    /* execute write operation,
-     * dfs_write writes all bytes if there is no error */
-    int rc = dfs_write(mfu_file->dfs, mfu_file->obj, &sgl, offset, NULL); 
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_write %s failed (%d %s)",
-                file, rc, strerror(rc));
+    daos_size_t write_size = size;
+    int rc = dfs_sys_write(mfu_file->dfs_sys, mfu_file->obj, buf, offset, &write_size, NULL);
+    if (rc != 0) {
         errno = rc;
+        /* report -1 if dfs_sys_write encounters error */
         return -1;
     }
-
-    return (ssize_t)size;
+    return (ssize_t)write_size;
 #else
-    return (ssize_t)0;
+    return (ssize_t)mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1846,74 +1113,20 @@ retry:
 int daos_truncate(const char* file, off_t length, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(file, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        errno = ENOENT;
-        rc = -1;
-    } else {
-        /* open the obj in the parent */
-        dfs_obj_t* obj;
-        rc = dfs_open(mfu_file->dfs, parent, name,
-                      S_IFREG, O_RDWR,
-                      0, 0, NULL, &obj);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_open %s failed (%d %s)",
-                    name, rc, strerror(rc));
-            errno = rc;
-            rc = -1;
-        } else {
-            /* truncate the obj */
-            daos_off_t offset = (daos_off_t) length;
-            rc = dfs_punch(mfu_file->dfs, obj, offset, DFS_MAX_FSIZE);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_punch failed (%d %s)",
-                        rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-
-            /* close the obj */
-            int tmp_rc = dfs_release(mfu_file->obj);
-            if (tmp_rc && (rc != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                        file, tmp_rc, strerror(tmp_rc));
-                errno = tmp_rc;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_punch(mfu_file->dfs_sys, file, length, DFS_MAX_FSIZE);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
 int daos_ftruncate(mfu_file_t* mfu_file, off_t length)
 {
 #ifdef DAOS_SUPPORT
-    daos_off_t offset = (daos_off_t) length;
-    int rc = dfs_punch(mfu_file->dfs, mfu_file->obj, offset, DFS_MAX_FSIZE);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_punch failed (%d %s)",
-                rc, strerror(rc));
-        errno = rc;
-        rc = -1;
-    }
-    return rc;
+    int rc = dfs_punch(mfu_file->dfs, mfu_file->obj, length, DFS_MAX_FSIZE);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -1967,42 +1180,13 @@ int mfu_file_unlink(const char* file, mfu_file_t* mfu_file)
     } 
 }
 
-/* emulates unlink on a DAOS file or symlink.
- * Since checking the file type would require another
- * lookup, for performance considerations,
- * this also works on directories. */
 int daos_unlink(const char* file, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(file, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    } 
-    else {
-        /* remove the file */
-        rc = dfs_remove(mfu_file->dfs, parent, name, false, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_remove failed (%d %s)",
-                    rc, strerror(rc));
-            errno = rc;
-            rc = -1;
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_remove(mfu_file->dfs_sys, file, false, NULL);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -2067,37 +1251,10 @@ void mfu_getcwd(char* buf, size_t size)
 int daos_mkdir(const char* dir, mode_t mode, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(dir, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* only call mkdir if name is not the root DFS directory */
-    if (name && strcmp(name, "/") != 0) {
-        /* Lookup the parent directory */
-        dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-        if (parent == NULL) {
-            rc = -1;
-        } else {
-            /* Make the directory */
-            rc = dfs_mkdir(mfu_file->dfs, parent, name, mode, 0);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_mkdir %s failed (%d %s)", 
-                        name, rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_mkdir(mfu_file->dfs_sys, dir, mode, 0);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -2174,148 +1331,26 @@ retry:
 int daos_rmdir(const char* dir, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(dir, &name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Can't delete root */
-    if (name == NULL) {
-        errno = EINVAL;
-        rc = -1;
-        goto out;
-    }
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-        goto out;
-    }
-
-    /* Lookup the object */
-    dfs_obj_t* obj;
-    mode_t mode;
-    int lookup_flags = O_RDWR | O_NOFOLLOW;
-    rc = dfs_lookup_rel(mfu_file->dfs, parent, name, lookup_flags, &obj, &mode, NULL);
-    if (rc) {
-        errno = rc;
-        rc = -1;
-        goto out;
-    }
-
-    /* Release the obj */
-    rc = dfs_release(obj);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                dir, rc, strerror(rc));
-        errno = rc;
-        rc = -1;
-        goto out;
-    }
-
-    /* It must be a directory */
-    if (!S_ISDIR(mode)) {
-        errno = ENOTDIR;
-        rc = -1;
-        goto out;
-    }
-
-    /* Delete the dir */
-    rc = dfs_remove(mfu_file->dfs, parent, name, false, NULL);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_remove %s failed (%d %s)",
-                name, rc, strerror(rc));
-        errno = rc;
-        rc = -1;
-        goto out;
-    }
-
-out:
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_remove_type(mfu_file->dfs_sys, dir, false, S_IFDIR, NULL);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
-#define NUM_DIRENTS 24
-
-#ifdef DAOS_SUPPORT
-struct dfs_mfu_t {
-    dfs_obj_t* dir;
-    struct dirent ents[NUM_DIRENTS];
-    daos_anchor_t anchor;
-    int num_ents;
-};
-#endif
-
 /* open directory. The entry itself is not cached in mfu_file->dir_hash */
 DIR* daos_opendir(const char* dir, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    struct dfs_mfu_t* dirp = calloc(1, sizeof(*dirp));
-    if (dirp == NULL) {
-        errno = ENOMEM;
-        return NULL;
-    }
-
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(dir, &name, &dir_name);
-    assert(dir_name);
-
-    if (!name || strcmp(name, "/") == 0) {
-        /* For root, just lookup the entry */
-        int rc = dfs_lookup(mfu_file->dfs, dir, O_RDWR, &dirp->dir, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup %s failed", dir);
-            errno = rc;
-            free(dirp);
-            return NULL;
-        }
-    } else {
-        /* for non-root, try to cache the parent */
-        dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-        if (parent == NULL) {
-            goto err_dirp;
-        } else {
-            mode_t mode;
-            int rc = dfs_lookup_rel(mfu_file->dfs, parent, name, O_RDWR, &dirp->dir,
-                                   &mode, NULL);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed", dir);
-                errno = rc;
-                goto err_dirp;
-            } else {
-                if (!S_ISDIR(mode)) {
-                    errno = ENOTDIR;
-                    rc = dfs_release(dirp->dir);
-                    if (rc) {
-                        MFU_LOG(MFU_LOG_ERR, "dfs_release %s failed (%d %s)",
-                                dir, rc, strerror(rc));
-                    }
-                    goto err_dirp;
-                }
-            }
-        }
+    DIR* dirp = NULL;
+    int rc = dfs_sys_opendir(mfu_file->dfs_sys, dir, 0, &dirp);
+    if (rc != 0) {
+        errno = rc;
+        dirp = NULL;
     }
-
-    mfu_free(&dir_name);
-    mfu_free(&name);
-
-    return (DIR *)dirp;
-
-err_dirp:
-    mfu_free(&dir_name);
-    mfu_free(&name);
-    free(dirp);
-    return NULL;
+    return dirp;
 #else
+    errno = ENOSYS;
     return NULL;
 #endif
 }
@@ -2357,21 +1392,13 @@ DIR* mfu_file_opendir(const char* dir, mfu_file_t* mfu_file)
 }
 
 /* close dir. This is not cached in mfu_file->dir_hash */
-int daos_closedir(DIR* _dirp, mfu_file_t* mfu_file)
+int daos_closedir(DIR* dirp, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    struct dfs_mfu_t *dirp = (struct dfs_mfu_t *)_dirp;
-    int rc = dfs_release(dirp->dir);
-    if (rc) {
-        MFU_LOG(MFU_LOG_ERR, "dfs_release failed (%d %s)",
-                rc, strerror(rc));
-        errno = rc;
-        rc = -1;
-    }
-    free(dirp);
-    return rc;
+    int rc = dfs_sys_closedir(dirp);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -2410,36 +1437,18 @@ int mfu_file_closedir(DIR* dirp, mfu_file_t* mfu_file)
     }
 }
 
-struct dirent* daos_readdir(DIR* _dirp, mfu_file_t* mfu_file)
+struct dirent* daos_readdir(DIR* dirp, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    struct dfs_mfu_t *dirp = (struct dfs_mfu_t *)_dirp;
-    if (dirp->num_ents) {
-        goto ret;
-    }
-    dirp->num_ents = NUM_DIRENTS;
-    int rc;
-    while (!daos_anchor_is_eof(&dirp->anchor)) {
-        rc = dfs_readdir(mfu_file->dfs, dirp->dir,
-                         &dirp->anchor, &dirp->num_ents,
-                         dirp->ents);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_readdir failed (%d %s)", rc, strerror(rc));
-            dirp->num_ents = 0;
-            errno = ENOENT;
-            return NULL;
-        }
-        if (dirp->num_ents == 0) {
-            continue;
-        }
-        goto ret;
+    struct dirent* dirent = NULL;
+    int rc = dfs_sys_readdir(mfu_file->dfs_sys, dirp, &dirent);
+    if (rc != 0) {
+        errno = rc;
+        dirent = NULL;
     }
-    assert(daos_anchor_is_eof(&dirp->anchor));
-    return NULL;
-ret:
-    dirp->num_ents--;
-    return &dirp->ents[dirp->num_ents];
+    return dirent;
 #else
+    errno = ENOSYS;
     return NULL;
 #endif
 }
@@ -2506,65 +1515,18 @@ ssize_t mfu_llistxattr(const char* path, char* list, size_t size)
 ssize_t daos_llistxattr(const char* path, char* list, size_t size, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    daos_size_t got_size = (daos_size_t) size;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        got_size = -1;
-    }
-    else {
-        /* lookup and open name */
-        dfs_obj_t* obj;
-        int lookup_flags = O_RDWR | O_NOFOLLOW;
-        int rc = dfs_lookup_rel(mfu_file->dfs, parent, name, lookup_flags, &obj, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed (%d %s)",
-                    path, rc, strerror(rc));
-            errno = rc;
-            got_size = -1;
-        } else {
-            /* list the xattrs */
-            rc = dfs_listxattr(mfu_file->dfs, obj, list, &got_size);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_listxattr %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                got_size = -1;
-            } else if (size == 0) {
-                /* we will just return got_size */
-            } else if (size < got_size) {
-                errno = ERANGE;
-                got_size = -1;
-            }
-
-            /* Release the obj.
-             * Don't log the error if we already have a different error. */
-            rc = dfs_release(obj);
-            if (rc && (got_size != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release failed (%d %s)",
-                        rc, strerror(rc));
-                errno = rc;
-                got_size = -1;
-            }
-        }
+    daos_size_t got_size = size;
+    int rc = dfs_sys_listxattr(mfu_file->dfs_sys, path, list, &got_size, O_NOFOLLOW);
+    if (rc != 0) {
+        errno = rc;
     }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
     return (ssize_t) got_size;
 #else
-    return (ssize_t) 0;
+    return (ssize_t) mfu_errno2rc(ENOSYS);
 #endif
 }
 
-/* list xattrs (link dereference) */
+/* list xattrs (link dereference) */
 ssize_t mfu_file_listxattr(const char* path, char* list, size_t size, mfu_file_t* mfu_file)
 {
     if (mfu_file->type == POSIX) {
@@ -2590,59 +1552,14 @@ ssize_t mfu_listxattr(const char* path, char* list, size_t size)
 ssize_t daos_listxattr(const char* path, char* list, size_t size, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* name     = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &name, &dir_name);
-    assert(dir_name);
-
-    daos_size_t got_size = (daos_size_t) size;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        got_size = -1;
-    }
-    else {
-        /* lookup the object */
-        dfs_obj_t* obj;
-        int rc = dfs_lookup_rel(mfu_file->dfs, parent, name, O_RDWR, &obj, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed (%d %s)",
-                    path, rc, strerror(rc));
-            errno = rc;
-            got_size = -1;
-        } else {
-            /* list the xattrs of the obj */
-            rc = dfs_listxattr(mfu_file->dfs, obj, list, &got_size);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_listxattr %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                got_size = -1;
-            } else if (size == 0) {
-                /* we will just return got_size */
-            } else if (size < got_size) {
-                errno = ERANGE;
-                got_size = -1;
-            }
-        }
-
-        /* Release the obj */
-        rc = dfs_release(obj);
-        if (rc && (got_size != -1)) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_release failed (%d %s)",
-                    rc, strerror(rc));
-            errno = rc;
-            got_size = -1;
-        }
+    daos_size_t got_size = size;
+    int rc = dfs_sys_listxattr(mfu_file->dfs_sys, path, list, &got_size, 0);
+    if (rc != 0) {
+        errno = rc;
     }
-
-    mfu_free(&name);
-    mfu_free(&dir_name);
-
     return (ssize_t) got_size;
 #else
-    return (ssize_t) 0;
+    return (ssize_t) mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -2670,60 +1587,14 @@ ssize_t mfu_lgetxattr(const char* path, const char* name, void* value, size_t si
 ssize_t daos_lgetxattr(const char* path, const char* name, void* value, size_t size, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* obj_name = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &obj_name, &dir_name);
-    assert(dir_name);
-
-    daos_size_t got_size = (daos_size_t) size;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        got_size = -1;
-    }
-    else {
-        /* lookup and open obj_name */
-        dfs_obj_t* obj;
-        int lookup_flags = O_RDWR | O_NOFOLLOW;
-        int rc = dfs_lookup_rel(mfu_file->dfs, parent, obj_name, lookup_flags, &obj, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed (%d %s)",
-                    path, rc, strerror(rc));
-            errno = rc;
-            got_size = -1;
-        } else {
-            /* get the xattr */
-            rc = dfs_getxattr(mfu_file->dfs, obj, name, value, &got_size);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_getxattr %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                got_size = -1;
-            } else if (size == 0) {
-                /* we will just return got_size */
-            } else if (size < got_size) {
-                errno = ERANGE;
-                got_size = -1;
-            }
-
-            /* Release the obj */
-            rc = dfs_release(obj);
-            if (rc && (got_size != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release failed (%d %s)",
-                        rc, strerror(rc));
-                errno = rc;
-                got_size = -1;
-            }
-        }
+    daos_size_t got_size = size;
+    int rc = dfs_sys_getxattr(mfu_file->dfs_sys, path, name, value, &got_size, O_NOFOLLOW);
+    if (rc != 0) {
+        errno = rc;
     }
-
-    mfu_free(&obj_name);
-    mfu_free(&dir_name);
-
     return (ssize_t) got_size;
 #else
-    return (ssize_t) 0;
+    return (ssize_t) mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -2751,59 +1622,14 @@ ssize_t mfu_getxattr(const char* path, const char* name, void* value, size_t siz
 ssize_t daos_getxattr(const char* path, const char* name, void* value, size_t size, mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* obj_name = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &obj_name, &dir_name);
-    assert(dir_name);
-
-    daos_size_t got_size = (daos_size_t) size;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        got_size = -1;
-    }
-    else {
-        /* lookup the object */
-        dfs_obj_t* obj;
-        int rc = dfs_lookup_rel(mfu_file->dfs, parent, obj_name, O_RDWR, &obj, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed (%d %s)",
-                    path, rc, strerror(rc));
-            errno = rc;
-            got_size = -1;
-        } else {
-            /* get the xattr of the obj */
-            rc = dfs_getxattr(mfu_file->dfs, obj, name, value, &got_size);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_getxattr %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                got_size = -1;
-            } else if (size == 0) {
-                /* we will just return got_size */
-            } else if (size < got_size) {
-                errno = ERANGE;
-                got_size = -1;
-            }
-        }
-
-        /* Release the obj */
-        rc = dfs_release(obj);
-        if (rc && (got_size != -1)) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_release failed (%d %s)",
-                    rc, strerror(rc));
-            errno = rc;
-            got_size = -1;
-        }
+    daos_size_t got_size = size;
+    int rc = dfs_sys_getxattr(mfu_file->dfs_sys, path, name, value, &got_size, 0);
+    if (rc != 0) {
+        errno = rc;
     }
-
-    mfu_free(&obj_name);
-    mfu_free(&dir_name);
-
     return (ssize_t) got_size;
 #else
-    return (ssize_t) 0;
+    return (ssize_t) mfu_errno2rc(ENOSYS);
 #endif
 }
 
@@ -2833,54 +1659,9 @@ int daos_lsetxattr(const char* path, const char* name, const void* value, size_t
                    mfu_file_t* mfu_file)
 {
 #ifdef DAOS_SUPPORT
-    char* obj_name = NULL;
-    char* dir_name = NULL;
-    parse_filename(path, &obj_name, &dir_name);
-    assert(dir_name);
-
-    int rc = 0;
-
-    /* Lookup the parent directory */
-    dfs_obj_t* parent = daos_hash_lookup(dir_name, mfu_file);
-    if (parent == NULL) {
-        rc = -1;
-    }
-    else {
-        /* lookup and open obj_name */
-        dfs_obj_t* obj;
-        int lookup_flags = O_RDWR | O_NOFOLLOW;
-        rc = dfs_lookup_rel(mfu_file->dfs, parent, obj_name, lookup_flags, &obj, NULL, NULL);
-        if (rc) {
-            MFU_LOG(MFU_LOG_ERR, "dfs_lookup_rel %s failed (%d %s)",
-                    path, rc, strerror(rc));
-            errno = rc;
-            rc = -1;
-        } else {
-            /* set the xattr */
-            rc = dfs_setxattr(mfu_file->dfs, obj, name, value, size, flags);
-            if (rc) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_setxattr %s failed (%d %s)",
-                        path, rc, strerror(rc));
-                errno = rc;
-                rc = -1;
-            }
-
-            /* Release the obj */
-            int rc_rel = dfs_release(obj);
-            if (rc_rel && (rc != -1)) {
-                MFU_LOG(MFU_LOG_ERR, "dfs_release failed (%d %s)",
-                        rc_rel, strerror(rc_rel));
-                errno = rc_rel;
-                rc = -1;
-            }
-        }
-    }
-
-    mfu_free(&obj_name);
-    mfu_free(&dir_name);
-
-    return rc;
+    int rc = dfs_sys_setxattr(mfu_file->dfs_sys, path, name, value, size, flags, O_NOFOLLOW);
+    return mfu_errno2rc(rc);
 #else
-    return 0;
+    return mfu_errno2rc(ENOSYS);
 #endif
 }
diff --git a/src/common/mfu_io.h b/src/common/mfu_io.h
index 1274b54..1686087 100644
--- a/src/common/mfu_io.h
+++ b/src/common/mfu_io.h
@@ -37,6 +37,7 @@ extern "C" {
 #ifdef DAOS_SUPPORT
 #include <daos.h>
 #include <daos_fs.h>
+#include <daos_fs_sys.h>
 #endif
 
 #include "mfu_param_path.h"
@@ -95,7 +96,7 @@ int daos_stat(const char* path, struct stat* buf, mfu_file_t* mfu_file);
 
 /* call mknod, retry a few times on EINTR or EIO */
 int mfu_file_mknod(const char* path, mode_t mode, dev_t dev, mfu_file_t* mfu_file);
-int daos_mknod(const char* path, mode_t mode, dev_t dev, mfu_file_t* mfu_file);
+int daos_mknod(const char* path, mode_t mode, mfu_file_t* mfu_file);
 int mfu_mknod(const char* path, mode_t mode, dev_t dev);
 
 /* call remove, retry a few times on EINTR or EIO */
diff --git a/src/common/mfu_param_path.c b/src/common/mfu_param_path.c
index d065fdf..488d4f2 100644
--- a/src/common/mfu_param_path.c
+++ b/src/common/mfu_param_path.c
@@ -303,6 +303,31 @@ static void mfu_unpack_param(const char** pptr, mfu_param_path* param)
     return;
 }
 
+/*
+ * Parse an option string provided by the user to determine
+ * which xattrs to copy from source to destination.
+ */
+attr_copy_t parse_copy_xattrs_option(char *optarg)
+{
+    if (strcmp(optarg,"none") == 0) {
+        return XATTR_COPY_NONE;
+    }
+
+    if (strcmp(optarg,"non-lustre") == 0) {
+        return XATTR_SKIP_LUSTRE;
+    }
+
+    if (strcmp(optarg,"libattr") == 0) {
+        return XATTR_USE_LIBATTR;
+    }
+
+    if (strcmp(optarg,"all") == 0) {
+        return XATTR_COPY_ALL;
+    }
+
+    return XATTR_COPY_INVAL;
+}
+
 /**
  * Analyze all file path inputs and place on the work queue.
  *
@@ -372,11 +397,14 @@ char* mfu_param_path_copy_dest(const char* name, int numpaths,
     /* get number of components in source path */
     int src_components = mfu_path_components(src);
 
-    /* if copying into directory, keep last component,
+    /* if copying into directory, keep last component.
+     * if path is root, keep last component.
      * otherwise cut all components listed in source path */
     int cut = src_components;
     if (mfu_copy_opts->copy_into_dir && cut > 0) {
-        if ((mfu_copy_opts->do_sync != 1) &&
+        if (strcmp(paths[i].orig, "/") == 0) {
+            cut--;
+        } else if ((mfu_copy_opts->do_sync != 1) &&
             (paths[i].orig[strlen(paths[i].orig) - 1] != '/')) {
             cut--;
         }
@@ -534,8 +562,8 @@ void mfu_param_path_check_copy(uint64_t num, const mfu_param_path* paths,
 
             /* check that dest is writable */
             if(mfu_file_access(destpath->path, W_OK, mfu_dst_file) < 0) {
-                MFU_LOG(MFU_LOG_ERR, "Destination is not writable `%s'",
-                    destpath->path);
+                MFU_LOG(MFU_LOG_ERR, "Destination is not writable `%s' (errno=%d %s)",
+                    destpath->path, errno, strerror(errno));
                 valid = 0;
                 goto bcast;
             }
@@ -552,8 +580,8 @@ void mfu_param_path_check_copy(uint64_t num, const mfu_param_path* paths,
 
             /* check that parent is writable */
             if(mfu_file_access(parent_str, W_OK, mfu_dst_file) < 0) {
-                MFU_LOG(MFU_LOG_ERR, "Destination parent directory is not writable `%s'",
-                    parent_str);
+                MFU_LOG(MFU_LOG_ERR, "Destination parent directory is not writable `%s' (errno=%d %s)",
+                    parent_str, errno, strerror(errno));
                 valid = 0;
                 mfu_free(&parent_str);
                 goto bcast;
diff --git a/src/common/mfu_param_path.h b/src/common/mfu_param_path.h
index 5c6bb4c..156be26 100644
--- a/src/common/mfu_param_path.h
+++ b/src/common/mfu_param_path.h
@@ -58,10 +58,10 @@ typedef struct {
     int                  fd;
 #ifdef DAOS_SUPPORT
     /* DAOS specific variables for I/O */
-    daos_off_t           offset;
-    dfs_obj_t*           obj;
-    dfs_t*               dfs;
-    struct d_hash_table* dfs_hash;
+    daos_off_t           offset;  /* file offset */
+    dfs_obj_t*           obj;     /* open object handle */
+    dfs_sys_t*           dfs_sys; /* handle for high-level file operations */
+    dfs_t*               dfs;     /* handle for lower-level file operations */
 #endif
 } mfu_file_t;
 
@@ -115,26 +115,41 @@ typedef struct {
     int dereference;    /* flag option to dereference symbolic links */
 } mfu_walk_opts_t;
 
+typedef enum {
+    XATTR_COPY_INVAL,
+    XATTR_COPY_NONE,
+    XATTR_SKIP_LUSTRE,
+    XATTR_USE_LIBATTR,
+    XATTR_COPY_ALL,
+} attr_copy_t;
+
 /* options passed to mfu_ */
 typedef struct {
-    int    copy_into_dir;  /* flag indicating whether copying into existing dir */
-    int    do_sync;        /* flag option to sync src dir with dest dir */ 
-    char*  dest_path;      /* prefex of destination directory */
-    char*  input_file;     /* file name of input list */
-    bool   preserve;       /* whether to preserve timestamps, ownership, permissions, etc. */
-    int    dereference;    /* if true, dereference symbolic links in the source.
-                            * this is not a perfect opposite of no_dereference */
-    int    no_dereference; /* if true, don't dereference source symbolic links */
-    bool   direct;         /* whether to use O_DIRECT */
-    bool   sparse;         /* whether to create sparse files */
-    size_t chunk_size;     /* size to chunk files by */
-    size_t buf_size;       /* buffer size to read/write to file system */
-    char*  block_buf1;     /* buffer to read / write data */
-    char*  block_buf2;     /* another buffer to read / write data */
-    int    grouplock_id;   /* Lustre grouplock ID */
-    uint64_t batch_files;  /* max batch size to copy files, 0 implies no limit */
+    int          copy_into_dir;    /* flag indicating whether copying into existing dir */
+    int          do_sync;          /* flag option to sync src dir with dest dir */
+    char*        dest_path;        /* prefex of destination directory */
+    char*        input_file;       /* file name of input list */
+    bool         preserve;         /* whether to preserve timestamps, ownership, permissions, etc. */
+    attr_copy_t  copy_xattrs;      /* which xattrs to copy; important for Lustre */
+    int          dereference;      /* if true, dereference symbolic links in the source.
+                                    * this is not a perfect opposite of no_dereference */
+    int          no_dereference;   /* if true, don't dereference source symbolic links */
+    bool         direct;           /* whether to use O_DIRECT */
+    bool         sparse;           /* whether to create sparse files */
+    size_t       chunk_size;       /* size to chunk files by */
+    size_t       buf_size;         /* buffer size to read/write to file system */
+    char*        block_buf1;       /* buffer to read / write data */
+    char*        block_buf2;       /* another buffer to read / write data */
+    int          grouplock_id;     /* Lustre grouplock ID */
+    uint64_t     batch_files;      /* max batch size to copy files, 0 implies no limit */
 } mfu_copy_opts_t;
 
+/*
+ * Parse an option string provided by the user to determine
+ * which xattrs to copy from source to destination.
+ */
+attr_copy_t parse_copy_xattrs_option(char *optarg);
+
 /* Given a source item name, determine which source path this item
  * is contained within, extract directory components from source
  * path to this item and then prepend destination prefix.
diff --git a/src/common/mfu_util.c b/src/common/mfu_util.c
index 44f59f0..0be0fa7 100644
--- a/src/common/mfu_util.c
+++ b/src/common/mfu_util.c
@@ -100,6 +100,28 @@ void* mfu_malloc(size_t size, const char* file, int line)
     return NULL;
 }
 
+/* if size > 0 allocates size bytes and returns pointer,
+ * calls mfu_abort if calloc fails, returns NULL if size == 0 */
+void* mfu_calloc(size_t nelem, size_t elsize, const char* file, int line)
+{
+    /* only bother if size > 0 */
+    if (nelem > 0 && elsize > 0) {
+        /* try to allocate memory and check whether we succeeded */
+        void* ptr = calloc(nelem, elsize);
+        if (ptr == NULL) {
+            /* allocate failed, abort */
+            mfu_abort(file, line, 1, "Failed to allocate %llu * %llu bytes. Try using more nodes.",
+                        (unsigned long long) nelem, (unsigned long long) elsize
+                       );
+        }
+
+        /* return the pointer */
+        return ptr;
+    }
+
+    return NULL;
+}
+
 /* if size > 0, allocates size bytes aligned with specified alignment
  * and returns pointer, calls mfu_abort on failure,
  * returns NULL if size == 0 */
diff --git a/src/common/mfu_util.h b/src/common/mfu_util.h
index d6f94dd..dcc32e5 100644
--- a/src/common/mfu_util.h
+++ b/src/common/mfu_util.h
@@ -131,6 +131,16 @@ void* mfu_malloc(
   int line
 );
 
+/* if size > 0 allocates size bytes and returns pointer,
+ * calls mfu_abort if calloc fails, returns NULL if size == 0 */
+#define MFU_CALLOC(X, Y) mfu_calloc(X, Y, __FILE__, __LINE__)
+void* mfu_calloc(
+  size_t nelem,
+  size_t elsize,
+  const char* file,
+  int line
+);
+
 /* if size > 0, allocates size bytes aligned with specified alignment
  * and returns pointer, calls mfu_abort on failure,
  * returns NULL if size == 0 */
diff --git a/src/common/strmap.c b/src/common/strmap.c
index a8761ae..fef5a7d 100644
--- a/src/common/strmap.c
+++ b/src/common/strmap.c
@@ -765,17 +765,20 @@ int strmap_unset(strmap* tree, const char* key)
 
             /* found it, identify the node to replace it */
             if (node->left != NULL && node->right != NULL) {
-                /* we have two children, extract the rightmost node in
-                 * our left subtree */
+                /* we have two children, identify rightmost node of left subtree */
                 strmap_node* replacement = (strmap_node*) strmap_node_rightmost(node->left);
-                strmap_node_extract_single(replacement);
-
-                /* update the left child of this node to point to our left child,
-                 * (note that this works correctly even if the replacement is our
-                 * original left child, because the extract call would update our
-                 * left child to be our left grandchild) */
-                replacement->left = node->left;
-                node->left->parent = replacement;
+
+                /* if the rightmost node of our left subtree is not our left child,
+                 * extract it and promote it to be our replacement */ 
+                if (replacement != node->left) {
+                    /* since this is a rightmost node, it has at most one child,
+                     * so safe to use extract_single  */
+                    strmap_node_extract_single(replacement);
+
+                    /* update the left child of this node to point to our replacement */
+                    replacement->left = node->left;
+                    node->left->parent = replacement;
+                }
 
                 /* update the right child of this node to point to our right child,
                  * (we're guaranteed that the rightmost node from our left subtree
diff --git a/src/daos-deserialize/CMakeLists.txt b/src/daos-deserialize/CMakeLists.txt
new file mode 100644
index 0000000..84e5cf1
--- /dev/null
+++ b/src/daos-deserialize/CMakeLists.txt
@@ -0,0 +1 @@
+MFU_ADD_TOOL(daos-deserialize)
diff --git a/src/daos-deserialize/daos-deserialize.c b/src/daos-deserialize/daos-deserialize.c
new file mode 100644
index 0000000..974134d
--- /dev/null
+++ b/src/daos-deserialize/daos-deserialize.c
@@ -0,0 +1,425 @@
+#include <stdio.h>
+#include <getopt.h>
+#include <string.h>
+#include <stdlib.h>
+#include <ctype.h>
+#include <inttypes.h>
+#include <unistd.h>
+#include <errno.h>
+#include <stdbool.h>
+
+/* for daos */
+#include "mfu_daos.h"
+#include "mpi.h"
+#include "mfu.h"
+
+#include "mfu_errors.h"
+
+/** Print a usage message. */
+void print_usage(void)
+{
+    printf("\n");
+    printf("Usage: daos-deserialize [options] [<h5file> <h5file> ...] || [</path/to/dir>]\n");
+    printf("\n");
+    printf("Options:\n");
+    printf("  -p, --pool               - pool uuid for containers\n");
+    printf("  -l, --cont-label         - use a label name for deserialize container\n");
+    printf("  -v, --verbose            - verbose output\n");
+    printf("  -q, --quiet              - quiet output\n");
+    printf("  -h, --help               - print usage\n");
+    printf("For more information see https://mpifileutils.readthedocs.io.\n");
+    printf("\n");
+    fflush(stdout);
+}
+
+/* if a directory is passed in we need to count the files in it,
+ * and read them into the paths array */
+static int count_files(char **argpaths, char ***paths, int *numpaths) {
+    int         rc = 0;
+    int         i;
+    struct stat statbuf;
+    int         num_files = 0;
+
+    rc = stat(argpaths[0], &statbuf);
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to stat input file"
+                MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        rc = 1;
+        goto out;
+    }
+
+    if (S_ISDIR(statbuf.st_mode)) {
+        DIR *dir;
+        struct dirent *entry;
+
+        /* set a max paths so that we do not 
+         * have to read directory twice,
+         * once to get number of files, and
+         * once to copy/save strings to paths array */
+        int max_paths = 1024;
+
+        /* use paths instead of argpaths in case directory is used */
+        *paths = MFU_MALLOC(max_paths * sizeof(char*));
+        if (*paths == NULL) {
+            rc = ENOMEM;
+            goto out;
+        }
+
+        dir = opendir(argpaths[0]);
+        if (dir == NULL) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to open directory"
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+            rc = 1;
+            goto out;
+        }
+
+        while((entry = readdir(dir)) != NULL) {
+            /* don't count or copy into paths array if this
+             * is not a regular file */
+            struct stat stbuf;
+            int len;
+            char name[FILENAME_LEN];
+            len = snprintf(name, FILENAME_LEN, "%s/%s", argpaths[0],
+                           entry->d_name);
+            if (len >= FILENAME_LEN) {
+                MFU_LOG(MFU_LOG_ERR, "filename is too long");
+                rc = 1;
+                goto out;
+            }
+            rc = stat(name, &stbuf);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "failed to stat path: %s", name);
+                rc = 1;
+                goto out;
+            }
+            if (S_ISREG(stbuf.st_mode)) {   
+                char *path = MFU_STRDUP(name);
+                if (path == NULL) {
+                    rc = ENOMEM;
+                    goto out;
+                }
+                (*paths)[num_files] = strdup(name);
+                mfu_free(&path);
+                num_files++;
+                if (num_files > max_paths) {
+                    /* TODO: maybe there is a better way to handle this..
+                     * possibly moving over to libcircle might avoid
+                     * having to set a maximum for the number of files
+                     * deserialized from one directory */
+                    MFU_LOG(MFU_LOG_ERR, "number of files exceeds max number "
+                            "allowed, aborting");
+                    rc = 1;
+                    goto out;
+                }
+            } else {
+                continue;
+            }
+        }
+        *numpaths = num_files;
+        closedir(dir);
+    } else {
+        /* use paths instead of argpaths in case directory is used */
+        *paths = MFU_MALLOC(*numpaths * sizeof(char*));
+        if (*paths == NULL) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to allocate paths.");
+            rc = 1;
+            goto out;
+        }
+
+        num_files = *numpaths;
+        for (i = 0; i < *numpaths; i++) {
+            (*paths)[i] = MFU_STRDUP(argpaths[i]);
+            if ((*paths)[i] == NULL) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to allocate paths.");
+                rc = 1;
+                goto out;
+            }
+        }
+    }
+out:
+    if (rc != 0) {
+        if (*paths != NULL) {
+            for (i = 0; i < num_files; i++) {
+                mfu_free(&(*paths)[i]); // TODO check this pointer handling
+            }
+            mfu_free(paths);
+        }
+    }
+    return rc;
+}
+
+int main(int argc, char** argv)
+{
+    /* assume we'll exit with success */
+    int rc = 0;
+
+    /* initialize MPI */
+    MPI_Init(&argc, &argv);
+    mfu_init();
+
+    /* get our rank */
+    int rank;
+    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
+
+    /* verbose by default */
+    mfu_debug_level = MFU_LOG_VERBOSE;
+
+    int option_index = 0;
+    static struct option long_options[] = {
+        {"pool"                 , required_argument , 0, 'p'},
+        {"cont-label"           , required_argument , 0, 'l'},
+        {"verbose"              , no_argument       , 0, 'v'},
+        {"quiet"                , no_argument       , 0, 'q'},
+        {"help"                 , no_argument       , 0, 'h'},
+        {0                      , 0                 , 0,  0 }
+    };
+
+    /* Parse options */
+    unsigned long long bytes = 0;
+
+    /* DAOS vars */ 
+    daos_args_t* daos_args = daos_args_new();    
+    /* option to deserialize container with label name */
+    char *cont_label = NULL;
+
+    int usage = 0;
+    while(1) {
+        int c = getopt_long(
+                    argc, argv, "p:l:vqh",
+                    long_options, &option_index
+                );
+
+        if (c == -1) {
+            break;
+        }
+
+        switch(c) {
+            case 'p':
+                snprintf(daos_args->src_pool, DAOS_PROP_LABEL_MAX_LEN + 1, "%s", optarg);
+                break;
+            case 'l':
+                cont_label = MFU_STRDUP(optarg);
+                break;
+            case 'v':
+                mfu_debug_level = MFU_LOG_VERBOSE;
+                break;
+            case 'q':
+                mfu_debug_level = MFU_LOG_NONE;
+                break;
+            case 'h':
+                usage = 1;
+                break;
+            case '?':
+                usage = 1;
+                break;
+            default:
+                if(rank == 0) {
+                    printf("?? getopt returned character code 0%o ??\n", c);
+                }
+        }
+    }
+
+    /* If we need to print the usage
+     * then do so before internal processing */
+    if (usage) {
+        if (rank == 0) {
+            print_usage();
+        }
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    int tmp_rc;
+    char** argpaths = (&argv[optind]);
+
+    /* The remaining arguments are treated as src/dst paths */
+    int numpaths = argc - optind;
+
+    /* advance to next set of options */
+    optind += numpaths;
+
+    if (numpaths < 1 || daos_args->src_pool ==  NULL) {
+        MFU_LOG(MFU_LOG_ERR, "At least one file or directory and "
+                "a pool UUID is required:"
+                MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    /* create an empty file list */
+    mfu_flist tmplist = mfu_flist_new();
+
+    rc = daos_init();
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to initialize daos");
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    /* Initialize some stats */
+    mfu_daos_stats_t stats;
+    mfu_daos_stats_init(&stats);
+    mfu_daos_stats_start(&stats);
+
+    /* TODO: I think maybe this can be adjusted to use
+     * libcircle in the case that a user specifies
+     * a directory, it could only add regular files
+     * though, so i am not sure how easy/hard this
+     * would be. I am concerned about the case where
+     * there are thousands of larger files in one directory.
+     * It would be nice if multiple ranks were counting/adding
+     * to the flist. We may also be able to avoid the 
+     * mfu_flist_spread since if we can use libcircle then
+     * each rank can process a file as it receives work,
+     * instead of gathering the work on rank 0 first */
+    if (rank == 0) {
+        uint64_t                files_generated = 0;
+        hid_t                   status;
+        struct                  hdf5_args hdf5;
+        daos_cont_layout_t      cont_type;
+
+        char **paths = NULL;
+
+        /* if a directory is given then count and store paths */
+        rc = count_files(argpaths, &paths, &numpaths);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to count files");
+            daos_fini();
+            mfu_finalize();
+            MPI_Finalize();
+            return 1;
+        }
+        
+        /* make sure the number of files generated on serialization
+         * matches the number of files passed into the deserialization
+         * grab the first file, since each file stores this attribute */
+        hdf5.file = H5Fopen(paths[0], H5F_ACC_RDONLY, H5P_DEFAULT);
+        if (hdf5.file < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to open file");
+            rc = 1;
+        }
+
+        hid_t files_gen_attr = H5Aopen(hdf5.file, "Files Generated", H5P_DEFAULT);
+        if (files_gen_attr < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to open files generated attr");
+            rc = 1;
+        }
+
+        hid_t attr_dtype = H5Aget_type(files_gen_attr);
+        if (attr_dtype < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get attr type");
+            rc = 1;
+        }
+
+        status = H5Aread(files_gen_attr, attr_dtype, &files_generated); 
+        if (status < 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to read files generated");
+            rc = 1;
+        }
+
+        if (files_generated != numpaths) {
+            MFU_LOG(MFU_LOG_ERR, "number of files for deserialization does "
+                                 "not match number of files generated during "
+                                 "serialization, contianer data is missing\n");
+            rc = 1;
+        }
+
+        int i;
+        for (i = 0; i < numpaths; i++) {
+            uint64_t idx = mfu_flist_file_create(tmplist);
+            mfu_flist_file_set_cont(tmplist, idx, paths[i]);
+        }
+
+        tmp_rc = daos_cont_deserialize_connect(daos_args, &hdf5, &cont_type, cont_label);
+        if (tmp_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to connect to container\n");
+            rc = 1;
+        }
+
+        for (i = 0; i < numpaths; i++) {
+            mfu_free(&paths[i]);
+        }
+        mfu_free(&paths);
+
+        /* close hdf5 */
+        H5Fclose(hdf5.file);
+        H5Aclose(files_gen_attr);
+        H5Tclose(attr_dtype);
+    }
+
+    /* use rank 0's paths, and spread them evenly among ranks
+     * Each "path" is an HDF5 file */
+    mfu_flist_summarize(tmplist);
+    mfu_flist newflist = mfu_flist_spread(tmplist);
+
+    /* get size of local list for each rank */
+    uint64_t size = mfu_flist_size(newflist);
+
+    /* broadcast rank 0's pool and cont handle to everyone else */
+    daos_bcast_handle(rank, &daos_args->src_poh, &daos_args->src_poh, POOL_HANDLE); 
+    daos_bcast_handle(rank, &daos_args->src_coh, &daos_args->src_poh, CONT_HANDLE); 
+
+    /* connect to each pool/cont in local list, then serialize */
+    int i;
+    for (i = 0; i < size; i++) {
+        const char *path = mfu_flist_file_get_name(newflist, i);
+
+        /* deserialize this hdf5 file to a DAOS container */
+        tmp_rc = daos_cont_deserialize_hdlr(rank, daos_args, path, &stats);
+        if (tmp_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to deserialize container (%d)", rc);
+            rc = 1;
+        }
+    }
+
+    /* Record end time */
+    mfu_daos_stats_end(&stats);
+
+    /* Sum and print the stats */
+    mfu_daos_stats_print_sum(rank, &stats, false, true, false, false);
+
+    mfu_flist_free(&newflist);
+    mfu_free(&cont_label);
+
+    /* don't close anything until all ranks are done using handles */
+    MPI_Barrier(MPI_COMM_WORLD);
+
+    tmp_rc = daos_cont_close(daos_args->src_coh, NULL);
+    MPI_Barrier(MPI_COMM_WORLD);
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to close container (%d)", rc);
+        rc = 1;
+    }
+    
+    tmp_rc = daos_pool_disconnect(daos_args->src_poh, NULL);
+    MPI_Barrier(MPI_COMM_WORLD);
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to disconnect pool (%d)", rc);
+        rc = 1;
+    }
+
+    /* Alert the user if there were copy errors */
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "One or more errors were detected while "
+                "deserializing: " MFU_ERRF, MFU_ERRP(MFU_ERR_DAOS));
+    }
+
+    tmp_rc = daos_fini();
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to finalize DAOS "DF_RC, DP_RC(rc));
+        rc = 1;
+    }
+
+    mfu_finalize();
+
+    /* shut down MPI */
+    MPI_Finalize();
+
+    if (rc != 0) {
+        return 1;
+    }
+    return 0;
+}
diff --git a/src/daos-gen/CMakeLists.txt b/src/daos-gen/CMakeLists.txt
new file mode 100644
index 0000000..f2ead98
--- /dev/null
+++ b/src/daos-gen/CMakeLists.txt
@@ -0,0 +1 @@
+MFU_ADD_TOOL(daos-gen)
diff --git a/src/daos-gen/daos-gen.c b/src/daos-gen/daos-gen.c
new file mode 100644
index 0000000..d96373a
--- /dev/null
+++ b/src/daos-gen/daos-gen.c
@@ -0,0 +1,304 @@
+#include <stdio.h>
+#include <getopt.h>
+#include <string.h>
+#include <stdlib.h>
+#include <ctype.h>
+#include <inttypes.h>
+#include <unistd.h>
+#include <errno.h>
+#include <stdbool.h>
+
+/* for daos */
+#include "mfu_daos.h"
+#include "mpi.h"
+#include "mfu.h"
+
+#include "mfu_errors.h"
+
+#define BUFLEN 80
+#define UUID_STR_LEN 129
+
+static uint32_t obj_id_gen = 1;
+
+/** Print a usage message. */
+void print_usage(void)
+{
+    printf("\n");
+    printf("Usage: daos-gen [options]\n");
+    printf("\n");
+    printf("Options:\n");
+    printf("  -p, --pool               - pool uuid for containers\n");
+    printf("  -o, --num-objects  - number of objects to generate\n");
+    printf("  -k, --keys-per-object - number of keys per object\n");
+    printf("  -v, --verbose            - verbose output\n");
+    printf("  -q, --quiet              - quiet output\n");
+    printf("  -h, --help               - print usage\n");
+    printf("For more information see https://mpifileutils.readthedocs.io.\n");
+    printf("\n");
+    fflush(stdout);
+}
+
+/* stole this from the DAOS test code for generating obj id's */
+daos_obj_id_t dts_oid_gen(unsigned seed)
+{
+    daos_obj_id_t   oid;
+    uint64_t    hdr;
+
+    hdr = seed;
+    hdr <<= 32;
+
+    /* generate a unique and not scary long object ID */
+    oid.lo  = obj_id_gen++;
+    oid.lo  |= hdr;
+    oid.hi  = rand() % 100;
+
+    return oid;
+}
+
+int main(int argc, char** argv)
+{
+    /* assume we'll exit with success */
+    int rc = 0;
+
+    /* initialize MPI */
+    MPI_Init(&argc, &argv);
+    mfu_init();
+
+    /* get our rank */
+    int rank, size;
+    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
+    MPI_Comm_size(MPI_COMM_WORLD, &size);
+
+    /* verbose by default */
+    mfu_debug_level = MFU_LOG_VERBOSE;
+
+    int option_index = 0;
+    static struct option long_options[] = {
+        {"pool"                 , required_argument , 0, 'p'},
+        {"num-objects"          , required_argument , 0, 'o'},
+        {"keys-per-object"      , required_argument , 0, 'k'},
+        {"verbose"              , no_argument       , 0, 'v'},
+        {"quiet"                , no_argument       , 0, 'q'},
+        {"help"                 , no_argument       , 0, 'h'},
+        {0                      , 0                 , 0,  0 }
+    };
+
+    /* TODO: currently only generates DAOS_OF_KV_FLAT data, but could be
+     * updated to include more object types */
+
+    /* Parse options */
+    int num_objects = 0;
+    int keys_per_object = 0;
+
+    /* DAOS vars */ 
+    daos_args_t* daos_args = daos_args_new();    
+
+    int usage = 0;
+    while(1) {
+        int c = getopt_long(
+                    argc, argv, "p:o:k:vqh",
+                    long_options, &option_index
+                );
+
+        if (c == -1) {
+            break;
+        }
+
+        switch(c) {
+            case 'p':
+                strncpy(daos_args->src_pool, optarg, DAOS_PROP_LABEL_MAX_LEN);
+                break;
+            case 'o':
+                num_objects = atoi(optarg);
+                if (rank == 0) {
+                    MFU_LOG(MFU_LOG_INFO, "generating %d objects\n", num_objects);
+                }
+                break;
+            case 'k':
+                keys_per_object = atoi(optarg);
+                if (rank == 0) {
+                    MFU_LOG(MFU_LOG_INFO, "generating %d keys per object\n", keys_per_object);
+                }
+                break;
+            case 'v':
+                mfu_debug_level = MFU_LOG_VERBOSE;
+                break;
+            case 'q':
+                mfu_debug_level = MFU_LOG_NONE;
+                break;
+            case 'h':
+                usage = 1;
+                break;
+            case '?':
+                usage = 1;
+                break;
+            default:
+                if(rank == 0) {
+                    printf("?? getopt returned character code 0%o ??\n", c);
+                }
+        }
+    }
+
+    /* If we need to print the usage
+     * then do so before internal processing */
+    if (usage) {
+        if (rank == 0) {
+            print_usage();
+        }
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    int tmp_rc;
+
+    rc = daos_init();
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to initialize daos");
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    /* set default number of objects and keys per object,
+     * if one is not passed in */
+    if (num_objects == 0) {
+        num_objects = 10;
+    }
+    if (keys_per_object == 0) {
+        keys_per_object = 20;
+    }
+
+    daos_obj_id_t       oid[num_objects];
+    daos_handle_t	    oh[num_objects];
+    char		        buf[BUFLEN];
+    const char          *key_fmt = "key%d";
+    char                key[keys_per_object];
+    int                 i,j;
+    char                uuid_str[UUID_STR_LEN];
+    daos_ofeat_t        ofeats;
+
+    ofeats = DAOS_OF_DKEY_UINT64 | DAOS_OF_KV_FLAT | DAOS_OF_KV_FLAT;
+    /* connect to pool/cont then broadcast to rest of ranks */
+    if (rank == 0) {
+        /* generate container UUID */
+        uuid_generate(daos_args->src_cont);
+        daos_pool_info_t pool_info = {0};
+        daos_cont_info_t co_info = {0};
+#if DAOS_API_VERSION_MAJOR < 1
+        rc = daos_pool_connect(daos_args->src_pool, NULL, NULL, DAOS_PC_RW,
+                               &(daos_args->src_poh), &pool_info, NULL);
+#else
+        rc = daos_pool_connect(daos_args->src_pool, NULL, DAOS_PC_RW,
+                               &(daos_args->src_poh), &pool_info, NULL);
+#endif
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to connect to pool: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto out;
+        }
+
+        /* create cont and open */
+        rc = daos_cont_create(daos_args->src_poh, daos_args->src_cont, NULL, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to create cont: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto err_cont;
+        }
+        rc = daos_cont_open(daos_args->src_poh, daos_args->src_cont,
+                            DAOS_COO_RW, &daos_args->src_coh, &co_info, NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to open container: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto out_err;
+        }
+    }
+
+    /* broadcast rank 0's pool and cont handle to everyone else */
+    daos_bcast_handle(rank, &(daos_args->src_poh), &(daos_args->src_poh), POOL_HANDLE); 
+    daos_bcast_handle(rank, &(daos_args->src_coh), &(daos_args->src_poh), CONT_HANDLE); 
+
+    /* TODO: generate different types of data, and different for each key */
+	memset(buf, 'A', BUFLEN);
+    for (i = 0; i < num_objects; i++) {
+        oid[i] = dts_oid_gen(0);
+                                   
+        rc = daos_obj_generate_oid(daos_args->src_coh, &oid[i], ofeats, OC_RP_XSF, 0, 0);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to generate oid: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto out_err;
+        }
+
+    	/** create the KV store */
+        rc = daos_kv_open(daos_args->src_coh, oid[i], DAOS_OO_RW, &oh[i], NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to open kv object: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto out_err;
+        }
+
+        /* insert keys */
+        for (j = 0; j < keys_per_object; j++) {
+            sprintf(key, key_fmt, j);
+            rc = daos_kv_put(oh[i], DAOS_TX_NONE, 0, key, BUFLEN, buf, NULL);
+            if (rc != 0) {
+                MFU_LOG(MFU_LOG_ERR, "Failed to put kv object: "DF_RC, DP_RC(rc));
+                rc = 1;
+                goto out_err;
+            }
+        }
+
+        rc = daos_kv_close(oh[i], NULL);
+        if (rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to close kv object: "DF_RC, DP_RC(rc));
+            rc = 1;
+            goto out_err;
+        }
+    }
+
+    /* calculate total objects and num_keys_per object, each rank
+     * generates same amount */
+    if (rank == 0) {
+        int total_num_objects = size * num_objects; 
+        uuid_unparse(daos_args->src_cont, uuid_str);
+        printf("Container UUID: %s\n\ttotal objects:%d\n"
+               "\tkeys per object:%d\n", uuid_str, total_num_objects, keys_per_object);
+    }
+
+    /* don't close anything until all ranks are done using handles */
+    MPI_Barrier(MPI_COMM_WORLD);
+
+out_err:
+    tmp_rc = daos_cont_close(daos_args->src_coh, NULL);
+    MPI_Barrier(MPI_COMM_WORLD);
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to close container (%d)", rc);
+        rc = 1;
+    }
+
+err_cont:
+    tmp_rc = daos_pool_disconnect(daos_args->src_poh, NULL);
+    MPI_Barrier(MPI_COMM_WORLD);
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to disconnect pool (%d)", rc);
+        rc = 1;
+    }
+
+out:
+    tmp_rc = daos_fini();
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to finalize DAOS "DF_RC, DP_RC(rc));
+        rc = 1;
+    }
+
+    mfu_finalize();
+
+    /* shut down MPI */
+    MPI_Finalize();
+
+    if (rc != 0) {
+        return 1;
+    }
+    return 0;
+}
diff --git a/src/daos-serialize/CMakeLists.txt b/src/daos-serialize/CMakeLists.txt
new file mode 100644
index 0000000..3f9c9f9
--- /dev/null
+++ b/src/daos-serialize/CMakeLists.txt
@@ -0,0 +1 @@
+MFU_ADD_TOOL(daos-serialize)
diff --git a/src/daos-serialize/daos-serialize.c b/src/daos-serialize/daos-serialize.c
new file mode 100644
index 0000000..f70f2c7
--- /dev/null
+++ b/src/daos-serialize/daos-serialize.c
@@ -0,0 +1,302 @@
+#include <stdio.h>
+#include <getopt.h>
+#include <string.h>
+#include <stdlib.h>
+#include <ctype.h>
+#include <inttypes.h>
+#include <unistd.h>
+#include <errno.h>
+#include <stdbool.h>
+
+/* for daos */
+#include "mfu_daos.h"
+#include "mpi.h"
+#include "mfu.h"
+
+#include "mfu_errors.h"
+
+/** Print a usage message. */
+void print_usage(void)
+{
+    printf("\n");
+    printf("Usage: daos-serialize [options] daos://<pool>/<cont>\n");
+    printf("\n");
+    printf("DAOS paths can be specified as:\n");
+    printf("       daos://<pool>/<cont> | <UNS path>\n");
+    printf("\n");
+    printf("Options:\n");
+    printf("  -o  --output-path        - path to output serialized hdf5 files\n");
+    printf("  -v, --verbose            - verbose output\n");
+    printf("  -f, --force              - force serialization even if container has unhealthy status\n");
+    printf("  -q, --quiet              - quiet output\n");
+    printf("  -h, --help               - print usage\n");
+    printf("For more information see https://mpifileutils.readthedocs.io.\n");
+    printf("\n");
+    fflush(stdout);
+}
+
+int main(int argc, char** argv)
+{
+    /* assume we'll exit with success */
+    int rc = 0;
+
+    /* initialize MPI */
+    MPI_Init(&argc, &argv);
+    mfu_init();
+
+    /* get our rank */
+    int rank;
+    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
+
+    /* verbose by default */
+    mfu_debug_level = MFU_LOG_VERBOSE;
+
+    int option_index = 0;
+    static struct option long_options[] = {
+        {"output-path"          , required_argument, 0, 'o'},
+        {"verbose"              , no_argument      , 0, 'v'},
+        {"force"                , no_argument      , 0, 'f'},
+        {"quiet"                , no_argument      , 0, 'q'},
+        {"help"                 , no_argument      , 0, 'h'},
+        {0                      , 0                , 0, 0  }
+    };
+
+    /* Parse options */
+    unsigned long long bytes = 0;
+    int usage = 0;
+    char *output_path = NULL;
+    bool force_serialize = false;
+    while (1) {
+        int c = getopt_long(
+                    argc, argv, "o:vfqh",
+                    long_options, &option_index
+                );
+
+        if (c == -1) {
+            break;
+        }
+
+        switch(c) {
+            case 'o':
+                output_path = MFU_STRDUP(optarg);
+                break;
+            case 'v':
+                mfu_debug_level = MFU_LOG_VERBOSE;
+                break;
+            case 'f':
+                force_serialize = true;
+                break;
+            case 'q':
+                mfu_debug_level = MFU_LOG_NONE;
+                break;
+            case 'h':
+                usage = 1;
+                break;
+            case '?':
+                usage = 1;
+                break;
+            default:
+                if(rank == 0) {
+                    printf("?? getopt returned character code 0%o ??\n", c);
+                }
+        }
+    }
+
+    /* If we need to print the usage
+     * then do so before internal processing */
+    if (usage) {
+        if (rank == 0) {
+            print_usage();
+        }
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    char** argpaths = (&argv[optind]);
+    
+    /* The remaining arguments are treated as src/dst paths */
+    int numpaths = argc - optind;
+
+    /* advance to next set of options */
+    optind += numpaths;
+
+    /* Before processing, make sure we have at least one path */
+    if (numpaths < 1) {
+        MFU_LOG(MFU_LOG_ERR, "At least one pool and container is required:"
+                MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    /* create an empty file list */
+    mfu_flist flist = mfu_flist_new();
+
+    rc = daos_init();
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to initialize daos");
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    /* DAOS vars */ 
+    daos_args_t* daos_args = daos_args_new();    
+
+    int len = strlen(argpaths[0]); 
+
+    int tmp_rc;
+    tmp_rc = daos_parse_path(argpaths[0], len, &daos_args->src_pool, &daos_args->src_cont);
+    if (tmp_rc != 0 || daos_args->src_cont == NULL) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to resolve DAOS path");
+         rc = 1;
+    }
+    
+    tmp_rc = daos_connect(rank, daos_args, &daos_args->src_pool,
+                          &daos_args->src_cont, &daos_args->src_poh,
+                          &daos_args->src_coh, force_serialize, true,
+                          false, false, false, NULL, true);
+    if (tmp_rc != 0) {
+        daos_fini();
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    /* Initialize some stats */
+    mfu_daos_stats_t stats;
+    mfu_daos_stats_init(&stats);
+    mfu_daos_stats_start(&stats);
+
+    /* take a snapshot and walk container to get list of objects,
+     * returns epoch number of snapshot */
+    tmp_rc = mfu_daos_flist_walk(daos_args, daos_args->src_coh, &daos_args->src_epc, flist);
+    if (tmp_rc != 0) {
+        rc = 1;
+    }
+
+    /* all objects are on rank 0 at this point,
+     * evenly spread them among the ranks */
+    mfu_flist newflist = mfu_flist_spread(flist);
+
+    /* get size of local list for each rank */
+    uint64_t size = mfu_flist_size(newflist);
+
+    /* serialize pool/cont to an hdf5 file */
+    uint64_t files_written = 0;
+    struct hdf5_args hdf5;
+
+    /* only create a directory if one is passed in with output_path
+     * option, otherwise use current working dir */
+    if (output_path == NULL) {
+        char cwd[FILENAME_LEN];
+        getcwd(cwd, FILENAME_LEN);
+        if (cwd == NULL) {
+            MFU_LOG(MFU_LOG_ERR, "failed to get current working directory");
+            rc = 1;
+        }
+        output_path = MFU_STRDUP(cwd);
+    } else {
+        tmp_rc = mkdir(output_path, S_IRWXU | S_IRWXG | S_IROTH | S_IXOTH);
+        if (tmp_rc != 0 && errno != EEXIST) {
+            MFU_LOG(MFU_LOG_ERR, "failed to create output directory");
+            rc = 1;
+        }
+    }
+
+    /* don't bother running if this rank doesn't have any oids */
+    if (size > 0) {
+        tmp_rc = daos_cont_serialize_hdlr(rank, &hdf5, output_path, &files_written,
+                                          daos_args, newflist, size, &stats);
+        if (tmp_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Failed to serialize container (%d)", rc);
+            rc = 1;
+        }
+    }
+
+    /* sum files_written across all ranks to get total */
+    uint64_t total_files_written = 0;
+    MPI_Allreduce(&files_written, &total_files_written, 1, MPI_UNSIGNED,
+                  MPI_SUM, MPI_COMM_WORLD);
+
+    /* no file created if this rank received no oids */
+    if (size > 0) {
+        tmp_rc = daos_cont_serialize_files_generated(&hdf5, &total_files_written);
+        if (tmp_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "failed to serialize files generated");
+            rc = 1;
+        }
+    }
+
+    /* Record end time */
+    mfu_daos_stats_end(&stats);
+
+    /* Sum and print the stats */
+    mfu_daos_stats_print_sum(rank, &stats, true, false, false, false);
+
+    /* destroy snapshot after copy */
+    /* TODO consider moving this into mfu_flist_copy_daos */
+    if (rank == 0) {
+        daos_epoch_range_t epr;
+        epr.epr_lo = daos_args->src_epc;
+        epr.epr_hi = daos_args->src_epc;
+        tmp_rc = daos_cont_destroy_snap(daos_args->src_coh, epr, NULL);
+        if (tmp_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "DAOS destroy snapshot failed: ", MFU_ERRF,
+                    MFU_ERRP(-MFU_ERR_DAOS));
+            rc = 1;
+        }
+    }
+
+    /* free flists */
+    mfu_flist_free(&newflist);
+    mfu_flist_free(&flist);
+
+    /* free output path for hdf5 files */
+    mfu_free(&output_path);
+
+    MPI_Barrier(MPI_COMM_WORLD);
+
+    tmp_rc = daos_cont_close(daos_args->src_coh, NULL);
+    MPI_Barrier(MPI_COMM_WORLD);
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to close container (%d)", rc);
+        rc = 1;
+    }
+
+    tmp_rc = daos_pool_disconnect(daos_args->src_poh, NULL);
+    MPI_Barrier(MPI_COMM_WORLD);
+    if (tmp_rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to disconnect pool (%d)", rc);
+        rc = 1;
+    }
+   
+    /* free daos_args */
+    daos_args_delete(&daos_args);
+
+    /* Alert the user if there were copy errors */
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "One or more errors were detected while "
+                "serializing: " MFU_ERRF, MFU_ERRP(MFU_ERR_DAOS));
+    }
+
+    tmp_rc = daos_fini();
+    if (rc != 0) {
+        MFU_LOG(MFU_LOG_ERR, "Failed to finalize DAOS "DF_RC, DP_RC(rc));
+        rc = 1;
+    }
+
+    int global_rc;
+    MPI_Allreduce(&rc, &global_rc, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD);
+
+    mfu_finalize();
+
+    /* shut down MPI */
+    MPI_Finalize();
+
+    if (global_rc != 0) {
+        return 1;
+    }
+    return 0;
+}
diff --git a/src/dcmp/dcmp.c b/src/dcmp/dcmp.c
index c7e3770..cc4deb0 100644
--- a/src/dcmp/dcmp.c
+++ b/src/dcmp/dcmp.c
@@ -42,7 +42,6 @@ static void print_usage(void)
     printf("      --bufsize <SIZE>      - IO buffer size in bytes (default " MFU_BUFFER_SIZE_STR ")\n");
     printf("      --chunksize <SIZE>    - minimum work size per task in bytes (default " MFU_CHUNK_SIZE_STR ")\n");
 #ifdef DAOS_SUPPORT
-    printf("      --daos-prefix         - DAOS prefix for unified namespace path\n");
     printf("      --daos-api            - DAOS API in {DFS, DAOS} (default uses DFS for POSIX containers)\n");
 #endif
     printf("  -s, --direct              - open files with O_DIRECT\n");
@@ -2287,7 +2286,7 @@ int main(int argc, char **argv)
     bool daos_do_exit = false;
     
     /* Set up DAOS arguments, containers, dfs, etc. */
-    int daos_rc = daos_setup(rank, argpaths, daos_args, mfu_src_file, mfu_dst_file);
+    int daos_rc = daos_setup(rank, argpaths, numargs, daos_args, mfu_src_file, mfu_dst_file);
     if (daos_rc != 0) {
         daos_do_exit = true;
     }
diff --git a/src/dcp/dcp.c b/src/dcp/dcp.c
index 813a667..fc47f19 100644
--- a/src/dcp/dcp.c
+++ b/src/dcp/dcp.c
@@ -79,14 +79,18 @@ void print_usage(void)
 #endif
     printf("  -b, --bufsize <SIZE>     - IO buffer size in bytes (default " MFU_BUFFER_SIZE_STR ")\n");
     printf("  -k, --chunksize <SIZE>   - work size per task in bytes (default " MFU_CHUNK_SIZE_STR ")\n");
+    printf("  -X, --xattrs <OPT>       - copy xattrs (none, all, non-lustre, libattr)\n");
 #ifdef DAOS_SUPPORT
-    printf("      --daos-prefix        - DAOS prefix for unified namespace path\n");
     printf("      --daos-api           - DAOS API in {DFS, DAOS} (default uses DFS for POSIX containers)\n");
+#ifdef HDF5_SUPPORT
+    printf("      --daos-preserve      - preserve DAOS container properties and user attributes, a filename "
+    					 "to write the metadata to is expected\n");
+#endif
 #endif
     printf("  -i, --input <file>       - read source list from file\n");
     printf("  -L, --dereference        - copy original files instead of links\n");
     printf("  -P, --no-dereference     - don't follow links in source\n");
-    printf("  -p, --preserve           - preserve permissions, ownership, timestamps, extended attributes\n");
+    printf("  -p, --preserve           - preserve permissions, ownership, timestamps (see also --xattrs)\n");
     printf("  -s, --direct             - open files with O_DIRECT\n");
     printf("  -S, --sparse             - create sparse files when possible\n");
     printf("      --progress <N>       - print progress every N seconds\n");
@@ -141,10 +145,12 @@ int main(int argc, char** argv)
         {"bufsize"              , required_argument, 0, 'b'},
         {"debug"                , required_argument, 0, 'd'}, // undocumented
         {"grouplock"            , required_argument, 0, 'g'}, // untested
-        {"daos-prefix"          , required_argument, 0, 'X'},
-        {"daos-api"             , required_argument, 0, 'x'},
+        {"daos-prefix"          , required_argument, 0, 'Y'},
+        {"daos-api"             , required_argument, 0, 'y'},
+        {"daos-preserve"        , required_argument, 0, 'D'},
         {"input"                , required_argument, 0, 'i'},
         {"chunksize"            , required_argument, 0, 'k'},
+        {"xattrs"               , required_argument, 0, 'X'},
         {"dereference"          , no_argument      , 0, 'L'},
         {"no-dereference"       , no_argument      , 0, 'P'},
         {"preserve"             , no_argument      , 0, 'p'},
@@ -163,7 +169,7 @@ int main(int argc, char** argv)
     int usage = 0;
     while(1) {
         int c = getopt_long(
-                    argc, argv, "b:d:g:i:k:LPpsSvqh",
+                    argc, argv, "b:d:g:i:k:LPpsSvqhX:",
                     long_options, &option_index
                 );
 
@@ -226,6 +232,15 @@ int main(int argc, char** argv)
                     }
                 }
                 break;
+            case 'X':
+                mfu_copy_opts->copy_xattrs = parse_copy_xattrs_option(optarg);
+                if (mfu_copy_opts->copy_xattrs == XATTR_COPY_INVAL) {
+                    if (rank == 0) {
+                        MFU_LOG(MFU_LOG_ERR, "Unrecognized option '%s' for --xattrs", optarg);
+                    }
+                    usage = 1;
+                }
+                break;
 #ifdef LUSTRE_SUPPORT
             case 'g':
                 mfu_copy_opts->grouplock_id = atoi(optarg);
@@ -236,15 +251,22 @@ int main(int argc, char** argv)
                 break;
 #endif
 #ifdef DAOS_SUPPORT
-            case 'X':
+            case 'Y':
                 daos_args->dfs_prefix = MFU_STRDUP(optarg);
                 break;
-            case 'x':
+            case 'y':
                 if (daos_parse_api_str(optarg, &daos_args->api) != 0) {
                     MFU_LOG(MFU_LOG_ERR, "Failed to parse --daos-api");
                     usage = 1;
                 }
                 break;
+#ifdef HDF5_SUPPORT
+            /* daos_preserve needs hdf5 support */
+            case 'D':
+                daos_args->daos_preserve      = true;
+                daos_args->daos_preserve_path = MFU_STRDUP(optarg);
+                break;
+#endif
 #endif
             case 'i':
                 inputname = MFU_STRDUP(optarg);
@@ -358,17 +380,31 @@ int main(int argc, char** argv)
 
 #ifdef DAOS_SUPPORT
     /* Set up DAOS arguments, containers, dfs, etc. */
-    rc = daos_setup(rank, argpaths, daos_args, mfu_src_file, mfu_dst_file);
-    if (rc != 0) {
+    if (daos_args->api != DAOS_API_HDF5) {
+        rc = daos_setup(rank, argpaths, numpaths, daos_args, mfu_src_file, mfu_dst_file);
+    }
+
+#ifdef HDF5_SUPPORT
+    /* if hdf5 API is specified, then h5repack is used */
+    if (daos_args->api == DAOS_API_HDF5) {
+        rc = mfu_daos_hdf5_copy(argpaths, daos_args);
+        if (rc != 0) {
+            rc = 1;
+        }
         mfu_finalize();
         MPI_Finalize();
-        return 1;
+        return rc;
     }
-    
+#endif
+
     /* TODO add support for this */
     if (inputname && mfu_src_file->type == DFS) {
         MFU_LOG(MFU_LOG_ERR, "--input is not supported with DAOS"
                 MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        rc = 1;
+    }
+
+    if (rc != 0) {
         daos_cleanup(daos_args, mfu_src_file, mfu_dst_file);
         mfu_finalize();
         MPI_Finalize();
@@ -376,16 +412,13 @@ int main(int argc, char** argv)
     }
 #endif
 
-    /* paths to walk come after the options */
-    mfu_param_path* paths = NULL;
-
     /* create an empty file list */
     mfu_flist flist = mfu_flist_new();
 
     /* Perform a POSIX copy for non-DAOS types */
     if (mfu_src_file->type != DAOS && mfu_dst_file->type != DAOS) {
         /* allocate space for each path */
-        paths = (mfu_param_path*) MFU_MALLOC((size_t)numpaths * sizeof(mfu_param_path));
+        mfu_param_path* paths = (mfu_param_path*) MFU_MALLOC((size_t)numpaths * sizeof(mfu_param_path));
 
         /* last item in the list is the destination path */
         mfu_param_path* destpath = &paths[numpaths - 1];
@@ -417,7 +450,7 @@ int main(int argc, char** argv)
             mfu_param_path_free_all(numpaths, paths);
             mfu_free(&paths);
 #ifdef DAOS_SUPPORT
-	    daos_cleanup(daos_args, mfu_src_file, mfu_dst_file);
+            daos_cleanup(daos_args, mfu_src_file, mfu_dst_file);
 #endif
             mfu_finalize();
             MPI_Finalize();
@@ -451,68 +484,48 @@ int main(int argc, char** argv)
             rc = 1;
         }
 
-        /* free the file list */
-        mfu_flist_free(&flist);
-
         /* free the path parameters */
         mfu_param_path_free_all(numpaths, paths);
 
         /* free memory allocated to hold params */
         mfu_free(&paths);
-
-        /* free the input file name */
-        mfu_free(&inputname);
     } 
 #ifdef DAOS_SUPPORT
     /* Perform an object-level copy for DAOS types */
-    /* TODO consider moving most of this into mfu_daos as a single call.
-     * The benefit would be that we could reuse it for dsync. */
     else {
         /* take a snapshot and walk container to get list of objects,
          * returns epoch number of snapshot */
-        int tmp_rc = mfu_flist_walk_daos(daos_args, flist);
+        int tmp_rc = mfu_daos_flist_walk(daos_args, daos_args->src_coh,
+                                         &daos_args->src_epc, flist);
         if (tmp_rc != 0) {
             rc = 1;
+            goto daos_cleanup;
         }
 
-        /* all objects are on rank 0 at this point,
-         * evenly spread them among the ranks */
-        mfu_flist newflist = mfu_flist_spread(flist);
-
-        /* perform copy after oids are spread evenly across all ranks */
-        tmp_rc = mfu_flist_copy_daos(daos_args, newflist);
+        /* Collectively copy all objects */
+        tmp_rc = mfu_daos_flist_sync(daos_args, flist, false, true);
         if (tmp_rc != 0) {
             rc = 1;
+            goto daos_cleanup;
         }
 
         /* Rank 0 prints success if needed */
         if (rc == 0 && rank == 0) {
             MFU_LOG(MFU_LOG_INFO, "Successfully copied to DAOS Destination Container.");
         }
-
-        /* destroy snapshot after copy */
-        /* TODO consider moving this into mfu_flist_copy_daos */
-        if (rank == 0) {
-            daos_epoch_range_t epr;
-            epr.epr_lo = daos_args->epc;
-            epr.epr_hi = daos_args->epc;
-            rc = daos_cont_destroy_snap(daos_args->src_coh, epr, NULL);
-            if (rc != 0) {
-                MFU_LOG(MFU_LOG_ERR, "DAOS destroy snapshot failed: ", MFU_ERRF,
-                        MFU_ERRP(-MFU_ERR_DAOS));
-                rc = 1;
-            }
-        }
-        MPI_Bcast(&rc, 1, MPI_INT, 0, MPI_COMM_WORLD);
-
-	    /* free newflist that was created for non-posix copy */
-	    mfu_flist_free(&newflist);
     }
 
+daos_cleanup:
     /* Cleanup DAOS-related variables, etc. */
     daos_cleanup(daos_args, mfu_src_file, mfu_dst_file);
 #endif
 
+    /* free the file list */
+    mfu_flist_free(&flist);
+
+    /* free the input file name */
+    mfu_free(&inputname);
+
     /* free the copy options */
     mfu_copy_opts_delete(&mfu_copy_opts);
 
diff --git a/src/dcp1/CMakeLists.txt b/src/dcp1/CMakeLists.txt
index ba72276..07c59c0 100644
--- a/src/dcp1/CMakeLists.txt
+++ b/src/dcp1/CMakeLists.txt
@@ -1,4 +1,4 @@
-INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR})
+INCLUDE_DIRECTORIES(BEFORE ${CMAKE_CURRENT_SOURCE_DIR})
 
 # MFU_ADD_TOOL(dcp1)
 
diff --git a/src/dfind/dfind.c b/src/dfind/dfind.c
index 3137188..2cfe20e 100644
--- a/src/dfind/dfind.c
+++ b/src/dfind/dfind.c
@@ -13,8 +13,13 @@
 #include "mpi.h"
 
 #include "mfu.h"
+#include "mfu_errors.h"
 #include "common.h"
 
+#ifdef DAOS_SUPPORT
+#include "mfu_daos.h"
+#endif
+
 int MFU_PRED_EXEC  (mfu_flist flist, uint64_t idx, void* arg);
 int MFU_PRED_PRINT (mfu_flist flist, uint64_t idx, void* arg);
 
@@ -145,13 +150,19 @@ static void print_usage(void)
 {
     printf("\n");
     printf("Usage: dfind [options] <path> EXPRESSIONS...\n");
+#ifdef DAOS_SUPPORT
+    printf("\n");
+    printf("DAOS paths can be specified as:\n");
+    printf("       daos://<pool>/<cont>[/<path>] | <UNS path>\n");
+#endif
     printf("\n");
     printf("Options:\n");
-    printf("  -i, --input <file>                      - read list from file\n");
-    printf("  -o, --output <file>                     - write processed list to file\n");
-    printf("  -v, --verbose                           - verbose output\n");
-    printf("  -q, --quiet                             - quiet output\n");
-    printf("  -h, --help                              - print usage\n");
+    printf("  -i, --input <file>      - read list from file\n");
+    printf("  -o, --output <file>     - write processed list to file\n");
+    printf("  -t, --text              - use with -o; write processed list to file in ascii format\n");
+    printf("  -v, --verbose           - verbose output\n");
+    printf("  -q, --quiet             - quiet output\n");
+    printf("  -h, --help              - print usage\n");
     printf("\n");
     printf("Tests:\n");
     printf("  --amin N       - last accessed N minutes ago\n");
@@ -297,13 +308,20 @@ int main (int argc, char** argv)
     char* outputname = NULL;
     int walk = 0;
     int text = 0;
+    int rc = 0;
+
+#ifdef DAOS_SUPPORT
+    /* DAOS vars */
+    daos_args_t* daos_args = daos_args_new();
+#endif
 
     static struct option long_options[] = {
-        {"input",     1, 0, 'i'},
-        {"output",    1, 0, 'o'},
-        {"verbose",   0, 0, 'v'},
-        {"quiet",     0, 0, 'q'},
-        {"help",      0, 0, 'h'},
+        {"input",       1, 0, 'i'},
+        {"output",      1, 0, 'o'},
+        {"text",        0, 0, 't'},
+        {"verbose",     0, 0, 'v'},
+        {"quiet",       0, 0, 'q'},
+        {"help",        0, 0, 'h'},
 
         { "maxdepth", required_argument, NULL, 'd' },
 
@@ -326,7 +344,7 @@ int main (int argc, char** argv)
         { "path",     required_argument, NULL, 'P' },
         { "regex",    required_argument, NULL, 'r' },
         { "size",     required_argument, NULL, 's' },
-        { "type",     required_argument, NULL, 't' },
+        { "type",     required_argument, NULL, 'T' },
 
         { "print",    no_argument,       NULL, 'p' },
         { "exec",     required_argument, NULL, 'e' },
@@ -338,7 +356,7 @@ int main (int argc, char** argv)
     int usage = 0;
     while (1) {
         int c = getopt_long(
-                    argc, argv, "i:o:vqh",
+                    argc, argv, "i:o:tvqh",
                     long_options, NULL
                 );
 
@@ -507,7 +525,7 @@ int main (int argc, char** argv)
     	    mfu_pred_add(pred_head, MFU_PRED_PRINT, NULL);
     	    break;
 
-    	case 't':
+    	case 'T':
             ret = add_type(pred_head, *optarg);
             if (ret != 1) {
                 if (rank == 0) {
@@ -516,13 +534,15 @@ int main (int argc, char** argv)
     	        exit(1);
             }
     	    break;
-
         case 'i':
             inputname = MFU_STRDUP(optarg);
             break;
         case 'o':
             outputname = MFU_STRDUP(optarg);
             break;
+        case 't':
+            text = 1;
+            break;
         case 'v':
             mfu_debug_level = MFU_LOG_VERBOSE;
             break;
@@ -542,25 +562,77 @@ int main (int argc, char** argv)
     	}
     }
 
+    /* print usage if we need to */
+    if (usage) {
+        if (rank == 0) {
+            print_usage();
+        }
+        mfu_file_delete(&mfu_file);
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    char** argpaths = &argv[optind];
+
+    /* The remaining arguments are treated as paths */
+    int numpaths = argc - optind;
+
+    /* advance to next set of options */
+    optind += numpaths;
+
+#ifdef DAOS_SUPPORT
+    /* Set up DAOS arguments, containers, dfs, etc. */
+    rc = daos_setup(rank, argpaths, numpaths, daos_args, mfu_file, NULL);
+    if (rc != 0) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "Detected one or more DAOS errors: "MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+    if (inputname && mfu_file->type == DFS) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "--input is not supported with DAOS"
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+    /* Not yet supported */
+    if (mfu_file->type == DAOS) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "dfind only supports DAOS POSIX containers with the DFS API.");
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+daos_setup_done:
+    if (rc != 0) {
+        daos_cleanup(daos_args, mfu_file, NULL);
+        mfu_file_delete(&mfu_file);
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+#endif
+
     pred_commit(pred_head);
 
     /* paths to walk come after the options */
-    int numpaths = 0;
     mfu_param_path* paths = NULL;
-    if (optind < argc) {
+    if (numpaths > 0) {
         /* got a path to walk */
         walk = 1;
 
-        /* determine number of paths specified by user */
-        numpaths = argc - optind;
-
         /* allocate space for each path */
         paths = (mfu_param_path*) MFU_MALLOC((size_t)numpaths * sizeof(mfu_param_path));
 
         /* process each path */
-        char** p = &argv[optind];
-        mfu_param_path_set_all((uint64_t)numpaths, (const char**)p, paths, mfu_file, true);
-        optind += numpaths;
+        mfu_param_path_set_all((uint64_t)numpaths, (const char**)argpaths, paths, mfu_file, true);
 
         /* don't allow user to specify input file with walk */
         if (inputname != NULL) {
@@ -571,6 +643,9 @@ int main (int argc, char** argv)
         /* if we're not walking, we must be reading,
          * and for that we need a file */
         if (inputname == NULL) {
+            if (rank == 0) {
+                MFU_LOG(MFU_LOG_ERR, "Either a <path> or --input is required.");
+            }
             usage = 1;
         }
     }
@@ -579,6 +654,9 @@ int main (int argc, char** argv)
         if (rank == 0) {
             print_usage();
         }
+#ifdef DAOS_SUPPORT
+        daos_cleanup(daos_args, mfu_file, NULL);
+#endif
         mfu_file_delete(&mfu_file);
         mfu_finalize();
         MPI_Finalize();
@@ -610,6 +688,10 @@ int main (int argc, char** argv)
         }
     }
 
+#ifdef DAOS_SUPPORT
+    daos_cleanup(daos_args, mfu_file, NULL);
+#endif
+
     /* free off the filtered list */
     mfu_flist_free(&flist2);
 
@@ -642,5 +724,5 @@ int main (int argc, char** argv)
     mfu_finalize();
     MPI_Finalize();
 
-    return 0;
+    return rc;
 }
diff --git a/src/drm/drm.c b/src/drm/drm.c
index 083fc8a..949c727 100644
--- a/src/drm/drm.c
+++ b/src/drm/drm.c
@@ -17,9 +17,14 @@
 
 #include <libgen.h> /* dirname */
 
+#ifdef DAOS_SUPPORT
+#include "mfu_daos.h"
+#endif
+
 #include "libcircle.h"
 #include "dtcmp.h"
 #include "mfu.h"
+#include "mfu_errors.h"
 
 /*****************************
  * Driver functions
@@ -29,6 +34,11 @@ static void print_usage(void)
 {
     printf("\n");
     printf("Usage: drm [options] <path> ...\n");
+#ifdef DAOS_SUPPORT
+    printf("\n");
+    printf("DAOS paths can be specified as:\n");
+    printf("       daos://<pool>/<cont>[/<path>] | <UNS path>\n");
+#endif
     printf("\n");
     printf("Options:\n");
     printf("  -i, --input   <file>   - read list from file\n");
@@ -55,6 +65,7 @@ static void print_usage(void)
 int main(int argc, char** argv)
 {
     int i;
+    int rc = 0;
 
     /* initialize MPI */
     MPI_Init(&argc, &argv);
@@ -79,6 +90,11 @@ int main(int argc, char** argv)
     int traceless    = 0;
     int text         = 0;
 
+#ifdef DAOS_SUPPORT
+    /* DAOS vars */
+    daos_args_t* daos_args = daos_args_new();
+#endif
+
     /* with drm, we don't stat files on walk by default,
      * since that info is not needed to remove items and
      * avoiding the stat can significantly speed up the walk */
@@ -197,28 +213,76 @@ int main(int argc, char** argv)
         usage = 1;
     }
 
+    /* print usage if we need to */
+    if (usage) {
+        if (rank == 0) {
+            print_usage();
+        }
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
     /* create new mfu_file objects */
     mfu_file_t* mfu_file = mfu_file_new();
 
-    /* paths to walk come after the options */
-    int numpaths = 0;
+    char** argpaths = &argv[optind];
+
+    /* The remaining arguments are treated as paths */
+    int numpaths = argc - optind;
+
+    /* advance to next set of options */
+    optind += numpaths;
+
+#ifdef DAOS_SUPPORT
+    /* Set up DAOS arguments, containers, dfs, etc. */
+    rc = daos_setup(rank, argpaths, numpaths, daos_args, mfu_file, NULL);
+    if (rc != 0) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "Detected one or more DAOS errors: "MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+    if (inputname && mfu_file->type == DFS) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "--input is not supported with DAOS"
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+    /* Not yet supported */
+    if (mfu_file->type == DAOS) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "drm only supports DAOS POSIX containers with the DFS API.");
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+daos_setup_done:
+    if (rc != 0) {
+        daos_cleanup(daos_args, mfu_file, NULL);
+        mfu_file_delete(&mfu_file);
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+#endif
+
     mfu_param_path* paths = NULL;
-    if (optind < argc) {
+    if (numpaths > 0) {
         /* got a path to walk */
         walk = 1;
 
-        /* determine number of paths specified by user */
-        numpaths = argc - optind;
-
         /* allocate space for each path */
         paths = (mfu_param_path*) MFU_MALLOC((size_t)numpaths * sizeof(mfu_param_path));
 
         /* process each path */
-        const char** argpaths = (const char**)(&argv[optind]);
-        mfu_param_path_set_all(numpaths, argpaths, paths, mfu_file, true);
-
-        /* advance to next set of options */
-        optind += numpaths;
+        mfu_param_path_set_all(numpaths, (const char**)argpaths, paths, mfu_file, true);
 
         /* don't allow input file and walk */
         if (inputname != NULL) {
@@ -229,6 +293,9 @@ int main(int argc, char** argv)
         /* if we're not walking, we must be reading,
          * and for that we need a file */
         if (inputname == NULL) {
+            if (rank == 0) {
+                MFU_LOG(MFU_LOG_ERR, "Either a <path> or --input is required.");
+            }
             usage = 1;
         }
     }
@@ -252,6 +319,9 @@ int main(int argc, char** argv)
         if (rank == 0) {
             print_usage();
         }
+#ifdef DAOS_SUPPORT
+        daos_cleanup(daos_args, mfu_file, NULL);
+#endif
         mfu_file_delete(&mfu_file);
         mfu_finalize();
         MPI_Finalize();
@@ -304,6 +374,10 @@ int main(int argc, char** argv)
         }
     }
 
+#ifdef DAOS_SUPPORT
+    daos_cleanup(daos_args, mfu_file, NULL);
+#endif
+
     /* free list if it was used */
     if (filtered_flist != MFU_FLIST_NULL) {
         /* free the filtered flist (if any) */
@@ -338,5 +412,5 @@ int main(int argc, char** argv)
     mfu_finalize();
     MPI_Finalize();
 
-    return 0;
+    return rc;
 }
diff --git a/src/dsync/dsync.c b/src/dsync/dsync.c
index e14c9f5..ac98e14 100644
--- a/src/dsync/dsync.c
+++ b/src/dsync/dsync.c
@@ -41,6 +41,8 @@
 #include "strmap.h"
 #include "list.h"
 
+#include "mfu_errors.h"
+
 /* for daos */
 #ifdef DAOS_SUPPORT
 #include "mfu_daos.h"
@@ -61,8 +63,8 @@ static void print_usage(void)
     printf("  -b  --batch-files <N>   - batch files into groups of N during copy\n");
     printf("      --bufsize <SIZE>    - IO buffer size in bytes (default " MFU_BUFFER_SIZE_STR ")\n");
     printf("      --chunksize <SIZE>  - minimum work size per task in bytes (default " MFU_CHUNK_SIZE_STR ")\n");
+    printf("  -X, --xattrs <OPT>      - copy xattrs (none, all, non-lustre, libattr)\n");
 #ifdef DAOS_SUPPORT
-    printf("      --daos-prefix       - DAOS prefix for unified namespace path \n");
     printf("      --daos-api          - DAOS API in {DFS, DAOS} (default uses DFS for POSIX containers)\n");
 #endif
     printf("  -c, --contents          - read and compare file contents rather than compare size and mtime\n");
@@ -101,7 +103,7 @@ typedef enum _dsync_state {
       * This file only exist in dest directory.
       * Only valid for DCMPF_EXIST.
       * Not used yet,
-      * becuase we don't want to waste a loop in dsync_strmap_compare()
+      * because we don't want to waste a loop in dsync_strmap_compare()
       */
     DCMPS_ONLY_DEST,
 
@@ -1921,6 +1923,10 @@ static int dsync_strmap_compare(
         }
     }
 
+    if (rank == 0) {
+        MFU_LOG(MFU_LOG_INFO, "Completed updating timestamps");
+    }
+
     /* done with our list of files for refreshing metadata */
     strmap_delete(&metadata_refresh);
 
@@ -2869,6 +2875,95 @@ static int dsync_validate_link_dest(const char *link_dest, const char *dest, mfu
     }
 }
 
+#ifdef DAOS_SUPPORT
+/* Setup DAOS for dsync */
+static int dsync_daos_setup(
+    int rank,
+    daos_args_t* daos_args,
+    char** argpaths,
+    int numpaths,
+    mfu_file_t* mfu_src_file,
+    mfu_file_t* mfu_dst_file)
+{
+    /* For error handling */
+    bool daos_do_cleanup = false;
+    bool daos_do_exit = false;
+
+    /* Always allow the destination to exist. */
+    daos_args->allow_exist_dst_cont = true;
+
+    /* Set up DAOS arguments, containers, dfs, etc. */
+    int daos_rc = daos_setup(rank, argpaths, numpaths, daos_args, mfu_src_file, mfu_dst_file);
+    if (daos_rc != 0) {
+        return 1;
+    }
+
+    /* DAOS does not support hard links */
+    if (options.link_dest != NULL &&
+            (mfu_src_file->type == DAOS || mfu_src_file->type == DFS ||
+             mfu_dst_file->type == DAOS || mfu_dst_file->type == DFS)) {
+        MFU_LOG(MFU_LOG_ERR, "DAOS does not support --link-dest.");
+        return 1;
+    }
+
+    /* Not yet supported */
+    if (options.delete && (mfu_src_file->type == DAOS || mfu_dst_file->type == DAOS)) {
+        MFU_LOG(MFU_LOG_ERR, "DAOS API does not support --delete.");
+        return 1;
+    }
+
+    return 0;
+}
+
+/* Perform a DAOS object sync */
+static int dsync_daos(
+    int rank,
+    daos_args_t* daos_args,
+    mfu_copy_opts_t* copy_opts,
+    mfu_file_t* mfu_src_file,
+    mfu_file_t* mfu_dst_file)
+{
+    /* always compare contents */
+    /* TODO DAOS figure out a reliable "lite" comparison */
+    options.contents = 1;
+
+    mfu_flist flist = mfu_flist_new();
+
+    if (rank == 0) {
+        MFU_LOG(MFU_LOG_INFO, "Gathering source objects");
+    }
+    int rc = mfu_daos_flist_walk(daos_args, daos_args->src_coh,
+                                 &daos_args->src_epc, flist);
+    if (rc != 0) {
+        rc = 1;
+        goto dsync_daos_out;
+    }
+
+    if (mfu_flist_global_size(flist) == 0) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "ERROR: No objects found in source container.");
+        }
+        rc = 1;
+        goto dsync_daos_out;
+    }
+
+    /* Collectively copy all objects */
+    if (rank == 0) {
+        MFU_LOG(MFU_LOG_INFO, "Copying source objects");
+    }
+    bool compare_dst = options.contents;
+    bool write_dst = !options.dry_run;
+    int tmp_rc = mfu_daos_flist_sync(daos_args, flist, compare_dst, write_dst);
+    if (tmp_rc != 0) {
+        rc = 1;
+    }
+
+dsync_daos_out:
+    mfu_flist_free(&flist);
+    return rc;
+}
+#endif
+
 int main(int argc, char **argv)
 {
     int rc = 0;
@@ -2926,8 +3021,9 @@ int main(int argc, char **argv)
         {"batch-files",    1, 0, 'b'},
         {"bufsize",        1, 0, 'B'},
         {"chunksize",      1, 0, 'k'},
-        {"daos-prefix",    1, 0, 'X'},
-        {"daos-api",       1, 0, 'x'},
+        {"xattrs",         1, 0, 'X'},
+        {"daos-prefix",    1, 0, 'Y'},
+        {"daos-api",       1, 0, 'y'},
         {"contents",       0, 0, 'c'},
         {"delete",         0, 0, 'D'},
         {"dereference",    0, 0, 'L'},
@@ -2956,7 +3052,7 @@ int main(int argc, char **argv)
 
     while (1) {
         int c = getopt_long(
-            argc, argv, "b:cDso:LPSvqh",
+            argc, argv, "b:cDso:LPSvqhX:",
             long_options, &option_index
         );
 
@@ -2990,11 +3086,20 @@ int main(int argc, char **argv)
                 copy_opts->chunk_size = bytes;
             }
             break;
-#ifdef DAOS_SUPPORT
         case 'X':
+            copy_opts->copy_xattrs = parse_copy_xattrs_option(optarg);
+            if (copy_opts->copy_xattrs == XATTR_COPY_INVAL) {
+                if (rank == 0) {
+                    MFU_LOG(MFU_LOG_ERR, "Unrecognized option '%s' for --xattrs", optarg);
+                }
+                usage = 1;
+            }
+            break;
+#ifdef DAOS_SUPPORT
+        case 'Y':
             daos_args->dfs_prefix = MFU_STRDUP(optarg);
             break;
-        case 'x':
+        case 'y':
             if (daos_parse_api_str(optarg, &daos_args->api) != 0) {
                 MFU_LOG(MFU_LOG_ERR, "Failed to parse --daos-api");
                 usage = 1;
@@ -3108,56 +3213,35 @@ int main(int argc, char **argv)
         if (rank == 0) {
             print_usage();
         }
-        dsync_option_fini();
-        mfu_finalize();
-        MPI_Finalize();
-        return 1;
+        rc = 1;
+        goto dsync_common_cleanup;
     }
 
-    /* allocate space for each path */
-    mfu_param_path* paths = (mfu_param_path*) MFU_MALLOC((size_t)numargs * sizeof(mfu_param_path));
-
     /* pointer to path arguments */
     char** argpaths = &argv[optind];
 
 #ifdef DAOS_SUPPORT
-    /* For error handling */
-    bool daos_do_cleanup = false;
-    bool daos_do_exit = false;
-
-    /* Set up DAOS arguments, containers, dfs, etc. */
-    int daos_rc = daos_setup(rank, argpaths, daos_args, mfu_src_file, mfu_dst_file);
+    int daos_rc = dsync_daos_setup(rank, daos_args, argpaths, numargs, mfu_src_file, mfu_dst_file);
     if (daos_rc != 0) {
-        daos_do_exit = true;
-    }
-
-    /* DAOS does not support hard links */
-    if (options.link_dest != NULL &&
-            (mfu_src_file->type == DAOS || mfu_src_file->type == DFS) &&
-            (mfu_dst_file->type == DAOS || mfu_dst_file->type == DFS)) {
-        MFU_LOG(MFU_LOG_ERR, "DAOS does not support hard links.");
-        daos_do_cleanup = true;
-        daos_do_exit = true;
+        MFU_LOG(MFU_LOG_ERR, "Detected one or more DAOS errors: "MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+        rc = 1;
+        goto dsync_common_cleanup;
     }
 
-    /* Not yet supported */
+    /* Handle the DAOS API case */
     if (mfu_src_file->type == DAOS || mfu_dst_file->type == DAOS) {
-        MFU_LOG(MFU_LOG_ERR, "dysnc only supports DAOS POSIX containers with the DFS API.");
-        daos_do_cleanup = true;
-        daos_do_exit = true;
-    }
-
-    if (daos_do_cleanup) {
-        daos_cleanup(daos_args, mfu_src_file, mfu_dst_file);
-    }
-    if (daos_do_exit) {
-        dsync_option_fini();
-        mfu_finalize();
-        MPI_Finalize();
-        return 1;
+        daos_rc = dsync_daos(rank, daos_args, copy_opts, mfu_src_file, mfu_dst_file);
+        if (daos_rc != 0) {
+            MFU_LOG(MFU_LOG_ERR, "Detected one or more DAOS errors: "MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+            rc = 1;
+        }
+        goto dsync_common_cleanup;
     }
 #endif
 
+    /* allocate space for each path */
+    mfu_param_path* paths = (mfu_param_path*) MFU_MALLOC((size_t)numargs * sizeof(mfu_param_path));
+
     /* first item is source and second is dest */
     mfu_param_path* srcpath  = &paths[0];
     mfu_param_path* destpath = &paths[1];
@@ -3219,15 +3303,8 @@ int main(int argc, char **argv)
             mfu_param_path_free(linkpath);
             mfu_free(&linkpath);
         }
-        mfu_param_path_free_all(numargs, paths);
-        mfu_free(&paths);
-#ifdef DAOS_SUPPORT
-        daos_cleanup(daos_args, mfu_src_file, mfu_dst_file);
-#endif
-        dsync_option_fini();
-        mfu_finalize();
-        MPI_Finalize();
-        return 1;
+        rc = 1;
+        goto dsync_common_cleanup;
     }
 
     /* walk destinaton path.
@@ -3304,18 +3381,20 @@ int main(int argc, char **argv)
         mfu_flist_free(&flist_link);
     }
 
-    /* free all param paths */
-    mfu_param_path_free_all(numargs, paths);
-
-    /* free memory allocated to hold params */
-    mfu_free(&paths);
-
     /* free param path for link-dest if we have one */
     if (options.link_dest != NULL) {
         mfu_param_path_free(linkpath);
         mfu_free(&linkpath);
     }
 
+    /* free all param paths */
+    mfu_param_path_free_all(numargs, paths);
+
+    /* free memory allocated to hold params */
+    mfu_free(&paths);
+
+    /* Common cleanup for all APIs and early exit conditions */
+dsync_common_cleanup:
     dsync_option_fini();
 
     /* free the copy options structure */
@@ -3333,6 +3412,10 @@ int main(int argc, char **argv)
     mfu_file_delete(&mfu_src_file);
     mfu_file_delete(&mfu_dst_file);
 
+    if (rank == 0) {
+        MFU_LOG(MFU_LOG_INFO, "Completed sync");
+    }
+
     /* shut down */
     mfu_finalize();
     MPI_Finalize();
diff --git a/src/dtar/CMakeLists.txt b/src/dtar/CMakeLists.txt
index de4db8c..c1fa5e0 100644
--- a/src/dtar/CMakeLists.txt
+++ b/src/dtar/CMakeLists.txt
@@ -1,4 +1,4 @@
-INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR})
+INCLUDE_DIRECTORIES(BEFORE ${CMAKE_CURRENT_SOURCE_DIR})
 
 # MFU_ADD_TOOL(dtar)
 ADD_EXECUTABLE(dtar dtar.c)
diff --git a/src/dwalk/dwalk.c b/src/dwalk/dwalk.c
index 467b992..3676c62 100644
--- a/src/dwalk/dwalk.c
+++ b/src/dwalk/dwalk.c
@@ -24,6 +24,11 @@
 #include "dtcmp.h"
 #include "mfu.h"
 #include "mfu_flist.h"
+#include "mfu_errors.h"
+
+#ifdef DAOS_SUPPORT
+#include "mfu_daos.h"
+#endif
 
 // getpwent getgrent to read user and group entries
 
@@ -299,6 +304,11 @@ static void print_usage(void)
 {
     printf("\n");
     printf("Usage: dwalk [options] <path> ...\n");
+#ifdef DAOS_SUPPORT
+    printf("\n");
+    printf("DAOS paths can be specified as:\n");
+    printf("       daos://<pool>/<cont>[/<path>] | <UNS path>\n");
+#endif
     printf("\n");
     printf("Options:\n");
     printf("  -i, --input <file>      - read list from file\n");
@@ -325,6 +335,7 @@ static void print_usage(void)
 int main(int argc, char** argv)
 {
     int i;
+    int rc = 0;
 
     /* initialize MPI */
     MPI_Init(&argc, &argv);
@@ -338,6 +349,11 @@ int main(int argc, char** argv)
     /* pointer to mfu_walk_opts */
     mfu_walk_opts_t* walk_opts = mfu_walk_opts_new();
 
+#ifdef DAOS_SUPPORT
+    /* DAOS vars */
+    daos_args_t* daos_args = daos_args_new();
+#endif
+
     /* TODO: extend options
      *   - allow user to cache scan result in file
      *   - allow user to load cached scan as input
@@ -452,26 +468,74 @@ int main(int argc, char** argv)
         usage = 1;
     }
 
+    /* print usage if we need to */
+    if (usage) {
+        if (rank == 0) {
+            print_usage();
+        }
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+
+    char** argpaths = &argv[optind];
+
+    /* The remaining arguments are treated as paths */
+    int numpaths = argc - optind;
+
     /* create new mfu_file object */
     mfu_file_t* mfu_file = mfu_file_new();
 
+#ifdef DAOS_SUPPORT
+    /* Set up DAOS arguments, containers, dfs, etc. */
+    rc = daos_setup(rank, argpaths, numpaths, daos_args, mfu_file, NULL);
+    if (rc != 0) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "Detected one or more DAOS errors: "MFU_ERRF, MFU_ERRP(-MFU_ERR_DAOS));
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+    if (inputname && mfu_file->type == DFS) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "--input is not supported with DAOS"
+                    MFU_ERRF, MFU_ERRP(-MFU_ERR_INVAL_ARG));
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+    /* Not yet supported */
+    if (mfu_file->type == DAOS) {
+        if (rank == 0) {
+            MFU_LOG(MFU_LOG_ERR, "dwalk only supports DAOS POSIX containers with the DFS API.");
+        }
+        rc = 1;
+        goto daos_setup_done;
+    }
+
+daos_setup_done:
+    if (rc != 0) {
+        daos_cleanup(daos_args, mfu_file, NULL);
+        mfu_file_delete(&mfu_file);
+        mfu_finalize();
+        MPI_Finalize();
+        return 1;
+    }
+#endif
+
     /* paths to walk come after the options */
-    int numpaths = 0;
     mfu_param_path* paths = NULL;
-    if (optind < argc) {
+    if (numpaths > 0) {
         /* got a path to walk */
         walk = 1;
 
-        /* determine number of paths specified by user */
-        numpaths = argc - optind;
-
         /* allocate space for each path */
         paths = (mfu_param_path*) MFU_MALLOC((size_t)numpaths * sizeof(mfu_param_path));
 
         /* process each path */
-        char** p = &argv[optind];
-        mfu_param_path_set_all((uint64_t)numpaths, (const char**)p, paths, mfu_file, true);
-        optind += numpaths;
+        mfu_param_path_set_all((uint64_t)numpaths, (const char**)argpaths, paths, mfu_file, true);
 
         /* don't allow user to specify input file with walk */
         if (inputname != NULL) {
@@ -482,6 +546,9 @@ int main(int argc, char** argv)
         /* if we're not walking, we must be reading,
          * and for that we need a file */
         if (inputname == NULL) {
+            if (rank == 0) {
+                MFU_LOG(MFU_LOG_ERR, "Either a <path> or --input is required.");
+            }
             usage = 1;
         }
     }
@@ -570,6 +637,9 @@ int main(int argc, char** argv)
         if (rank == 0) {
             print_usage();
         }
+#ifdef DAOS_SUPPORT
+        daos_cleanup(daos_args, mfu_file, NULL);
+#endif
         mfu_file_delete(&mfu_file);
         mfu_finalize();
         MPI_Finalize();
@@ -624,6 +694,10 @@ int main(int argc, char** argv)
         }
     }
 
+#ifdef DAOS_SUPPORT
+    daos_cleanup(daos_args, mfu_file, NULL);
+#endif
+
     /* free users, groups, and files objects */
     mfu_flist_free(&flist);
 
@@ -649,5 +723,5 @@ int main(int argc, char** argv)
     mfu_finalize();
     MPI_Finalize();
 
-    return 0;
+    return rc;
 }
diff --git a/test/tests/test_dsync/test_xattr.py b/test/tests/test_dsync/test_xattr.py
new file mode 100644
index 0000000..a4021ff
--- /dev/null
+++ b/test/tests/test_dsync/test_xattr.py
@@ -0,0 +1,16 @@
+#!/usr/bin/env python2
+import subprocess
+
+# change paths here for bash script as necessary
+mpifu_path     = "~/mpifileutils/test/tests/test_dsync/test_xattr.sh"
+
+# vars in bash script
+dsync_test_bin   = "/root/mpifileutils/install/bin/dsync"
+dsync_src_dir    = "/mnt/lustre"
+dsync_dest_dir   = "/mnt/lustre2"
+dsync_test_file  = "file_test_xattr_XXX"
+
+def test_xattr():
+        p = subprocess.Popen(["%s %s %s %s %s %s %s" % (dsync_test_bin,
+          dsync_src_dir, dsync_dest_dir, dsync_test_file)], shell=True,
+          executable="/bin/bash").communicate()
diff --git a/test/tests/test_dsync/test_xattr.sh b/test/tests/test_dsync/test_xattr.sh
new file mode 100755
index 0000000..694c623
--- /dev/null
+++ b/test/tests/test_dsync/test_xattr.sh
@@ -0,0 +1,162 @@
+#!/bin/bash
+
+##############################################################################
+# Description:
+#
+#   A test to check if dsync properly copies xattrs
+#
+##############################################################################
+
+# Turn on verbose output
+#set -x
+
+DSYNC_TEST_BIN=${DSYNC_TEST_BIN:-${1}}
+DSYNC_SRC_DIR=${DSYNC_SRC_DIR:-${2}}
+DSYNC_DEST_DIR=${DSYNC_DEST_DIR:-${3}}
+DSYNC_TMP_FILE=${DSYNC_TMP_FILE:-${4}}
+
+echo "Using dsync binary at: $DSYNC_TEST_BIN"
+echo "Using src directory at: $DSYNC_SRC_DIR"
+echo "Using dest directory at: $DSYNC_DEST_DIR"
+echo "Using temp file name: $DSYNC_TMP_FILE"
+
+set -a lustre_xattr_names
+lustre_xattr_names=("lustre.lov" "trusted.som" "trusted.lov" "trusted.lma" "trusted.lmv" "trusted.dmv" "trusted.link" "trusted.fid" "trusted.version" "trusted.hsm" "trusted.lfsck_bitmap" "trusted.dummy")
+
+set -a other_xattr_names
+other_xattr_names=("xattr1" "456" "testing" "sync_and_verify_test")
+
+function fs_type()
+{
+	fname=$1
+	df -T ${fname} | awk '$1 != "Filesystem" {print $2}'
+}
+
+# set all xattrs in other_xattr_names on file
+# no need for equivalent for lustre xattrs, because they cannot be set from userspace
+function set_other_xattrs()
+{
+	fname=$1
+
+	set -e
+	for attrname in ${other_xattr_names[*]}; do
+		attr -s $attrname -V "$attrname:1234567890" $fname
+	done
+	set +e
+}
+
+function list_all_xattrs()
+{
+	fname=$1
+
+	echo Listing xattrs on $fname
+	attr -l $fname
+}
+
+function compare_xattr_lists()
+{
+	fname0=$1
+	fname1=$2
+
+	for attrname in ${lustre_xattr_names[*]} ${other_xattr_names[*]}; do
+		in0=$(attr -g -q $attrname $fname0 2>/dev/null && echo 1)
+		in1=$(attr -g -q $attrname $fname1 2>/dev/null && echo 1)
+		if [ "$in0" -eq 1 -o "$in1" -eq 1 ]; then
+			echo "$attrname $in0 $in1"
+		fi
+	done
+}
+
+function sync_and_verify()
+{
+	local srcdir=$1
+	local destdir=$2
+	local name=$3
+	local opt=$4
+
+	local result=0
+	local dest_type=$(fs_type $destdir)
+
+	if [ $opt = "non-lustre" ]; then
+		echo "SKIPPED verify of option $opt"
+		return 0
+	fi
+
+	if [ $opt = "libattr" -a $(id -u) -ne 0 ]; then
+		echo "SKIPPED verify of option $opt, need root to test"
+		return 0
+	fi
+
+	set -e
+	rm -f $destdir/$name
+
+	if [ $opt = "libattr" ]; then
+		echo "user.sync_and_verify_test  skip" >> /etc/xattr.conf
+	fi
+
+	$DSYNC_TEST_BIN --quiet --xattrs=$opt $srcdir $destdir
+
+	if [ $opt = "libattr" ]; then
+		sed --in-place "/^user.sync_and_verify_test/d" /etc/xattr.conf
+	fi
+
+	srclog=$(mktemp /tmp/sync_and_verify.src.XXXXX)
+	destlog=$(mktemp /tmp/sync_and_verify.dest.XXXXX)
+
+	list_all_xattrs  $srcdir/$name  | awk '!/Listing xattrs on/ {print $1,$2,$3,$4,$5}'  > $srclog
+	list_all_xattrs  $destdir/$name | awk '!/Listing xattrs on/ {print $1,$2,$3,$4,$5}'  > $destlog
+
+	case $opt in
+	  "none")
+		result=$(wc -c < $destlog)
+		;;
+	  "all")
+		diff $srclog $destlog
+		result=$?
+		;;
+	  "libattr")
+		diff <(grep -v -w sync_and_verify_test $srclog) $destlog
+		result=$?
+		;;
+	esac
+
+	set +e
+
+	if [ "$result" -eq 0 ]; then
+		echo "PASSED verify of option $opt for $destdir type $dest_type"
+	else
+		echo "FAILED verify of option $opt for $destdir type $dest_type"
+		echo =======================
+		echo "dest:"
+		cat $destlog
+		echo =======================
+		exit 1
+	fi
+
+	rm $srclog $destlog
+
+	return $result
+}
+
+# Create the source
+echo Preparing Source
+touch $DSYNC_SRC_DIR/aaa
+set_other_xattrs $DSYNC_SRC_DIR/aaa
+
+# Make sure the short option is accepted; rest of tests use long option
+set -e
+$DSYNC_TEST_BIN --quiet -X all $DSYNC_SRC_DIR $DSYNC_DEST_DIR
+set +e
+
+# Sync and verify
+echo
+echo Testing dsync
+sync_and_verify  $DSYNC_SRC_DIR $DSYNC_DEST_DIR aaa none
+sync_and_verify  $DSYNC_SRC_DIR $DSYNC_DEST_DIR aaa all
+sync_and_verify  $DSYNC_SRC_DIR $DSYNC_DEST_DIR aaa non-lustre
+sync_and_verify  $DSYNC_SRC_DIR $DSYNC_DEST_DIR aaa libattr
+
+# Clean up
+rm $DSYNC_SRC_DIR/aaa
+
+exit 0
